

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Homework 1: Language models (50 points) &#8212; Understanding LLMs</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'homework/01_language_modeling_solutions';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo-ULM-2024.png" class="logo__image only-light" alt="Understanding LLMs - Home"/>
    <script>document.write(`<img src="../_static/logo-ULM-2024.png" class="logo__image only-dark" alt="Understanding LLMs - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Course overview: Understanding LLMs
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">01 Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../lectures/01-introduction.html">Background</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/01-introduction.html">Sheet 1.1: Practical set-up &amp; Training data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">02 ANNs &amp; RNNs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../lectures/02-torch-ANNs-RNNs.html">PyTorch, ANNs &amp; LMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/02a-pytorch-intro.html">Sheet 2.1: PyTorch essentials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/02b-MLE.html">Sheet 2.2: ML-estimation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/02c-MLP-pytorch.html">Sheet 2.3: Non-linear regression (MLP w/ PyTorch modules)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/02d-char-level-RNN.html">Sheet 2.4: Character-level sequence modeling w/ RNNs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/02e-intro-to-hf.html">Sheet 2.5: Introduction to HuggingFace &amp; LMs</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">03 LSTMs &amp; transformers</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../lectures/03-LSTMs-Transformers.html">LSTMs &amp; Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/03a-tokenization-transformers.html">Sheet 3.1: Tokenization &amp; Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/03b-transformers-heads-training.html">Sheet 3.2: Transformer configurations &amp; Training utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lectures/04-LLMs-Prompting.html">Prompting &amp; Current LMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/03c-decoding-prompting.html">Sheet 3.3: Prompting &amp; Decoding</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">04 Fine-tuning &amp; RLHF</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../lectures/05-finetuning-RLHF.html">Fine-tuning and RLHF</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/04a-finetuning-RL.html">4.1 Supervised fine-tuning and RL fine-tuning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Homework</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01-language-modeling.html">Homework 1: Language models (50 points)</a></li>
<li class="toctree-l1"><a class="reference internal" href="02-prompting.html">Homework 2: Prompting &amp; Generation with LMs (50 points)</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/CogSciPrag/Understanding-LLMs-course/main?urlpath=tree/understanding-llms/homework/01_language_modeling_solutions.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onBinder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li><a href="https://colab.research.google.com/github/CogSciPrag/Understanding-LLMs-course/blob/main/understanding-llms/homework/01_language_modeling_solutions.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="initThebeSBT()"
  class="btn btn-sm btn-launch-thebe dropdown-item"
  title="Launch Thebe"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="btn__text-container">Live Code</span>
</button>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/CogSciPrag/Understanding-LLMs-course" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/CogSciPrag/Understanding-LLMs-course/issues/new?title=Issue%20on%20page%20%2Fhomework/01_language_modeling_solutions.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/homework/01_language_modeling_solutions.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Homework 1: Language models (50 points)</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistics">Logistics</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1-understanding-language-modeling-12-points">Exercise 1: Understanding language modeling (12 points)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#answer-exercise-1-understanding-language-modeling-12-points">Answer Exercise 1: Understanding language modeling (12 points)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#first-possibility">1. First Possibility</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#second-possibility">1. Second Possibility</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation">1.1 evaluation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">1.2 evaluation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">1.3 evaluation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2-extracting-llm-fingerprints-15-points">Exercise 2: Extracting LLM fingerprints (15 points)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-3-fine-tuning-gpt-2-for-qa-23-points">Exercise 3: Fine-tuning GPT-2 for QA (23 points)</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="homework-1-language-models-50-points">
<h1>Homework 1: Language models (50 points)<a class="headerlink" href="#homework-1-language-models-50-points" title="Permalink to this heading">#</a></h1>
<p>The first homework focuses on the following skills: being able to work with simple formal exercises on language modeling, on understanding and being eable to extract properties and configurations of state-of-the-art language models and, finally, conducting language model training yourself!</p>
<section id="logistics">
<h2>Logistics<a class="headerlink" href="#logistics" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>submission deadline: May 12th 23:59 German time via Moodle</p>
<ul>
<li><p>please upload a <strong>SINGLE ZIP FILE named Surname_FirtName_HW1.zip</strong> containing the .ipynb file of the notebook (if you solve it on Colab, you can go to File &gt; download) and the json file.</p></li>
</ul>
</li>
<li><p>please solve and submit the homework <strong>individually</strong>!</p></li>
<li><p>if you use Colab, to speed up the execution of the code on Colab (especially Exercise 3), you can use the available GPU (if Colab resources allow). For that, before executing your code, navigate to Runtime &gt; Change runtime type &gt; GPU &gt; Save.</p></li>
</ul>
</section>
<section id="exercise-1-understanding-language-modeling-12-points">
<h2>Exercise 1: Understanding language modeling (12 points)<a class="headerlink" href="#exercise-1-understanding-language-modeling-12-points" title="Permalink to this heading">#</a></h2>
<p>Please answer the following exercises. Importantly, please reason step by step; i.e., where calculations are required, please provide intermediate steps of how you arrived at your solution. You do not need to write any code, just mathematical solutions.</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>[6pts] Consider the corpus <span class="math notranslate nohighlight">\(C\)</span> with the following sentences: <span class="math notranslate nohighlight">\(C=\)</span>{“The cat sleeps”, “The mouse sings”, “The cat sleeps”, “A dog sings”}.
(a) Define the vocabulary <span class="math notranslate nohighlight">\(V\)</span> of this corpus (assuming by-word tokenization).
(b) Pick one of the four sentences in <span class="math notranslate nohighlight">\(C\)</span>. Formulate the probability of that sentence in the form of the chain rule. Calculate the probability of each termn in the chain rule, given the corpus.</p></li>
<li><p>[4pts] We want to train a neural network that takes as input two numbers <span class="math notranslate nohighlight">\(x_1, x_2\)</span>, passes them through three hidden linear layers, each with 13 neurons, each followed by the ReLU activation function, and outputs three numbers <span class="math notranslate nohighlight">\(y_1, y_2, y_3\)</span>. Write down all weight matrices of this network with their dimensions. (Example: if one weight matrix has the dimensions 3x5, write <span class="math notranslate nohighlight">\(M_1\in R^{3\times5}\)</span>)</p></li>
<li><p>[2pts] Consider the sequence: “Input: Some students trained each language model”. Assuming that each word+space/punctuation corresponds to one token, consider the following token probabilities of this sequence under some trained language model: <span class="math notranslate nohighlight">\(p = [0.67, 0.91, 0.83, 0.40, 0.29, 0.58, 0.75]\)</span>. Compute the average surprisal of this sequence under that language model. [Note: in this class we always assume the base <span class="math notranslate nohighlight">\(e\)</span> for <span class="math notranslate nohighlight">\(log\)</span>, unless indicated otherwise. This is also usually the case throughout NLP.]</p></li>
</ol>
</div></blockquote>
</section>
<section id="answer-exercise-1-understanding-language-modeling-12-points">
<h2>Answer Exercise 1: Understanding language modeling (12 points)<a class="headerlink" href="#answer-exercise-1-understanding-language-modeling-12-points" title="Permalink to this heading">#</a></h2>
<p>Note: The usage of BOS/EOS is not typical for theses models. Nonetheless both practices are correct.</p>
<blockquote>
<div></div></blockquote>
<section id="first-possibility">
<h3>1. First Possibility<a class="headerlink" href="#first-possibility" title="Permalink to this heading">#</a></h3>
<p>a. vocabulary <span class="math notranslate nohighlight">\(V\)</span>= {‘the’, ‘cat’, ‘sleeps’, ‘mouse’, ‘sings’,’a’, ‘dog’}</p>
<p>b. The cat sleeps:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*} 
P_{LM} = &amp;\prod^n_{n=1}P_{LM}(w_i|w_{1:i-1})\\
\end{align*}\]</div>
<p><span class="math notranslate nohighlight">\(P_{LM}(the,cat,sleeps)= p(the)*p(cat| the)*p(sleeps∣the,cat)\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
P_{LM}(the,cat,sleeps)=&amp;\frac{3}{12}*\frac{2}{3}*\frac{2}{2}= \frac{1}{4}*\frac{2}{3}=\frac{2}{12}= \frac{1}{6}= 0.167\\
\end{align*}\]</div>
<p>The mouse sings:</p>
</section>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
P_{LM} = \prod^n_{n=1}P_{LM}(w_i|w_{1:i-1})
\end{align*}\]</div>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(P_{LM}(the,mouse,sings)=p(the)*p(mouse∣the)*p(sings∣the,mouse)\)</span></p>
</div></blockquote>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;P_{LM}(the,mouse,sings)=\frac{3}{12}*\frac{1}{3}*\frac{1}{2}=\frac{1}{4}*\frac{1}{3}*\frac{1}{2}= \frac{1}{24}= 0.125\\
\end{align*}\]</div>
<blockquote>
<div><p>A dog sings:</p>
</div></blockquote>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*} 
&amp;P_{LM} = \prod^n_{n=1}P_{LM}(w_i|w_{1:i-1})\\
\end{align*}\]</div>
<p><span class="math notranslate nohighlight">\(P_{LM}(a,dog,sings)=p(a)*p(dog∣a)*p(sings∣a, dog)\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;P_{LM}(a, dog,sings)=\frac{1}{12}*\frac{1}{1}*\frac{1}{1}= \frac{1}{12}= 0.083\\
\end{align*}\]</div>
<section id="second-possibility">
<h3>1. Second Possibility<a class="headerlink" href="#second-possibility" title="Permalink to this heading">#</a></h3>
<blockquote>
<div><ol class="arabic">
<li><p>a. vocabulary <span class="math notranslate nohighlight">\(V\)</span>= {‘&lt;BOS&gt;’,’the’, ‘cat’, ‘sleeps’, ‘mouse’, ‘sings’,’a’, ‘dog’,’&lt;EOS&gt;’}</p>
<p>b. The cat sleeps:</p>
</li>
</ol>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*} 
P_{LM} = &amp;\prod^n_{n=1}P_{LM}(w_i|w_{1:i-1})\\
\end{align*}\]</div>
<p><span class="math notranslate nohighlight">\(P_{LM}(&lt;BOS&gt;, the,cat,sleeps, &lt;EOS&gt;)= p(the|&lt;BOS&gt;)*p(cat|&lt;BOS&gt;, the)*p(sleeps∣&lt;BOS&gt;,the,cat)*p(&lt;EOS&gt;∣&lt;BOS&gt;,the,cat, sleeps)\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
P_{LM}(&lt;BOS&gt;, the,cat,sleeps, &lt;EOS&gt;)=&amp;\frac{3}{4}*\frac{2}{3}*\frac{2}{2}*\frac{2}{2}= \frac{6}{12} = \frac{1}{2}= 0.5\\
\end{align*}\]</div>
<p>The mouse sings:</p>
</div></blockquote>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
P_{LM} = \prod^n_{n=1}P_{LM}(w_i|w_{1:i-1})
\end{align*}\]</div>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(P_{LM}(&lt;BOS&gt;,the,mouse,sings,&lt;EOS&gt;)=p(the|&lt;BOS&gt;)*p(mouse∣&lt;BOS&gt;,the)*p(sings∣&lt;BOS&gt;,the,mouse)\)</span>*p(<EOS> ∣<BOS>,the,mouse, sings, <EOS>)$</p>
</div></blockquote>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;P_{LM}(&lt;BOS&gt;,the,mouse, sings, &lt;EOS&gt;)=\frac{3}{4}*\frac{1}{3}*\frac{1}{1}*\frac{1}{1}=\frac{3}{12}= \frac{1}{4} = 0.25\\
\end{align*}\]</div>
<blockquote>
<div><p>A dog sings:</p>
</div></blockquote>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*} 
&amp;P_{LM} = \prod^n_{n=1}P_{LM}(w_i|w_{1:i-1})\\
\end{align*}\]</div>
<p><span class="math notranslate nohighlight">\(P_{LM}(&lt;BOS&gt;,a,dog,sings,&lt;EOS&gt;)=p(a|&lt;BOS&gt;)*p(dog∣&lt;BOS&gt;,a)*p(sings∣&lt;BOS&gt;,a, dog)\)</span>*p(sings∣<BOS>,a, dog, sings,<EOS>)$</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;P_{LM}(&lt;BOS&gt;,a, dog,sings,&lt;EOS&gt;)=\frac{1}{4}*\frac{1}{1}*\frac{1}{1}*\frac{1}{1}= \frac{1}{4}= 0.25\\
\end{align*}\]</div>
</section>
<section id="evaluation">
<h3>1.1 evaluation<a class="headerlink" href="#evaluation" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>vocabulary correct = 2 points</p></li>
<li><p>calculation correct = 4 points</p></li>
<li><p>calculation incorrect, but P_{LM} formula correct = 2 points</p></li>
<li><p>calculation and formula incorrect but idea understandable = 1 point</p></li>
</ul>
<p>deduction:</p>
<ul class="simple">
<li><p>vocabulary with minor mistake = -1 point</p></li>
<li><p>caculation correct for one alternative but formula of the other alterative = -1 point</p></li>
</ul>
<hr class="docutils" />
<ol class="arabic simple" start="2">
<li></li>
</ol>
<p>Usually we multiply the weight matrix by the input matrix, i.e. <span class="math notranslate nohighlight">\(W\cdot X\)</span>. Note that - unlike scalar multiplication - matrix multiplication is not commutative. <span class="math notranslate nohighlight">\(W\cdot X\)</span> is different to <span class="math notranslate nohighlight">\(X\cdot W\)</span>. If we compute <span class="math notranslate nohighlight">\(W\cdot X\)</span> (and this is how it is implemented in Pytorch as well), The weight matrices would be:</p>
<blockquote>
<div><p>input_neurons = 2</p>
<p>hidden_neurons = 13</p>
<p>output_neurons = 3</p>
<p><span class="math notranslate nohighlight">\(M_1 ϵ R^{13x2}\)</span> input to hidden1</p>
<p><span class="math notranslate nohighlight">\(b_1 ϵR^{13x1}\)</span> bias of weights1</p>
<p><span class="math notranslate nohighlight">\(M_2 ϵ R^{13x13}\)</span> hidden1 to hidden2</p>
<p><span class="math notranslate nohighlight">\(b_2 ϵ R^{13x1}\)</span> bias of weights2</p>
<p><span class="math notranslate nohighlight">\(M_3 ϵ R^{13x13}\)</span> hidden2 to hidden3</p>
<p><span class="math notranslate nohighlight">\(b_3 ϵ R^{13x1}\)</span> bias of weights3</p>
<p><span class="math notranslate nohighlight">\(M_4 ϵ R^{3x13}\)</span> hidden3 to output</p>
<p><span class="math notranslate nohighlight">\(b_4 ϵ R^{3x1}\)</span> bias of weights4</p>
</div></blockquote>
<p>In this case the input would also be transposed: 2x1. The first calculation, mupltiplying the first weight matrix by the input would be <span class="math notranslate nohighlight">\(W^{13\times 2}\cdot X^{2\times 1}\)</span>. It is important that the inner dimensions match (2 here). The result will have as many rows as W and as many columns as X (13x1 here).</p>
<p>However, it is also possible to compute <span class="math notranslate nohighlight">\(X\cdot W\)</span>. In this case we would have to transpose all matrices and the result would also be the transpose of the result from the previous solution. This is possible because of the matrix property <span class="math notranslate nohighlight">\((A\cdot B)^T = B^T\cdot A^T\)</span> (reverse the order and transpose both factors to get the transpose of the orinial result). The first calculation step would be <span class="math notranslate nohighlight">\(X^{1\times 2}\cdot W^{2\times 13}\)</span>. Again, the inner dimensions match and the result would be of shape 1x13.</p>
<blockquote>
<div><p>input_neurons = 2</p>
<p>hidden_neurons = 13</p>
<p>output_neurons = 3</p>
<p><span class="math notranslate nohighlight">\(M_1 ϵ R^{2x13}\)</span> input to hidden1</p>
<p><span class="math notranslate nohighlight">\(b_1 ϵR^{1x13}\)</span> bias of weights1</p>
<p><span class="math notranslate nohighlight">\(M_2 ϵ R^{13x13}\)</span> hidden1 to hidden2</p>
<p><span class="math notranslate nohighlight">\(b_2 ϵ R^{1x13}\)</span> bias of weights2</p>
<p><span class="math notranslate nohighlight">\(M_3 ϵ R^{13x13}\)</span> hidden2 to hidden3</p>
<p><span class="math notranslate nohighlight">\(b_3 ϵ R^{1x13}\)</span> bias of weights3</p>
<p><span class="math notranslate nohighlight">\(M_4 ϵ R^{13x3}\)</span> hidden3 to output</p>
<p><span class="math notranslate nohighlight">\(b_4 ϵ R^{1x3}\)</span> bias of weights4</p>
</div></blockquote>
</section>
<section id="id1">
<h3>1.2 evaluation<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Matrizes <span class="math notranslate nohighlight">\(M_i\)</span> from layer to layer correct = 3 points</p></li>
<li><p>comment or definition of bias weights <span class="math notranslate nohighlight">\(b_i\)</span> = 1 points</p></li>
<li><p>Matrizes incorrect, but correct right amount of matrizes = 2 points</p></li>
<li><p>Amount of matrizes <span class="math notranslate nohighlight">\(M_i\)</span> incorrect, but matrizes layer correct = 2 points (if additionally $b_i += 1 point)</p></li>
</ul>
<hr class="docutils" />
<blockquote>
<div><ol class="arabic" start="3">
<li><p>Sentence = ‘Input: Some students trained each language model’</p>
<p><span class="math notranslate nohighlight">\(p=[0.67,0.91,0.83,0.40,0.29,0.58,0.75]\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&gt;    P_{LM} = &amp;\prod^n_{n=1}P_{LM}(w_i|w_{1:i-1})= 0.67*0.91*0.83*0.40*0.29*0.58*0.75=0.0255\\
&gt;    \end{align*}\]</div>
<p><span class="math notranslate nohighlight">\(\text{Avg-Surprisal}_{LM}(w_{1:n}) =-\frac{1}{n}logP_{LM}(w_{1:n})\)</span>
<span class="math notranslate nohighlight">\(= -\frac{1}{7}log(0.2553533346)=-\frac{1}{7}*-0.59 = 0.523\)</span></p>
</li>
</ol>
</div></blockquote>
</section>
<section id="id2">
<h3>1.3 evaluation<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>calculation correct = 2 points</p></li>
</ul>
<p>deduction:</p>
<ul class="simple">
<li><p>caluclation incorrect, but P_{LM} formula correct or calculation only minor mistake = 1 points</p></li>
</ul>
</section>
</section>
<section id="exercise-2-extracting-llm-fingerprints-15-points">
<h2>Exercise 2: Extracting LLM fingerprints (15 points)<a class="headerlink" href="#exercise-2-extracting-llm-fingerprints-15-points" title="Permalink to this heading">#</a></h2>
<p>For this task, your job is to extract the “fingerprint” of a state-of-the-art large language model from the paper. Specifically, you job is to:</p>
<ul class="simple">
<li><p>find the model that is assigned to your surname in the list <strong>HW1_Model2Group_assignment.csv</strong> (to be found on Moodle under topic 02). Please investigate the latest version of your model, unless the version is specified in the list.</p></li>
<li><p>submit a json file with your responses in the following format (below is a partial example).</p></li>
</ul>
<p>Note that, of course, it might be that some information is not available or that some categories are not applicable. The idea is, that, as a course we can create a fun website which will show a somewhat comprehensive graphical comparison of current language models and their configurations. Based on your collective json files, the lecturers will set up a front end at some point during the class.</p>
<p><strong>IMPORTANT</strong>: Please email the lecturers by the homework deadline if you DO NOT consent that your json file is used for this idea.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s2">&quot;model_name&quot;</span><span class="p">:</span> <span class="s2">&quot;GPT-35&quot;</span><span class="p">,</span>
    <span class="s2">&quot;huggingface_model_id&quot;</span><span class="p">:</span> <span class="s2">&quot;gpt35&quot;</span><span class="p">,</span>
    <span class="s2">&quot;paper_url&quot;</span><span class="p">:</span> <span class="s2">&quot;https://arxiv.org/abs/XXX&quot;</span><span class="p">,</span>
    <span class="s2">&quot;tokenizer_type&quot;</span><span class="p">:</span> <span class="s2">&quot;BPE&quot;</span><span class="p">,</span>
    <span class="s2">&quot;vocabulary_size&quot;</span><span class="p">:</span> <span class="s2">&quot;XXX&quot;</span><span class="p">,</span>
    <span class="s2">&quot;architecture&quot;</span><span class="p">:</span> <span class="s2">&quot;Mixture of transformer agents&quot;</span><span class="p">,</span>
    <span class="s2">&quot;architecture_type&quot;</span><span class="p">:</span> <span class="s2">&quot;decoder only&quot;</span><span class="p">,</span>
    <span class="s2">&quot;architecture_quirks&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="s2">&quot;sparse attention&quot;</span><span class="p">,</span>
        <span class="s2">&quot;...&quot;</span><span class="p">,</span>
    <span class="p">],</span>
    <span class="s2">&quot;parameters&quot;</span><span class="p">:</span> <span class="s2">&quot;XXX&quot;</span><span class="p">,</span>
    <span class="s2">&quot;finetuning_type&quot;</span><span class="p">:</span> <span class="s2">&quot;RLHF&quot;</span><span class="p">,</span>
    <span class="s2">&quot;training_data_cutoff&quot;</span><span class="p">:</span> <span class="s2">&quot;2050&quot;</span><span class="p">,</span>
    <span class="s2">&quot;number_training_tokens&quot;</span><span class="p">:</span> <span class="s2">&quot;XXX&quot;</span><span class="p">,</span>
    <span class="s2">&quot;pretraining_data_size&quot;</span><span class="p">:</span> <span class="s2">&quot;1GB&quot;</span><span class="p">,</span>
    <span class="s2">&quot;finetuning_data_size&quot;</span><span class="p">:</span> <span class="s2">&quot;XXX&quot;</span><span class="p">,</span>
    <span class="s2">&quot;training_data&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="s2">&quot;Books corpus&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Twitter&quot;</span><span class="p">,</span>
        <span class="s2">&quot;...&quot;</span>
    <span class="p">],</span>
    <span class="s2">&quot;finetuning_data&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="s2">&quot;XXX&quot;</span><span class="p">,</span>
        <span class="s2">&quot;XXX&quot;</span><span class="p">,</span>
        <span class="s2">&quot;...&quot;</span>
    <span class="p">],</span>
    <span class="s2">&quot;access&quot;</span><span class="p">:</span> <span class="s2">&quot;open&quot;</span><span class="p">,</span>
    <span class="s2">&quot;summary&quot;</span><span class="p">:</span> <span class="s2">&quot;A few sentences of what the model claims to be their unique selling point / main contribution&quot;</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;model_name&#39;: &#39;GPT-35&#39;,
 &#39;huggingface_model_id&#39;: &#39;gpt35&#39;,
 &#39;paper_url&#39;: &#39;https://arxiv.org/abs/XXX&#39;,
 &#39;tokenizer_type&#39;: &#39;BPE&#39;,
 &#39;vocabulary_size&#39;: &#39;XXX&#39;,
 &#39;architecture&#39;: &#39;Mixture of transformer agents&#39;,
 &#39;architecture_type&#39;: &#39;decoder only&#39;,
 &#39;architecture_quirks&#39;: [&#39;sparse attention&#39;, &#39;...&#39;],
 &#39;parameters&#39;: &#39;XXX&#39;,
 &#39;finetuning_type&#39;: &#39;RLHF&#39;,
 &#39;training_data_cutoff&#39;: &#39;2050&#39;,
 &#39;number_training_tokens&#39;: &#39;XXX&#39;,
 &#39;pretraining_data_size&#39;: &#39;1GB&#39;,
 &#39;finetuning_data_size&#39;: &#39;XXX&#39;,
 &#39;training_data&#39;: [&#39;Books corpus&#39;, &#39;Twitter&#39;, &#39;...&#39;],
 &#39;finetuning_data&#39;: [&#39;XXX&#39;, &#39;XXX&#39;, &#39;...&#39;],
 &#39;access&#39;: &#39;open&#39;,
 &#39;summary&#39;: &#39;A few sentences of what the model claims to be their unique selling point / main contribution&#39;}
</pre></div>
</div>
</div>
</div>
<p>List of models (maybe each student could even get their own model :’D). Mapping to names TBD.</p>
<ul class="simple">
<li><p>phi-2</p></li>
<li><p>mixtral, mixtral-instruct</p></li>
<li><p>mistral, mistral-instruct v2</p></li>
<li><p>mamba</p></li>
<li><p>jamba (yes i’m not joking)</p></li>
<li><p>llama-2 suite</p></li>
<li><p>llama-3 (8b, 70b)</p></li>
<li><p>gpt-3</p></li>
<li><p>gemini 1.5 (the whole suite is multimodal though)</p></li>
<li><p>claude (there seems to only be a technical report, not sure how many details)</p></li>
<li><p>palm-2</p></li>
<li><p>Bloom (might be interesting because it’s multilingual)</p></li>
<li><p>Grok 1.5 (not sure there is an actual paper)</p></li>
<li><p>Vicuna (blog not paper)</p></li>
<li><p>Falcon 40b</p></li>
<li><p>Gemma</p></li>
<li><p>DBRX</p></li>
<li><p>cmd-r+ (rag integrated, other provider (cohere))</p></li>
<li><p>h2o danube</p></li>
<li><p>BitNet</p></li>
<li><p>JetMoE-8b</p></li>
<li><p>qwen-1.5-MoE</p></li>
<li><p>wizardLM</p></li>
</ul>
</section>
<section id="exercise-3-fine-tuning-gpt-2-for-qa-23-points">
<h2>Exercise 3: Fine-tuning GPT-2 for QA (23 points)<a class="headerlink" href="#exercise-3-fine-tuning-gpt-2-for-qa-23-points" title="Permalink to this heading">#</a></h2>
<p>The learning goal of this exercise is to practice fine-tuning a pretrained LM, GPT-2 small, for a particular task, namely commonsense question answering (QA). We will use a task-specific dataset, <a class="reference external" href="https://huggingface.co/datasets/tau/commonsense_qa">CommonsenseQA</a>, that was introduced by <a class="reference external" href="https://arxiv.org/abs/1811.00937">Talmor et al. (2018)</a>. We will evaluate the performance of the model on our test split of the dataset over training to monitor whether the model’s performance is improving and compare the performance of the base pretrained GPT-2 and the fine-tuned model. We will need to perform the following steps:</p>
<ol class="arabic simple">
<li><p>Prepare data according to steps described in <a class="reference external" href="https://cogsciprag.github.io/Understanding-LLMs-course/tutorials/01-introduction.html#main-training-data-processing-steps">sheet 1.1</a></p>
<ol class="arabic simple">
<li><p>additionally to these steps, prepare a custom Dataset (like in <a class="reference external" href="https://cogsciprag.github.io/Understanding-LLMs-course/tutorials/02c-MLP-pytorch.html#preparing-the-training-data">sheet 2.3</a>) that massages the dataset from the format that it is shipped in on HuggingFace into strings that can be used for training. Some of the procesing steps will happen in the Dataset.</p></li>
</ol>
</li>
<li><p>Load the pretrained GPT-2 model</p></li>
<li><p>Set up training pipeline according to steps described in <span class="xref myst">sheet 2.5</span></p></li>
<li><p>Run the training while tracking the losses</p></li>
<li><p>Save plot of losses for submission</p></li>
</ol>
<p>Your tasks:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>[19pts] Complete the code in the spots where there is a comment “#### YOUR CODE HERE ####”. There are instructions in the comments as to what the code should implement. With you completed code, you should be able to let the training run without errors. Note that the point of the exercise is the implementation; we should not necessarily expect great performance of the fine-tuned model (and the actual performance will not be graded). Often there are several correct ways of implementing something. Anything that is correct will be accepted.</p></li>
<li><p>[4pts] Answer questions at the end of the execise.</p></li>
</ol>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># note: if you are on Colab, you might need to install some requirements</span>
<span class="c1"># as we did in Sheet 1.1. Otherwise, don&#39;t forget to activate your local environment</span>

<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="o">&lt;</span><span class="n">ipython</span><span class="o">-</span><span class="nb">input</span><span class="o">-</span><span class="mi">3</span><span class="o">-</span><span class="mi">66</span><span class="n">d1a2d3ccf0</span><span class="o">&gt;</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="c1"># as we did in Sheet 1.1. Otherwise, don&#39;t forget to activate your local environment</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> 
<span class="ne">----&gt; </span><span class="mi">4</span> <span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
<span class="g g-Whitespace">      </span><span class="mi">6</span> <span class="kn">import</span> <span class="nn">torch</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;datasets&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># additioanlly, we need to install accelerate</span>
<span class="c1"># uncomment and run the following line on Colab or in your environment</span>
<span class="c1"># !pip install accelerate</span>
<span class="c1"># NOTE: in a notebook, reloading of the kernel might be required after installation if you get dependency errors with the transformers package</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### 1. Prepare data with data prepping steps from sheet 1.1</span>

<span class="c1"># a. Acquiring data</span>
<span class="c1"># b. (minimally) exploring dataset</span>
<span class="c1"># c. cleaning / wrangling data (combines step 4 from sheet 1.1 and step 1.1 above)</span>
<span class="c1"># d. splitting data into training and test set (we will not do any hyperparam tuning)</span>
<span class="c1"># (we don&#39;t need further training set wrangling)</span>
<span class="c1"># e. tokenizing data and making sure it can be batched (i.e., conversted into 2d tensors)</span>
<span class="c1"># this will also happen in our custom Dataset class (common practice when working with text data)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># downaload dataset from HF</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;tau/commonsense_qa&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># inspect dataset</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span class="c1"># print a sample from the  dataset</span>
<span class="c1">### YOUR CODE HERE ####</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Note that the test split does not have ground truth answer labels. Therefore, we will use the validation split as our test split.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># load tokenizer</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
<span class="c1"># set padding side to be left because we are doing causal LM</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">padding_side</span> <span class="o">=</span> <span class="s2">&quot;left&quot;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">massage_input_text</span><span class="p">(</span><span class="n">example</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Helper for converting input examples which have </span>
<span class="sd">    a separate qquestion, labels, answer options</span>
<span class="sd">    into a single string.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    example: dict</span>
<span class="sd">        Sample input from the dataset which contains the </span>
<span class="sd">        question, answer labels (e.g. A, B, C, D),</span>
<span class="sd">        the answer options for the question, and which </span>
<span class="sd">        of the answers is correct.</span>
<span class="sd">    </span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    input_text: str</span>
<span class="sd">        Formatted training text which contains the question,</span>
<span class="sd">        the forwatted answer options (e.g., &#39;A. &lt;option 1&gt; B. &lt;option 2&gt;&#39; etc)</span>
<span class="sd">        and the ground truth answer.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># combine each label with its corresponding text</span>
    <span class="n">answer_options_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span>
        <span class="n">example</span><span class="p">[</span><span class="s2">&quot;choices&quot;</span><span class="p">][</span><span class="s2">&quot;label&quot;</span><span class="p">],</span>
        <span class="n">example</span><span class="p">[</span><span class="s2">&quot;choices&quot;</span><span class="p">][</span><span class="s2">&quot;text&quot;</span><span class="p">]</span>
    <span class="p">))</span>
    <span class="c1"># join each label and text with . and space</span>
    <span class="c1">### YOUR CODE HERE ####</span>
    <span class="n">answer_options</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">label</span><span class="si">}</span><span class="s2">. </span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">answer_options_list</span><span class="p">]</span>
    <span class="c1"># join the list of options with spaces into single string</span>
    <span class="c1">### YOUR CODE HERE ####</span>
    <span class="n">answer_options_string</span> <span class="o">=</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">answer_options</span><span class="p">)</span>
    <span class="c1"># combine question and answer options</span>
    <span class="n">input_text</span> <span class="o">=</span> <span class="n">example</span><span class="p">[</span><span class="s2">&quot;question&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot; &quot;</span> <span class="o">+</span> <span class="n">answer_options_string</span>
    <span class="c1"># append the true answer with a new line, &quot;Answer: &quot; and the label</span>
    <span class="n">input_text</span> <span class="o">+=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Answer: &quot;</span> <span class="o">+</span> <span class="n">example</span><span class="p">[</span><span class="s2">&quot;answerKey&quot;</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">input_text</span>

<span class="c1"># process input texts of train and test sets</span>
<span class="n">massaged_datasets</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="n">example</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="n">massage_input_text</span><span class="p">(</span><span class="n">example</span><span class="p">)</span>
    <span class="p">}</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># inspect a sample from our preprocessed data</span>
<span class="n">massaged_datasets</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot; </span>
<span class="sd">Note that unfortunately the Docstring for __len__ and __getitem__ were wrong. </span>
<span class="sd">Because we create a train and test dataset we need to return the respective split, not always the train split.</span>
<span class="sd">We did not deduct any points if you only used the train split as the Docstring suggests.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="k">class</span> <span class="nc">CommonsenseQADataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Custom dataset class for CommonsenseQA dataset.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">train_split</span><span class="p">,</span>
            <span class="n">test_split</span><span class="p">,</span>
            <span class="n">tokenizer</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
            <span class="n">dataset_split</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">,</span>
        <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        fill me.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_split</span> <span class="o">=</span> <span class="n">train_split</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">test_split</span> <span class="o">=</span> <span class="n">test_split</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_length</span> <span class="o">=</span> <span class="n">max_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dataset_split</span> <span class="o">=</span> <span class="n">dataset_split</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Method returning the length of the training dataset.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1">### YOUR CODE HERE ####</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_split</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset_split</span> <span class="o">==</span> <span class="s2">&quot;train&quot;</span> <span class="k">else</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">test_split</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Method returning a single training example.</span>
<span class="sd">        Note that it also tokenizes, truncates or pads the input text.</span>
<span class="sd">        Further, it creates a mask tensor for the input text which </span>
<span class="sd">        is used for causal masking in the transformer model.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        idx: int</span>
<span class="sd">            Index of training sample to be retrieved from the data.</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        --------</span>
<span class="sd">        tokenized_input: dict</span>
<span class="sd">            Dictionary with input_ids (torch.Tensor) and an attention_mask</span>
<span class="sd">            (torch.Tensor).</span>
<span class="sd">        &quot;&quot;&quot;</span>        <span class="c1"># retrieve a training sample at the specified index idx</span>
        <span class="c1"># HINT: note that this might depend on self.dataset_split</span>
        <span class="c1">### YOUR CODE HERE ####</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset_split</span> <span class="o">==</span> <span class="s2">&quot;train&quot;</span><span class="p">:</span>
            <span class="n">input_text</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_split</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">input_text</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">test_split</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">tokenized_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span>
            <span class="n">input_text</span><span class="p">,</span>
            <span class="c1">### YOUR CODE HERE ####</span>
            <span class="n">max_length</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_length</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;max_length&quot;</span><span class="p">,</span>
            <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span>
        <span class="p">)</span>
        <span class="n">tokenized_input</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">tokenized_input</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="o">!=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tokenized_input</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># move to accelerated device</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Device: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">elif</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;mps&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Device: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Device: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 2. init model</span>

<span class="c1"># load pretrained gpt2 for HF</span>
<span class="c1">### YOUR CODE HERE ####</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
<span class="c1"># print num of trainable parameters</span>
<span class="n">model_size</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GPT-2 size: </span><span class="si">{</span><span class="n">model_size</span><span class="o">/</span><span class="mi">1000</span><span class="o">**</span><span class="mi">2</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">M parameters&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 3. set up configurations required for the training loop</span>

<span class="c1"># instantiate dataset</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">CommonsenseQADataset</span><span class="p">(</span>
    <span class="c1">### YOUR CODE HERE ####</span>
    <span class="n">massaged_datasets</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">],</span>
    <span class="n">massaged_datasets</span><span class="p">[</span><span class="s2">&quot;validation&quot;</span><span class="p">],</span>
    <span class="n">tokenizer</span>
<span class="p">)</span>
<span class="c1"># instantiate test dataset with the downloaded commonsense_qa data</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">CommonsenseQADataset</span><span class="p">(</span>
    <span class="c1">### YOUR CODE HERE ####,</span>
    <span class="n">massaged_datasets</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">],</span>
    <span class="n">massaged_datasets</span><span class="p">[</span><span class="s2">&quot;validation&quot;</span><span class="p">],</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">dataset_split</span><span class="o">=</span><span class="s2">&quot;test&quot;</span>
<span class="p">)</span>
<span class="c1"># create a DataLoader for the dataset</span>
<span class="c1"># the data loader will automatically batch the data</span>
<span class="c1"># and iteratively return training examples (question answer pairs) in batches</span>
<span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
<span class="c1"># create a DataLoader for the test dataset</span>
<span class="c1"># reason for separate data loader is that we want to</span>
<span class="c1"># be able to use a different index for retreiving the test batches</span>
<span class="c1"># we might also want to use a different batch size etc.</span>
<span class="n">test_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">test_dataset</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 4. run the training of the model</span>
<span class="c1"># Hint: for implementing the forward pass and loss computation, carefully look at the exercise sheets </span>
<span class="c1"># and the links to examples in HF tutorials.</span>

<span class="c1"># put the model in training mode</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="c1"># move the model to the device (e.g. GPU)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># trianing configutations</span>
<span class="c1"># feel free to play around with these</span>
<span class="n">epochs</span>  <span class="o">=</span> <span class="mi">2</span>
<span class="n">train_steps</span> <span class="o">=</span>  <span class="nb">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span> <span class="o">//</span> <span class="mi">32</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of training steps: &quot;</span><span class="p">,</span> <span class="n">train_steps</span><span class="p">)</span>
<span class="c1"># number of test steps to perform every 10 training steps</span>
<span class="c1"># (smaller that the entore test split for reasons of comp. time)</span>
<span class="n">num_test_steps</span> <span class="o">=</span> <span class="mi">5</span>

<span class="c1"># define optimizer and learning rate</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">5e-4</span><span class="p">)</span> <span class="c1">### YOUR CODE HERE ###</span>
<span class="c1"># define some variables to accumulate the losses</span>
<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">test_losses</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># iterate over epochs</span>
<span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="c1"># iterate over training steps</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">train_steps</span><span class="p">)):</span>
        <span class="c1"># get a batch of data</span>
        <span class="n">x</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">dataloader</span><span class="p">))</span>
        <span class="c1"># move the data to the device (GPU)</span>
        <span class="c1">### YOUR CODE HERE ###</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># forward pass through the model</span>
        <span class="c1">### YOUR CODE HERE ###</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span>
            <span class="o">**</span><span class="n">x</span><span class="p">,</span>
            <span class="n">labels</span><span class="o">=</span><span class="n">x</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="c1"># get the loss</span>
        <span class="c1">### YOUR CODE HERE ###</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">loss</span>
        <span class="c1"># backward pass</span>
        <span class="c1">### YOUR CODE HERE ###</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
        <span class="c1"># update the parameters of the model</span>
        <span class="c1">### YOUR CODE HERE ###</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="c1"># zero out gradient for next step</span>
        <span class="c1">### YOUR CODE HERE ###</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="c1"># evaluate on test set every 10 steps</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">, step </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">, loss </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="c1"># TODO</span>
            <span class="c1"># track test loss for the evaluation iteration</span>
            <span class="n">test_loss</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_test_steps</span><span class="p">):</span>
                <span class="c1"># get test batch</span>
                <span class="n">x_test</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">test_dataloader</span><span class="p">))</span>
                <span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
                <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                    <span class="n">test_outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span>
                        <span class="o">**</span><span class="n">x_test</span><span class="p">,</span>
                        <span class="n">labels</span><span class="o">=</span><span class="n">x_test</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span>
                        <span class="c1">### YOUR CODE HERE ####</span>
                    <span class="p">)</span>
                <span class="n">test_loss</span> <span class="o">+=</span> <span class="n">test_outputs</span><span class="o">.</span><span class="n">loss</span><span class="c1">### YOUR CODE HERE ####</span>

            <span class="n">test_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_loss</span> <span class="o">/</span> <span class="n">num_test_steps</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test loss: &quot;</span><span class="p">,</span> <span class="n">test_loss</span><span class="o">/</span><span class="n">num_test_steps</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot the fine-tuning loss</span>
<span class="c1">### YOUR CODE HERE ###</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;train loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">losses</span><span class="p">),</span> <span class="mi">10</span><span class="p">),</span> <span class="n">y</span> <span class="o">=</span> <span class="n">test_losses</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;test loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Training steps&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Loss&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># print a few predictions on the eval dataset to see what the model predicts</span>

<span class="c1"># construct a list of questions without the ground truth label</span>
<span class="c1"># and compare prediction of the model with the ground truth</span>

<span class="k">def</span> <span class="nf">construct_test_samples</span><span class="p">(</span><span class="n">example</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Helper for converting input examples which have </span>
<span class="sd">    a separate qquestion, labels, answer options</span>
<span class="sd">    into a single string for testing the model.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    example: dict</span>
<span class="sd">        Sample input from the dataset which contains the </span>
<span class="sd">        question, answer labels (e.g. A, B, C, D),</span>
<span class="sd">        the answer options for the question, and which </span>
<span class="sd">        of the answers is correct.</span>
<span class="sd">    </span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    input_text: str, str</span>
<span class="sd">        Tuple: Formatted test text which contains the question,</span>
<span class="sd">        the forwatted answer options (e.g., &#39;A. &lt;option 1&gt; B. &lt;option 2&gt;&#39; etc); </span>
<span class="sd">        the ground truth answer label only.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="n">answer_options_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span>
        <span class="n">example</span><span class="p">[</span><span class="s2">&quot;choices&quot;</span><span class="p">][</span><span class="s2">&quot;label&quot;</span><span class="p">],</span>
        <span class="n">example</span><span class="p">[</span><span class="s2">&quot;choices&quot;</span><span class="p">][</span><span class="s2">&quot;text&quot;</span><span class="p">]</span>
    <span class="p">))</span>
    <span class="c1"># join each label and text with . and space</span>
    <span class="c1">### YOUR CODE HERE ####</span>
    <span class="n">answer_options</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">label</span><span class="si">}</span><span class="s2">. </span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">answer_options_list</span><span class="p">]</span>
    <span class="c1"># join the list of options with spaces into single string</span>
    <span class="c1">### YOUR CODE HERE ####</span>
    <span class="n">answer_options_string</span> <span class="o">=</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">answer_options</span><span class="p">)</span>
    <span class="c1"># combine question and answer options</span>
    <span class="n">input_text</span> <span class="o">=</span> <span class="n">example</span><span class="p">[</span><span class="s2">&quot;question&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot; &quot;</span> <span class="o">+</span> <span class="n">answer_options_string</span>
    <span class="c1"># create the test input text which should be:</span>
    <span class="c1"># the input text, followed by the string &quot;Answer: &quot;</span>
    <span class="c1"># we don&#39;t need to append the ground truth answer since we are creating test inputs</span>
    <span class="c1"># and the answer should be predicted.    </span>
    <span class="c1">### YOUR CODE HERE ####</span>
    <span class="n">input_text</span> <span class="o">+=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Answer: &quot;</span>

    <span class="k">return</span> <span class="n">input_text</span><span class="p">,</span> <span class="n">example</span><span class="p">[</span><span class="s2">&quot;answerKey&quot;</span><span class="p">]</span>

<span class="n">test_samples</span> <span class="o">=</span> <span class="p">[</span><span class="n">construct_test_samples</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;validation&quot;</span><span class="p">][</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)]</span>
<span class="n">test_samples</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Test the model</span>

<span class="c1"># set it to evaluation mode</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="n">predictions</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">test_samples</span><span class="p">:</span>
    <span class="n">input_text</span> <span class="o">=</span> <span class="n">sample</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">input_text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
        <span class="n">input_ids</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span>
        <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">predictions</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">input_text</span><span class="p">,</span> <span class="n">prediction</span><span class="p">,</span> <span class="n">sample</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Predictions of trained model &quot;</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Questions:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Provide a brief description of the CommonsenseQA dataset. What kind of task was it developed for, what do the single columns contain? <strong>Answer</strong> The CommonsenseQA dataset contains common sense question, i.e. questions where finding the correct answer involved basic world knowledge. These are presented as a question, a question concept, a number of labelled possible answers, and the label of the correct answer.</p></li>
<li><p>What loss function is computed? Where is it implemented? <strong>Answer</strong> The loss function is Negative Log-Likelihood Loss (you can show this by print(loss), which will show grad_fn=<NllLossBackward0>)</p></li>
<li><p>Given your loss curve, do you think your model will perform well oon answering common sense questions? (there is no single right answer; you need to interpret your specific plot)</p></li>
<li><p>Inspect the predictions above. On how many test questions did the model predict the right answer? Compute the accuracy.</p></li>
</ol>
</div></blockquote>
<blockquote>
<div><p>Accuracy = <span class="math notranslate nohighlight">\(\frac{\#correct\:predictions}{\#total\:predictions}\)</span></p>
</div></blockquote>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "CogSciPrag/Understanding-LLMs-course",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./homework"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistics">Logistics</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1-understanding-language-modeling-12-points">Exercise 1: Understanding language modeling (12 points)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#answer-exercise-1-understanding-language-modeling-12-points">Answer Exercise 1: Understanding language modeling (12 points)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#first-possibility">1. First Possibility</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#second-possibility">1. Second Possibility</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation">1.1 evaluation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">1.2 evaluation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">1.3 evaluation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2-extracting-llm-fingerprints-15-points">Exercise 2: Extracting LLM fingerprints (15 points)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-3-fine-tuning-gpt-2-for-qa-23-points">Exercise 3: Fine-tuning GPT-2 for QA (23 points)</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Michael Franke, Carsten Eickhoff, Polina Tsvilodub
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>