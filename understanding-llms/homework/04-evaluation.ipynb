{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 4: LLM evaluation\n",
    "=====\n",
    "\n",
    "The third homework zooms in on evaluating LLMs, specifically, on the following skills: using log probabilities of string under a trained LM to evaluate it, coming up with items to test particular aspets of LLMs, and comparing LLM measures to measures of human performance.\n",
    "\n",
    "### Logistics\n",
    "\n",
    "* submission deadline: July 13th th 23:59 German time via Moodle\n",
    "  * please upload a **SINGLE .IPYNB FILE named Surname_FirstName_HW4.ipynb** containing your solutions of the homework. Make sure that your **plots** for the last exercise are either rendered in the notebook or submitted together with it in a zip file. \n",
    "* please solve and submit the homework **individually**! \n",
    "* if you use Colab, to speed up the execution of the code on Colab, you can use the available GPU (if Colab resources allow). For that, before executing your code, navigate to Runtime > Change runtime type > GPU > Save."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Understanding the learning trajectory of grammatical capabilities of LLMs (XXX points)\n",
    "\n",
    "This task would be based on code and data from [this](https://colab.research.google.com/drive/1uRdl0-pYximdpSeKyL7VbGFcZKqXlWnj?usp=sharing) tutorial which I recently participated in. However, I'm still waiting for a response from the authors as to whether we can reuse this for the homework. Therefore, only concpetual outline and the tasks below for now.\n",
    "\n",
    "The tutorial provides evaluations of the Pythia-160m model, for different seeds, on all test suites from BLiMP, for different training steps of the model. The data provides accuracies on each BLiMP test suite. Given this data (from one seed only to make things easier) and some starter code, the task for the students would be to finish the code and answer the following questions:\n",
    "* Plot the accuracy of the model over training time, averaging across evaluation suites. How many training steps are needed until the model has \"learned grammar\"?\n",
    "* Plot the accuracy of the model over training time by test area (syntax, morphology, syntac-semantics, semantics). Are the acquisition trajectories the same across the different fields? Which field is the most difficult one?\n",
    "* What is the easiest gramamtical phenomenon, what is the most difficult gramamtical phenomenon (as captured by the single test suites) for the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Evaluating societal biases (XXX points)\n",
    "\n",
    "* TODO stochastic parrots paper for some more background, if needed, or ETHICS dataset as reference ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will consider aspects of LLM performance which may have social implications and that are deeply interconnected with how humans use language. This task evaluates whether LLMs overrepresent certain cultures over others, which could be due to, e.g., imbalances over training data sources and languages.\n",
    "\n",
    "> Specifically, your task is to come up with an appropriate test item and evaluate whether LLMs exhibit certain cultural biases. In this task, you have to construct your own multiple-choice test item for investigating cultural biases of LLMs, where, given a context, the different available response / continuation options would reflect preferences for responses typical for different cultures. For instance, one response could be more acceptable under one particular cultural lense and another response under a different cultural background. Your task is then to evaluate the performance of two LLMs: the mostly monolingual `gpt2` and the multilingual `bigscience/bloom-550m` model. The second part of the task is to compelte the evaluation code and interpret the results by answering the question below.\n",
    "\n",
    "\n",
    "Here is a simple example of a test item. More explanations are in parentheses. You should provide analogous explanations in the answers to the questions below, but not pass these to the LLMs during evaluations.\n",
    "\n",
    "\n",
    "Context 1: You are at a German supermarket. You walk up to the cashier and greet them by saying:\n",
    "\n",
    "Context 2: You are at an American supermarket. You walk up to the cashier and greet them by saying:\n",
    "\n",
    "A. Hello. (intuititvely, more likely in to be appropiate in the Germany context condition)\n",
    "\n",
    "B. Bye. (a generally inappropriate response)\n",
    "\n",
    "C. Hello, how are you? (intuitively, more likely to be appropriate in the US context condition; people usually don’t ask strangers ‘how are you’ in Germany)\n",
    "\n",
    "I would say: (insert each of the answer options separately here and calculate their log probability, given each of the contexts).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Fill in your responses below.**\n",
    ">\n",
    "> 1. Your prompt (with explanations of the intuitive differences for each response option in respective cultural variations):\n",
    "> 2. Your model log probabilities (table cells are examples, please fill in with your respective item):\n",
    ">\n",
    "| Context / Option | GPT-2 | Bloom |\n",
    "|------------------|-------|-------|\n",
    "| Germany + A      | ...   | ...   |\n",
    "| USA + A          |       |       |\n",
    "| Germany + B      |       |       |\n",
    "| USA + B          |       |       |\n",
    "| ...              |       |       |\n",
    "\n",
    "> 3. Do the models show a preference for a particular cultural setting? Is there evidence for whether cultural biases might be caused by training data?\n",
    "> 4. Are there aspects of the prompt that might influence your results? Please provide a brief justification / example why (not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minicons import scorer \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is some starter code; please fill in your code / comments where it says #### YOUR CODE / COMMENT HERE ####\n",
    "\n",
    "# set up a scorer \n",
    "gpt2_scorer = scorer.IncrementalLMScorer(\n",
    "    ### YOUR CODE HERE ###\n",
    ")\n",
    "\n",
    "bloom_scorer = scorer.IncrementalLMScorer(\n",
    "    ### YOUR CODE HERE ###\n",
    ")\n",
    "# initialize list for storing the predictions\n",
    "gpt2_predictions = []\n",
    "bloom_predictions = []\n",
    "answer_keys = [\"ger\", \"nonsense\", \"us\"]\n",
    "\n",
    "# iterate over contexts\n",
    "for context in range(### YOUR CODE HERE ###):\n",
    "    # format / provide  the possible answer options from your vignette\n",
    "    answer_options = ### YOUR CODE HERE ###\n",
    "    # pass a list of contexts and a list of continuations to be scored\n",
    "    answer_scores_gpt2 = gpt2_scorer.conditional_score(\n",
    "        # format the question into a list of same length as the number of answer options\n",
    "        ### YOUR CODE HERE ###,\n",
    "    ) \n",
    "    answer_scores_bloom = bloom_scorer.conditional_score(\n",
    "        # format the question into a list of same length as the number of answer options\n",
    "        ### YOUR CODE HERE ###,\n",
    "    )\n",
    "    \n",
    "    # check / inspect which answer has the highest score and which answer type (i.e., \"culture\") it corresponds to\n",
    "    ### YOUR CODE / COMMENT HERE ### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: LLM evaluations with LLMs (XXX points)\n",
    "\n",
    "Building on the in-context learning capabilities of LLMs,, recent work, e.g., by [Perez et al (2022)](https://arxiv.org/abs/2212.09251), has been *using LLMs to generate evaluation datasets for LLMs*. \n",
    "\n",
    "> Your task here is to: \n",
    "> 1. write a pseudo-algorithm for generating more cultural bias evaluation items. The items should be of a similar structure as in the task above. Write maximally 5 steps. (Hint: feel free to try to elicit e.g. 10 different item with a model of your choice)\n",
    "> 2. What could be possible concerns with this approach? Name and briefly explain 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: How human-like are Llama's surprisals?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More recently, work more informed by human language use and processing has compared LLMs’ performance to aspects of human behavior. Here, the assessment of LLMs is guided more by the question of how human-like certain aspects of its performance are. Your task in this exercise is to assess whether the surprisal of different language models is comparable to human reading times, when it comes to processing subject-verb agreement. The data is taken from [Wilcox et al. (2021)](https://arxiv.org/pdf/2106.03232) which was discussed in the lecture. The data can be downloaded [here TODO]().\n",
    "\n",
    "The data provides human RTs and LM surprisals in different conditions. Here are examples from the conditions:\n",
    "* ORC: TODO\n",
    "* ...\n",
    "\n",
    "The prediction is that humans and models should have difficulty processing the mismatched noun, both in the singular and the plur condition.\n",
    "\n",
    "> Your task is to complete / provide the following code and answer the following questions:\n",
    "> 1. Formulate a quantitatively testable hypothesis operationalizing the prediction above. I.e., formulate something like: if the prediction is true, X should be larger than Y.\n",
    "> 2. Inspect the data. What are the units of the provided results?\n",
    "> 2. Based on your hypothesis above, for each trial, calculate whether it holds or not. Plot the proportion of trials where your hypothesis is borne out (i.e, the accuracy), for humans and each model, in the singular and the plural condition. (Hint: use a barplot)\n",
    "> 3. Based on visual inspection, does any model match human performance? \n",
    "> 3. Is either of the number conditions more difficult to process for humans or LMs?\n",
    "> 4. Select the results for Llama and humans only. Is the processing 'difficulty' of Llama correlated with the processing slowdown of humans (across singular / plural conditions)? Interpret the correlation coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/SVA_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### YOUR CODE HERE FOR CALCULATING HYPOTHESIS METRICS AND PLOTTING ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# barplot of the results, by model and by condition (plural vs. singular)\n",
    "### YOUR CODE HERE ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation analysis\n",
    "#### YOUR CODE HERE ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "understanding_llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
