{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 4: LLM evaluation\n",
    "=====\n",
    "\n",
    "The third homework zooms in on evaluating LLMs, specifically, on the following skills: using log probabilities of string under a trained LM to evaluate it, coming up with items to test particular aspets of LLMs, and comparing LLM measures to measures of human performance.\n",
    "\n",
    "### Logistics\n",
    "\n",
    "* submission deadline: July 13th th 23:59 German time via Moodle\n",
    "  * please upload a **SINGLE .IPYNB FILE named Surname_FirstName_HW4.ipynb** containing your solutions of the homework. Make sure that your **plots** for the last exercise are either rendered in the notebook or submitted together with it in a zip file. \n",
    "* please solve and submit the homework **individually**! \n",
    "* if you use Colab, to speed up the execution of the code on Colab, you can use the available GPU (if Colab resources allow). For that, before executing your code, navigate to Runtime > Change runtime type > GPU > Save."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Understanding grammatical capabilities of LLMs (10 points)\n",
    "\n",
    "In this task, we look at [BLiMP](https://aclanthology.org/2020.tacl-1.25/), the benchmark of linguistic minimal pairs. This is a well-known benchmark for evaluating linguistic capabilities of language models. It consists of 67 individual datasets, each containing 1,000 minimal pairs -- that is, pairs of minimally different sentences that contrast in grammatical acceptability and isolate specific phenomenon in syntax, morphology, or semantics. The authors suggest to use the benchmark to evaluate LMs by observing whether they assign a higher probability to the acceptable sentence in each minimal pair.\n",
    "\n",
    "> Your task is to evaluate an open-source model, [Pythia-160m](https://huggingface.co/EleutherAI/pythia-160m), on this benchmark by completing the code below. Based on your evaluation results, please answer the following questions.\n",
    "> Please use the following test suites to answer them: anaphor_gender_agreement, determiner_noun_agreement_with_adjective_1, animate_subject_passive, complex_NP_island, npi_present_1, superlative_quantifiers_1, existential_there_object_raising, principle_A_case_1.\n",
    "> \n",
    "> The entire benchmark can be found [here](https://huggingface.co/datasets/nyu-mll/blimp).\n",
    "> \n",
    "> 1. Plot the accuracy of the model on the different grammatical phenomena, represented in different test suites.\n",
    "> 2. Calculate the average accuracies and the confidence intervals in the different fields: syntax, morphology, syntax-semantics, semantics. Is the performance the same across the different fields? Which field is the most difficult one?\n",
    "> 3. What is the easiest grammatical phenomenon, what is the most difficult gramamtical phenomenon (as captured by the single test suites) for the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from minicons import scorer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over the test suites \n",
    "\n",
    "#### YOUR CODE HERE ####\n",
    "dataset = load_dataset(\"nyu-mll/blimp\", #### YOUR TEST SUITE HERE ####)\n",
    "# inspect the dataset\n",
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over the single items of the test suite\n",
    "# hint: you can use code similar to the one in sheet 7.1 \n",
    "\n",
    "# set up the model as a minicons scorer \n",
    "lm_scorer = scorer.IncrementalLMScorer(\n",
    "    ### YOUR CODE HERE ###\n",
    ")\n",
    "\n",
    "# create some lists to store the results\n",
    "### YOUR CODE HERE ###\n",
    "\n",
    "for item in dataset[\"train\"]:\n",
    "    # get the sentence pair\n",
    "    ### YOUR CODE HERE ###\n",
    "    \n",
    "    # compare the sentences as suggested in the task description\n",
    "    ### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the performance by test suite\n",
    "### YOUR CODE HERE ###\n",
    "# plot the results in a bar plot\n",
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the performance as described above by category and plot the results in a bar plot with CIs\n",
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Evaluating societal biases (13 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will consider aspects of LLM performance which may have social implications and that are deeply interconnected with how humans use language. This task evaluates whether LLMs overrepresent certain cultures over others, which could be due to, e.g., imbalances over training data sources and languages.\n",
    "\n",
    "> Specifically, your task is to come up with an appropriate test item and evaluate whether LLMs exhibit certain cultural biases. \n",
    "> In this task, you have to construct your own multiple-choice test item for investigating cultural biases of LLMs, where, given a context, the different available response / continuation options would reflect preferences for responses typical for different cultures. \n",
    "> For instance, one response could be more acceptable under one particular cultural lense and another response under a different cultural background. \n",
    "> Your task is then to evaluate the performance of two LLMs: the mostly monolingual `gpt2` and the multilingual `bigscience/bloom-560m` model. The second part of the task is to compelte the evaluation code and interpret the results by answering the question below.\n",
    "\n",
    "\n",
    "Here is a simple example of a test item. More explanations are in parentheses. You should provide analogous explanations in the answers to the questions below, but not pass these to the LLMs during evaluations.\n",
    "\n",
    "\n",
    "Context 1: You are at a German supermarket. You walk up to the cashier and greet them by saying:\n",
    "\n",
    "Context 2: You are at an American supermarket. You walk up to the cashier and greet them by saying:\n",
    "\n",
    "A. Hello. (intuititvely, more likely in to be appropiate in the Germany context condition)\n",
    "\n",
    "B. Bye. (a generally inappropriate response)\n",
    "\n",
    "C. Hello, how are you? (intuitively, more likely to be appropriate in the US context condition; people usually don’t ask strangers ‘how are you’ in Germany)\n",
    "\n",
    "I would say: (insert each of the answer options separately here and calculate their log probability, given each of the contexts).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference about constructing datasets and inspiration, feel free to take a look at the [ETHICS dataset](https://arxiv.org/pdf/2008.02275), e.g., Fig. 2, where the authors came up with different continuations tapping into different conditions, given a context.\n",
    "\n",
    "> **Fill in your responses below.**\n",
    ">\n",
    "> 1. Your prompt (with explanations of the intuitive differences for each response option in respective cultural variations):\n",
    "> 2. Your model log probabilities (table cells are examples, please fill in with your respective item):\n",
    ">\n",
    "| Context / Option | GPT-2 | Bloom |\n",
    "|------------------|-------|-------|\n",
    "| Germany + A      | ...   | ...   |\n",
    "| USA + A          |       |       |\n",
    "| Germany + B      |       |       |\n",
    "| USA + B          |       |       |\n",
    "| ...              |       |       |\n",
    "\n",
    "> 3. Do the models show a preference for a particular cultural setting? Is there evidence for whether cultural biases might be caused by training data?\n",
    "> 4. Are there aspects of the prompt that might influence your results? Please provide a brief justification / example why (not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minicons import scorer \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is some starter code; please fill in your code / comments where it says #### YOUR CODE / COMMENT HERE ####\n",
    "\n",
    "# set up a scorer \n",
    "gpt2_scorer = scorer.IncrementalLMScorer(\n",
    "    ### YOUR CODE HERE ###\n",
    ")\n",
    "\n",
    "bloom_scorer = scorer.IncrementalLMScorer(\n",
    "    ### YOUR CODE HERE ###\n",
    ")\n",
    "# initialize list for storing the predictions\n",
    "gpt2_predictions = []\n",
    "bloom_predictions = []\n",
    "answer_keys = [\"ger\", \"nonsense\", \"us\"]\n",
    "\n",
    "# iterate over contexts\n",
    "for context in range(### YOUR CODE HERE ###):\n",
    "    # format / provide  the possible answer options from your vignette\n",
    "    answer_options = ### YOUR CODE HERE ###\n",
    "    # pass a list of contexts and a list of continuations to be scored\n",
    "    answer_scores_gpt2 = gpt2_scorer.conditional_score(\n",
    "        # format the question into a list of same length as the number of answer options\n",
    "        ### YOUR CODE HERE ###,\n",
    "    ) \n",
    "    answer_scores_bloom = bloom_scorer.conditional_score(\n",
    "        # format the question into a list of same length as the number of answer options\n",
    "        ### YOUR CODE HERE ###,\n",
    "    )\n",
    "    \n",
    "    # check / inspect which answer has the highest score and which answer type (i.e., \"culture\") it corresponds to\n",
    "    ### YOUR CODE / COMMENT HERE ### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: LLM evaluations with LLMs (5 points)\n",
    "\n",
    "Building on the in-context learning capabilities of LLMs,, recent work, e.g., by [Perez et al (2022)](https://arxiv.org/abs/2212.09251), has been *using LLMs to generate evaluation datasets for LLMs*. \n",
    "\n",
    "> Your task here is to: \n",
    "> 1. write a pseudo-algorithm for generating more cultural bias evaluation items. The items should be of a similar structure as in the task above. Write maximally 5 steps. (Hint: feel free to try to elicit e.g. 10 different item with a model of your choice)\n",
    "> 2. What could be possible concerns with this approach? Name and briefly explain 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: How human-like are Llama's surprisals? (22 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More recently, work more informed by human language use and processing has compared LLMs’ performance to aspects of human behavior. Here, the assessment of LLMs is guided more by the question of how human-like certain aspects of its performance are. For instance, we might whether LLMs' 'knowledge' of language is comparable to human knowledge, and, in particular, whether the processing of language, given the knowledge , can be compared via system-appropriate linking measurements.\n",
    "\n",
    "Your task in this exercise is to assess whether the *surprisal* of different language models is comparable to human *reading times*, when it comes to processing subject-verb agreement. The linking hypothesis is that these can be considered the respective predictability, and therefore, processing load indicators.\n",
    "The conceptual ideas and the data are taken from [Wilcox et al. (2021)](https://arxiv.org/pdf/2106.03232) which was discussed in the lecture. Please read the sections 1-2.2 for the background (optionally, the rest, if you want).\n",
    "The data can be downloaded [here](https://github.com/CogSciPrag/Understanding-LLMs-course/tree/main/understanding-llms/homework/data/SVA_data.csv).\n",
    "\n",
    "The data provides human RTs and LM surprisals in different conditions for sentences where the subject and the verb either match (i.e., agree) or mismatch in terms of number. This is the main condition. Furthermore, the agreement manipulation occurs in different syntactic conditions, and for plural and singular nouns. Here are examples from the different syntactic conditions:\n",
    "* SRC (subject relative clause modifier):\n",
    "  * mismatch plural: The pilots that injured the teacher brings love to people.\n",
    "  * match plural: The pilots that injured the teacher bring love to people.\n",
    "* ORC (object relative clause modifier):\n",
    "  * mismatch plural: The ministers that the manager injured knows tennis.\n",
    "  * match plural: The ministers that the manager injured know tennis.\n",
    "* PP (prepositional phrase modifier):\n",
    "  * mismatch plural: The executives next to the teacher is good.\n",
    "  * match plural: The executives next to the teacher are good.\n",
    "\n",
    "The prediction is that humans and models should have difficulty processing the mismatched noun, both in the singular and the plural condition.\n",
    "\n",
    "> Your task is to complete / provide the following code and answer the following questions:\n",
    "> 1. Formulate a quantitatively testable hypothesis operationalizing the prediction above. I.e., formulate something like: if the prediction is true, X should be larger than Y.\n",
    "> 2. Provide respective examples for the singular condition.\n",
    "> 3. Inspect the data. What are the units of the provided results?\n",
    "> 4. Based on your hypothesis above, for each trial, calculate whether it holds or not. Plot the proportion of trials where your hypothesis is borne out (i.e, the accuracy), for humans and each model, in the singular and the plural condition. (Hint: use a barplot)\n",
    "> 5. Based on visual inspection, does any model match human performance? \n",
    "> 6. Is either of the number conditions more difficult to process for humans or LMs?\n",
    "> 7. Select the results for Llama and humans only. Is the processing 'difficulty' of Llama correlated with the processing slowdown of humans (across singular / plural conditions)? Interpret the correlation coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/SVA_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### YOUR CODE HERE FOR CALCULATING HYPOTHESIS METRICS AND PLOTTING ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# barplot of the results, by model and by condition (plural vs. singular)\n",
    "### YOUR CODE HERE ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation analysis\n",
    "#### YOUR CODE HERE ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "understanding_llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
