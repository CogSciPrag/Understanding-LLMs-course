{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting LLM fingerprints\n",
    "\n",
    "For this task, your job is to extract the \"fingerprint\" of a state-of-the-art large language model from the paper. Specifically, you job is to find out the following charactersitcs of the model and submit a json file with your responses in the following format (below is a partial example). Note that, of course, it might be that some information is not available or that some categories are not applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"model_name\": \"GPT-35\",\n",
    "    \"huggingface_model_id\": \"gpt35\",\n",
    "    \"paper_url\": \"https://arxiv.org/abs/XXX\",\n",
    "    \"tokenizer_type\": \"BPE\",\n",
    "    \"vocabulary_size\": \"XXX\",\n",
    "    \"architecture\": \"Mixture of transformer agents\",\n",
    "    \"architecture_type\": \"decoder only\",\n",
    "    \"architecture_quirks\": [\n",
    "        \"sparse attention\", \n",
    "        \"...\",\n",
    "    ],\n",
    "    \"parameters\": \"XXX\",\n",
    "    \"finetuning_type\": \"RLHF\",\n",
    "    \"training_data_cutoff\": \"2050\",\n",
    "    \"number_training_tokens\": \"XXX\",\n",
    "    \"pretraining_data_size\": \"1GB\",\n",
    "    \"finetuning_data_size\": \"XXX\",\n",
    "    \"training_data\": [\n",
    "        \"Books corpus\",\n",
    "        \"Twitter\",\n",
    "        \"...\"\n",
    "    ],\n",
    "    \"finetuning_data\": [\n",
    "        \"XXX\",\n",
    "        \"XXX\",\n",
    "        \"...\"\n",
    "    ],\n",
    "    \"access\": \"open\",\n",
    "    \"summary\": \"A few sentences of what the model claims to be their unique selling point / main contribution\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of models (maybe each student could even get their own model :'D):\n",
    "\n",
    "* phi-2\n",
    "* mixtral, mixtral-instruct\n",
    "* mistral, mistral-instruct v2\n",
    "* mamba\n",
    "* jamba (yes i'm not joking)\n",
    "* llama-2 suite\n",
    "* llama-3 (8b, 70b)\n",
    "* gpt-3\n",
    "* gemini 1.5 (the whole suite is multimodal though)\n",
    "* claude (there seems to only be a technical report, not sure how many details)\n",
    "* palm-2\n",
    "* Bloom (might be interesting because it's multilingual)\n",
    "* Grok 1.5 (not sure there is an actual paper)\n",
    "* Vicuna (blog not paper)\n",
    "* Falcon 40b\n",
    "* Gemma \n",
    "* DBRX\n",
    "* cmd-r+ (rag integrated, other provider (cohere))\n",
    "* \n",
    "not sure about availability of any resources for the following:\n",
    "* h2o danube\n",
    "* BitNet\n",
    "* JetMoE-8b\n",
    "* qwen-1.5-MoE\n",
    "* wizardLM\n",
    "\n",
    "For many of them there are different sizes and different generations. I've mostly listed the latest ones (at least latest to my knowledge at the moment of committing this list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "understanding_llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
