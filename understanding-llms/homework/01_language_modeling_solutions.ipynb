{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8GQLg7lp6PG"
   },
   "source": [
    "# Homework 1: Language models (50 points)\n",
    "\n",
    "The first homework focuses on the following skills: being able to work with simple formal exercises on language modeling, on understanding and being eable to extract properties and configurations of state-of-the-art language models and, finally, conducting language model training yourself!\n",
    "\n",
    "### Logistics\n",
    "\n",
    "* submission deadline: May 12th 23:59 German time via Moodle\n",
    "  * please upload a **SINGLE ZIP FILE named Surname_FirtName_HW1.zip** containing the .ipynb file of the notebook (if you solve it on Colab, you can go to File > download) and the json file.\n",
    "* please solve and submit the homework **individually**!\n",
    "* if you use Colab, to speed up the execution of the code on Colab (especially Exercise 3), you can use the available GPU (if Colab resources allow). For that, before executing your code, navigate to Runtime > Change runtime type > GPU > Save."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVt4vgNhp6PV"
   },
   "source": [
    "## Exercise 1: Understanding language modeling (12 points)\n",
    "\n",
    "Please answer the following exercises. Importantly, please reason step by step; i.e., where calculations are required, please provide intermediate steps of how you arrived at your solution. You do not need to write any code, just mathematical solutions.\n",
    "\n",
    "> 1. [6pts] Consider the corpus $C$ with the following sentences: $C=${\"The cat sleeps\", \"The mouse sings\", \"The cat sleeps\", \"A dog sings\"}. \n",
    "> (a) Define the vocabulary $V$ of this corpus (assuming by-word tokenization).\n",
    "> (b) Pick one of the four sentences in $C$. Formulate the probability of that sentence in the form of the chain rule. Calculate the probability of each termn in the chain rule, given the corpus.\n",
    "> 2. [4pts] We want to train a neural network that takes as input two numbers $x_1, x_2$, passes them through three hidden linear layers, each with 13 neurons, each followed by the ReLU activation function, and outputs three numbers $y_1, y_2, y_3$. Write down all weight matrices of this network with their dimensions. (Example: if one weight matrix has the dimensions 3x5, write $M_1\\in R^{3\\times5}$) \n",
    "> 3. [2pts] Consider the sequence: \"Input: Some students trained each language model\". Assuming that each word+space/punctuation corresponds to one token, consider the following token probabilities of this sequence under some trained language model: $p = [0.67, 0.91, 0.83, 0.40, 0.29, 0.58, 0.75]$. Compute the average surprisal of this sequence under that language model. [Note: in this class we always assume the base $e$ for $log$, unless indicated otherwise. This is also usually the case throughout NLP.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer Exercise 1: Understanding language modeling (12 points)\n",
    "Note: The usage of BOS/EOS is not typical for theses models. Nonetheless both practices are correct.\n",
    "> #### 1. First Possibility\n",
    ">    a. vocabulary $V$= {'the', 'cat', 'sleeps', 'mouse', 'sings','a', 'dog'}\n",
    ">\n",
    ">    b. The cat sleeps:\n",
    ">\\begin{align*} \n",
    "P_{LM} = &\\prod^n_{n=1}P_{LM}(w_i|w_{1:i-1})\\\\\n",
    "\\end{align*}\n",
    ">$P_{LM}(the,cat,sleeps)= p(the)*p(cat| the)*p(sleeps∣the,cat)$\n",
    ">\\begin{align*}\n",
    "P_{LM}(the,cat,sleeps)=&\\frac{3}{12}*\\frac{2}{3}*\\frac{2}{2}= \\frac{1}{4}*\\frac{2}{3}=\\frac{2}{12}= \\frac{1}{6}= 0.167\\\\\n",
    "\\end{align*}\n",
    ">   The mouse sings:\n",
    "\\begin{align*}\n",
    "P_{LM} = \\prod^n_{n=1}P_{LM}(w_i|w_{1:i-1})\n",
    "\\end{align*}\n",
    ">$P_{LM}(the,mouse,sings)=p(the)*p(mouse∣the)*p(sings∣the,mouse)$\n",
    "\\begin{align*}\n",
    "&P_{LM}(the,mouse,sings)=\\frac{3}{12}*\\frac{1}{3}*\\frac{1}{2}=\\frac{1}{4}*\\frac{1}{3}*\\frac{1}{2}= \\frac{1}{24}= 0.125\\\\\n",
    "\\end{align*}   \n",
    ">   A dog sings:\n",
    "\\begin{align*} \n",
    "&P_{LM} = \\prod^n_{n=1}P_{LM}(w_i|w_{1:i-1})\\\\\n",
    "\\end{align*}\n",
    "$P_{LM}(a,dog,sings)=p(a)*p(dog∣a)*p(sings∣a, dog)$\n",
    "\\begin{align*}\n",
    "&P_{LM}(a, dog,sings)=\\frac{1}{12}*\\frac{1}{1}*\\frac{1}{1}= \\frac{1}{12}= 0.083\\\\\n",
    "\\end{align*}            \n",
    ">\n",
    "#### 1. Second Possibility\n",
    "> 1. a. vocabulary $V$= {'<BOS>','the', 'cat', 'sleeps', 'mouse', 'sings','a', 'dog''<EOS>'}\n",
    ">\n",
    ">    b. The cat sleeps:\n",
    ">\\begin{align*} \n",
    "P_{LM} = &\\prod^n_{n=1}P_{LM}(w_i|w_{1:i-1})\\\\\n",
    "\\end{align*}\n",
    ">$P_{LM}(<BOS>, the,cat,sleeps, <EOS>)= p(the|<BOS>)*p(cat|<BOS>, the)*p(sleeps∣<BOS>,the,cat)$*p(<EOS>∣<BOS>,the,cat, sleeps)$\n",
    ">\\begin{align*}\n",
    "P_{LM}(<BOS>, the,cat,sleeps, <EOS>)=&\\frac{3}{4}*\\frac{2}{3}*\\frac{2}{2}*\\frac{2}{2}= \\frac{6}{12} = \\frac{1}{2}= 0.5\\\\\n",
    "\\end{align*}\n",
    ">   The mouse sings:\n",
    "\\begin{align*}\n",
    "P_{LM} = \\prod^n_{n=1}P_{LM}(w_i|w_{1:i-1})\n",
    "\\end{align*}\n",
    ">$P_{LM}(<BOS>,the,mouse,sings,<EOS>)=p(the|<BOS>)*p(mouse∣<BOS>,the)*p(sings∣<BOS>,the,mouse)$*p(<EOS> ∣<BOS>,the,mouse, sings, <EOS>)$\n",
    "\\begin{align*}\n",
    "&P_{LM}(<BOS>,the,mouse, sings, <EOS>)=\\frac{3}{4}*\\frac{1}{3}*\\frac{1}{1}*\\frac{1}{1}=\\frac{3}{12}= \\frac{1}{4} = 0.25\\\\\n",
    "\\end{align*}   \n",
    ">   A dog sings:\n",
    "\\begin{align*} \n",
    "&P_{LM} = \\prod^n_{n=1}P_{LM}(w_i|w_{1:i-1})\\\\\n",
    "\\end{align*}\n",
    "$P_{LM}(<BOS>,a,dog,sings,<EOS>)=p(a|<BOS>)*p(dog∣<BOS>,a)*p(sings∣<BOS>,a, dog)$*p(sings∣<BOS>,a, dog, sings,<EOS>)$\n",
    "\\begin{align*}\n",
    "&P_{LM}(<BOS>,a, dog,sings,<EOS>)=\\frac{1}{4}*\\frac{1}{1}*\\frac{1}{1}*\\frac{1}{1}= \\frac{1}{4}= 0.25\\\\\n",
    "\\end{align*}            \n",
    ">\n",
    "### 1.1 evaluation\n",
    "\n",
    "* vocabulary correct = 2 points\n",
    "* calculation correct = 4 points\n",
    "* calculation incorrect, but P_{LM} formula correct = 2 points\n",
    "* calculation and formula incorrect but idea understandable = 1 point\n",
    "\n",
    "deduction:\n",
    "* vocabulary with minor mistake = -1 point\n",
    "* caculation correct for one alternative but formula of the other alterative = -1 point\n",
    "_____________________________________________________________\n",
    "> 2. input_neurons = 2\n",
    ">\n",
    ">    hidden_neurons = 13\n",
    ">\n",
    ">    output_neurons = 3\n",
    ">\n",
    ">    $M_1 ϵ R^{2x13}$ input to hidden1\n",
    ">\n",
    ">    $b_1 ϵR^{1x13}$ weights1\n",
    ">\n",
    ">    $M_2 ϵ R^{13x13}$ hidden1 to hidden2\n",
    ">\n",
    ">    $b_2 ϵ R^{1x13}$ weights2\n",
    ">\n",
    ">    $M_3 ϵ R^{13x13}$ hidden2 to hidden3\n",
    ">\n",
    ">    $b_3 ϵ R^{1x13}$ weights3\n",
    ">\n",
    ">    $M_4 ϵ R^{13x3}$ hidden3 to output\n",
    ">\n",
    ">    $b_4 ϵ R^{1x13}$ weights4\n",
    ">\n",
    ">\n",
    ">\n",
    "### 1.2 evaluation\n",
    "\n",
    "* Matrizes $M_i$ from layer to layer correct = 3 points\n",
    "* comment or definition of bias weights $b_i$ = 1 points\n",
    "* Matrizes incorrect, but correct right amount of matrizes = 2 points\n",
    "* Amount of matrizes $M_i$ incorrect, but matrizes layer correct = 2 points (if additionally $b_i += 1 point)\n",
    "\n",
    "_____________________________________________________________\n",
    "> 3. Sentence = 'Input: Some students trained each language model'\n",
    ">\n",
    ">    $p=[0.67,0.91,0.83,0.40,0.29,0.58,0.75]$\n",
    ">    \\begin{align*}\n",
    ">    P_{LM} = &\\prod^n_{n=1}P_{LM}(w_i|w_{1:i-1})= 0.67*0.91*0.83*0.40*0.29*0.58*0.75=0.0255\\\\\n",
    ">    \\end{align*}\n",
    ">    \n",
    ">    $\\text{Avg-Surprisal}_{LM}(w_{1:n}) =-\\frac{1}{n}logP_{LM}(w_{1:n})$\n",
    "                                        $= -\\frac{1}{7}log(0.2553533346)=-\\frac{1}{7}*-0.59 = 0.523$\n",
    ">    \n",
    ">    \n",
    "### 1.3 evaluation\n",
    "\n",
    "* calculation correct = 2 points\n",
    "\n",
    "deduction:\n",
    "* caluclation incorrect, but P_{LM} formula correct or calculation only minor mistake = 1 points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WV4uiBQ5p6PZ"
   },
   "source": [
    "## Exercise 2: Extracting LLM fingerprints (15 points)\n",
    "\n",
    "For this task, your job is to extract the \"fingerprint\" of a state-of-the-art large language model from the paper. Specifically, you job is to:\n",
    "* find the model that is assigned to your surname in the list **HW1_Model2Group_assignment.csv** (to be found on Moodle under topic 02). Please investigate the latest version of your model, unless the version is specified in the list.\n",
    "* submit a json file with your responses in the following format (below is a partial example).\n",
    "\n",
    "Note that, of course, it might be that some information is not available or that some categories are not applicable. The idea is, that, as a course we can create a fun website which will show a somewhat comprehensive graphical comparison of current language models and their configurations. Based on your collective json files, the lecturers will set up a front end at some point during the class.\n",
    "\n",
    "**IMPORTANT**: Please email the lecturers by the homework deadline if you DO NOT consent that your json file is used for this idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kkd6zDxBp6Pd",
    "outputId": "3f2b5da5-b904-4d16-8234-75127b265eff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'GPT-35',\n",
       " 'huggingface_model_id': 'gpt35',\n",
       " 'paper_url': 'https://arxiv.org/abs/XXX',\n",
       " 'tokenizer_type': 'BPE',\n",
       " 'vocabulary_size': 'XXX',\n",
       " 'architecture': 'Mixture of transformer agents',\n",
       " 'architecture_type': 'decoder only',\n",
       " 'architecture_quirks': ['sparse attention', '...'],\n",
       " 'parameters': 'XXX',\n",
       " 'finetuning_type': 'RLHF',\n",
       " 'training_data_cutoff': '2050',\n",
       " 'number_training_tokens': 'XXX',\n",
       " 'pretraining_data_size': '1GB',\n",
       " 'finetuning_data_size': 'XXX',\n",
       " 'training_data': ['Books corpus', 'Twitter', '...'],\n",
       " 'finetuning_data': ['XXX', 'XXX', '...'],\n",
       " 'access': 'open',\n",
       " 'summary': 'A few sentences of what the model claims to be their unique selling point / main contribution'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    \"model_name\": \"GPT-35\",\n",
    "    \"huggingface_model_id\": \"gpt35\",\n",
    "    \"paper_url\": \"https://arxiv.org/abs/XXX\",\n",
    "    \"tokenizer_type\": \"BPE\",\n",
    "    \"vocabulary_size\": \"XXX\",\n",
    "    \"architecture\": \"Mixture of transformer agents\",\n",
    "    \"architecture_type\": \"decoder only\",\n",
    "    \"architecture_quirks\": [\n",
    "        \"sparse attention\",\n",
    "        \"...\",\n",
    "    ],\n",
    "    \"parameters\": \"XXX\",\n",
    "    \"finetuning_type\": \"RLHF\",\n",
    "    \"training_data_cutoff\": \"2050\",\n",
    "    \"number_training_tokens\": \"XXX\",\n",
    "    \"pretraining_data_size\": \"1GB\",\n",
    "    \"finetuning_data_size\": \"XXX\",\n",
    "    \"training_data\": [\n",
    "        \"Books corpus\",\n",
    "        \"Twitter\",\n",
    "        \"...\"\n",
    "    ],\n",
    "    \"finetuning_data\": [\n",
    "        \"XXX\",\n",
    "        \"XXX\",\n",
    "        \"...\"\n",
    "    ],\n",
    "    \"access\": \"open\",\n",
    "    \"summary\": \"A few sentences of what the model claims to be their unique selling point / main contribution\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGg98OH6p6Pg"
   },
   "source": [
    "List of models (maybe each student could even get their own model :'D). Mapping to names TBD.\n",
    "\n",
    "* phi-2\n",
    "* mixtral, mixtral-instruct\n",
    "* mistral, mistral-instruct v2\n",
    "* mamba\n",
    "* jamba (yes i'm not joking)\n",
    "* llama-2 suite\n",
    "* llama-3 (8b, 70b)\n",
    "* gpt-3\n",
    "* gemini 1.5 (the whole suite is multimodal though)\n",
    "* claude (there seems to only be a technical report, not sure how many details)\n",
    "* palm-2\n",
    "* Bloom (might be interesting because it's multilingual)\n",
    "* Grok 1.5 (not sure there is an actual paper)\n",
    "* Vicuna (blog not paper)\n",
    "* Falcon 40b\n",
    "* Gemma\n",
    "* DBRX\n",
    "* cmd-r+ (rag integrated, other provider (cohere))\n",
    "* h2o danube\n",
    "* BitNet\n",
    "* JetMoE-8b\n",
    "* qwen-1.5-MoE\n",
    "* wizardLM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yL84t1GAp6Pj"
   },
   "source": [
    "## Exercise 3: Fine-tuning GPT-2 for QA \\(23 points)\n",
    "\n",
    "The learning goal of this exercise is to practice fine-tuning a pretrained LM, GPT-2 small, for a particular task, namely commonsense question answering (QA). We will use a task-specific dataset, [CommonsenseQA](https://huggingface.co/datasets/tau/commonsense_qa), that was introduced by [Talmor et al. (2018)](https://arxiv.org/abs/1811.00937). We will evaluate the performance of the model on our test split of the dataset over training to monitor whether the model's performance is improving and compare the performance of the base pretrained GPT-2 and the fine-tuned model. We will need to perform the following steps:\n",
    "\n",
    "1. Prepare data according to steps described in [sheet 1.1](https://cogsciprag.github.io/Understanding-LLMs-course/tutorials/01-introduction.html#main-training-data-processing-steps)\n",
    "   1. additionally to these steps, prepare a custom Dataset (like in [sheet 2.3](https://cogsciprag.github.io/Understanding-LLMs-course/tutorials/02c-MLP-pytorch.html#preparing-the-training-data)) that massages the dataset from the format that it is shipped in on HuggingFace into strings that can be used for training. Some of the procesing steps will happen in the Dataset.\n",
    "2. Load the pretrained GPT-2 model\n",
    "3. Set up training pipeline according to steps described in [sheet 2.5]()\n",
    "4. Run the training while tracking the losses\n",
    "5. Save plot of losses for submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmZao_7cp6Pn"
   },
   "source": [
    "Your tasks:\n",
    "> 1. [19pts] Complete the code in the spots where there is a comment \"#### YOUR CODE HERE ####\". There are instructions in the comments as to what the code should implement. With you completed code, you should be able to let the training run without errors. Note that the point of the exercise is the implementation; we should not necessarily expect great performance of the fine-tuned model (and the actual performance will not be graded). Often there are several correct ways of implementing something. Anything that is correct will be accepted.\n",
    "> 2. [4pts] Answer questions at the end of the execise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pXRkq7aHp6Pp",
    "outputId": "0688088a-76a0-435d-cf6e-2b38ee6c83d7"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-3-66d1a2d3ccf0>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;31m# as we did in Sheet 1.1. Otherwise, don't forget to activate your local environment\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mdatasets\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mload_dataset\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtransformers\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mAutoTokenizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mAutoModelForCausalLM\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "# note: if you are on Colab, you might need to install some requirements\n",
    "# as we did in Sheet 1.1. Otherwise, don't forget to activate your local environment\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Flsi4qTHp6Pt"
   },
   "outputs": [],
   "source": [
    "# additioanlly, we need to install accelerate\n",
    "# uncomment and run the following line on Colab or in your environment\n",
    "# !pip install accelerate\n",
    "# NOTE: in a notebook, reloading of the kernel might be required after installation if you get dependency errors with the transformers package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_tvAZOVpp6Pz"
   },
   "outputs": [],
   "source": [
    "### 1. Prepare data with data prepping steps from sheet 1.1\n",
    "\n",
    "# a. Acquiring data\n",
    "# b. (minimally) exploring dataset\n",
    "# c. cleaning / wrangling data (combines step 4 from sheet 1.1 and step 1.1 above)\n",
    "# d. splitting data into training and test set (we will not do any hyperparam tuning)\n",
    "# (we don't need further training set wrangling)\n",
    "# e. tokenizing data and making sure it can be batched (i.e., conversted into 2d tensors)\n",
    "# this will also happen in our custom Dataset class (common practice when working with text data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F-12eVbBp6P4"
   },
   "outputs": [],
   "source": [
    "# downaload dataset from HF\n",
    "dataset = load_dataset(\"tau/commonsense_qa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cLIq81gsp6P9"
   },
   "outputs": [],
   "source": [
    "# inspect dataset\n",
    "print(dataset.keys())\n",
    "# print a sample from the  dataset\n",
    "### YOUR CODE HERE ####\n",
    "print(dataset[\"train\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lHLPN0mPp6P_"
   },
   "source": [
    "Note that the test split does not have ground truth answer labels. Therefore, we will use the validation split as our test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JQKwdAPJp6QA"
   },
   "outputs": [],
   "source": [
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# set padding side to be left because we are doing causal LM\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NwX7gCFrp6QD"
   },
   "outputs": [],
   "source": [
    "def massage_input_text(example):\n",
    "    \"\"\"\n",
    "    Helper for converting input examples which have \n",
    "    a separate qquestion, labels, answer options\n",
    "    into a single string.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    example: dict\n",
    "        Sample input from the dataset which contains the \n",
    "        question, answer labels (e.g. A, B, C, D),\n",
    "        the answer options for the question, and which \n",
    "        of the answers is correct.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    input_text: str\n",
    "        Formatted training text which contains the question,\n",
    "        the forwatted answer options (e.g., 'A. <option 1> B. <option 2>' etc)\n",
    "        and the ground truth answer.\n",
    "    \"\"\"\n",
    "    # combine each label with its corresponding text\n",
    "    answer_options_list = list(zip(\n",
    "        example[\"choices\"][\"label\"],\n",
    "        example[\"choices\"][\"text\"]\n",
    "    ))\n",
    "    # join each label and text with . and space\n",
    "    ### YOUR CODE HERE ####\n",
    "    answer_options = [f\"{label}. {text}\" for label, text in answer_options_list]\n",
    "    # join the list of options with spaces into single string\n",
    "    ### YOUR CODE HERE ####\n",
    "    answer_options_string = \" \".join(answer_options)\n",
    "    # combine question and answer options\n",
    "    input_text = example[\"question\"] + \" \" + answer_options_string\n",
    "    # append the true answer with a new line, \"Answer: \" and the label\n",
    "    input_text += \"\\nAnswer: \" + example[\"answerKey\"]\n",
    "\n",
    "    return input_text\n",
    "\n",
    "# process input texts of train and test sets\n",
    "massaged_datasets = dataset.map(\n",
    "    lambda example: {\n",
    "        \"text\": massage_input_text(example)\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eVwt31FQp6QE"
   },
   "outputs": [],
   "source": [
    "# inspect a sample from our preprocessed data\n",
    "massaged_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k98dx8UOp6QJ"
   },
   "outputs": [],
   "source": [
    "class CommonsenseQADataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset class for CommonsenseQA dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            train_split,\n",
    "            test_split,\n",
    "            tokenizer,\n",
    "            max_length=64,\n",
    "            dataset_split=\"train\",\n",
    "        ) -> None:\n",
    "        \"\"\"\n",
    "        fill me.\n",
    "        \"\"\"\n",
    "        self.train_split = train_split['text']\n",
    "        self.test_split = test_split['text']\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.dataset_split = dataset_split\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Method returning the length of the training dataset.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ####\n",
    "        return len(self.train_split)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Method returning a single training example.\n",
    "        Note that it also tokenizes, truncates or pads the input text.\n",
    "        Further, it creates a mask tensor for the input text which \n",
    "        is used for causal masking in the transformer model.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        idx: int\n",
    "            Index of training sample to be retrieved from the data.\n",
    "        \n",
    "        Returns\n",
    "        --------\n",
    "        tokenized_input: dict\n",
    "            Dictionary with input_ids (torch.Tensor) and an attention_mask\n",
    "            (torch.Tensor).\n",
    "        \"\"\"        # retrieve a training sample at the specified index idx\n",
    "        # HINT: note that this might depend on self.dataset_split\n",
    "        ### YOUR CODE HERE ####\n",
    "        input_text = self.train_split[idx]\n",
    "        tokenized_input = self.tokenizer(\n",
    "            input_text,\n",
    "            ### YOUR CODE HERE ####\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        tokenized_input[\"attention_mask\"] = (tokenized_input[\"input_ids\"] != tokenizer.pad_token_id).long()\n",
    "        return tokenized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HA-2He2xp6QL"
   },
   "outputs": [],
   "source": [
    "# move to accelerated device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Device: {device}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(f\"Device: {device}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w0IcMfKpp6QQ"
   },
   "outputs": [],
   "source": [
    "# 2. init model\n",
    "\n",
    "# load pretrained gpt2 for HF\n",
    "### YOUR CODE HERE ####\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "# print num of trainable parameters\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q1lu2-6op6QT"
   },
   "outputs": [],
   "source": [
    "# 3. set up configurations required for the training loop\n",
    "\n",
    "# instantiate dataset\n",
    "train_dataset = CommonsenseQADataset(\n",
    "    ### YOUR CODE HERE ####\n",
    "    massaged_datasets[\"train\"],\n",
    "    massaged_datasets[\"test\"],\n",
    "    tokenizer\n",
    ")\n",
    "# instantiate test dataset with the downloaded commonsense_qa data\n",
    "test_dataset = CommonsenseQADataset(\n",
    "    ### YOUR CODE HERE ####,\n",
    "    massaged_datasets[\"train\"],\n",
    "    massaged_datasets[\"test\"],\n",
    "    tokenizer,\n",
    "    dataset_split=\"test\"\n",
    ")\n",
    "# create a DataLoader for the dataset\n",
    "# the data loader will automatically batch the data\n",
    "# and iteratively return training examples (question answer pairs) in batches\n",
    "dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n",
    "# create a DataLoader for the test dataset\n",
    "# reason for separate data loader is that we want to\n",
    "# be able to use a different index for retreiving the test batches\n",
    "# we might also want to use a different batch size etc.\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XciLLP29p6QV"
   },
   "outputs": [],
   "source": [
    "# 4. run the training of the model\n",
    "# Hint: for implementing the forward pass and loss computation, carefully look at the exercise sheets \n",
    "# and the links to examples in HF tutorials.\n",
    "\n",
    "# put the model in training mode\n",
    "model.train()\n",
    "# move the model to the device (e.g. GPU)\n",
    "model = model.to(device)\n",
    "\n",
    "# trianing configutations\n",
    "# feel free to play around with these\n",
    "epochs  = 2\n",
    "train_steps =  len(train_dataset) // 32\n",
    "print(\"Number of training steps: \", train_steps)\n",
    "# number of test steps to perform every 10 training steps\n",
    "# (smaller that the entore test split for reasons of comp. time)\n",
    "num_test_steps = 5\n",
    "\n",
    "# define optimizer and learning rate\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4) ### YOUR CODE HERE ###\n",
    "# define some variables to accumulate the losses\n",
    "losses = []\n",
    "test_losses = []\n",
    "\n",
    "# iterate over epochs\n",
    "for e in range(epochs):\n",
    "    # iterate over training steps\n",
    "    for i in tqdm(range(train_steps)):\n",
    "        # get a batch of data\n",
    "        x = next(iter(dataloader))\n",
    "        # move the data to the device (GPU)\n",
    "        ### YOUR CODE HERE ###\n",
    "        x = x.to(device)\n",
    "\n",
    "        # forward pass through the model\n",
    "        ### YOUR CODE HERE ###\n",
    "        outputs = model(\n",
    "            **x,\n",
    "            labels=x[\"input_ids\"]\n",
    "        )\n",
    "        # get the loss\n",
    "        ### YOUR CODE HERE ###\n",
    "        loss = outputs.loss\n",
    "        # backward pass\n",
    "        ### YOUR CODE HERE ###\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "        # update the parameters of the model\n",
    "        ### YOUR CODE HERE ###\n",
    "        optimizer.step()\n",
    "        # zero out gradient for next step\n",
    "        ### YOUR CODE HERE ###\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # evaluate on test set every 10 steps\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Epoch {e}, step {i}, loss {loss.item()}\")\n",
    "            # TODO\n",
    "            # track test loss for the evaluation iteration\n",
    "            test_loss = 0\n",
    "            for j in range(num_test_steps):\n",
    "                # get test batch\n",
    "                x_test = next(iter(test_dataloader))\n",
    "                x_test = x_test.to(device)\n",
    "                with torch.no_grad():\n",
    "                    test_outputs = model(\n",
    "                        **x_test,\n",
    "                        labels=x_test[\"input_ids\"]\n",
    "                        ### YOUR CODE HERE ####\n",
    "                    )\n",
    "                test_loss += test_outputs.loss### YOUR CODE HERE ####\n",
    "\n",
    "            test_losses.append(test_loss / num_test_steps)\n",
    "            print(\"Test loss: \", test_loss/num_test_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lkuo9ca1p6QZ"
   },
   "outputs": [],
   "source": [
    "# Plot the fine-tuning loss\n",
    "### YOUR CODE HERE ###\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Training steps\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b1_EGhSop6Qa"
   },
   "outputs": [],
   "source": [
    "# print a few predictions on the eval dataset to see what the model predicts\n",
    "\n",
    "# construct a list of questions without the ground truth label\n",
    "# and compare prediction of the model with the ground truth\n",
    "\n",
    "def construct_test_samples(example):\n",
    "    \"\"\"\n",
    "    Helper for converting input examples which have \n",
    "    a separate qquestion, labels, answer options\n",
    "    into a single string for testing the model.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    example: dict\n",
    "        Sample input from the dataset which contains the \n",
    "        question, answer labels (e.g. A, B, C, D),\n",
    "        the answer options for the question, and which \n",
    "        of the answers is correct.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    input_text: str, str\n",
    "        Tuple: Formatted test text which contains the question,\n",
    "        the forwatted answer options (e.g., 'A. <option 1> B. <option 2>' etc); \n",
    "        the ground truth answer label only.\n",
    "    \"\"\"\n",
    "    \n",
    "    answer_options_list = list(zip(\n",
    "        example[\"choices\"][\"label\"],\n",
    "        example[\"choices\"][\"text\"]\n",
    "    ))\n",
    "    # join each label and text with . and space\n",
    "    ### YOUR CODE HERE ####\n",
    "    answer_options = [f\"{label}. {text}\" for label, text in answer_options_list]\n",
    "    # join the list of options with spaces into single string\n",
    "    ### YOUR CODE HERE ####\n",
    "    answer_options_string = \" \".join(answer_options)\n",
    "    # combine question and answer options\n",
    "    input_text = example[\"question\"] + \" \" + answer_options_string\n",
    "    # create the test input text which should be:\n",
    "    # the input text, followed by the string \"Answer: \"\n",
    "    # we don't need to append the ground truth answer since we are creating test inputs\n",
    "    # and the answer should be predicted.    \n",
    "    ### YOUR CODE HERE ####\n",
    "    input_text += \"\\nAnswer: \"\n",
    "\n",
    "    return input_text, example[\"answerKey\"]\n",
    "\n",
    "test_samples = [construct_test_samples(dataset[\"validation\"][i]) for i in range(10)]\n",
    "test_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hZxRK7s2p6Qd"
   },
   "outputs": [],
   "source": [
    "# Test the model\n",
    "\n",
    "# set it to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "for sample in test_samples:\n",
    "    input_text = sample[0]\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "    output = model.generate(\n",
    "        input_ids.input_ids,\n",
    "        attention_mask = input_ids.attention_mask,\n",
    "        max_new_tokens=2,\n",
    "        do_sample=True,\n",
    "        temperature=0.4,\n",
    "    )\n",
    "    prediction = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    predictions.append((input_text, prediction, sample[1]))\n",
    "\n",
    "print(\"Predictions of trained model \", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-iwHoKHQp6Qe"
   },
   "source": [
    "Questions:\n",
    "> 1. Provide a brief description of the CommonsenseQA dataset. What kind of task was it developed for, what do the single columns contain? **Answer** The CommonsenseQA dataset contains common sense question, i.e. questions where finding the correct answer involved basic world knowledge. These are presented as a question, a question concept, a number of labelled possible answers, and the label of the correct answer.\n",
    "> 2. What loss function is computed? Where is it implemented? **Answer** The loss function is Negative Log-Likelihood Loss (you can show this by print(loss), which will show grad_fn=<NllLossBackward0>)\n",
    "> 3. Given your loss curve, do you think your model will perform well oon answering common sense questions? (there is no single right answer; you need to interpret your specific plot)\n",
    "> 4. Inspect the predictions above. On how many test questions did the model predict the right answer? Compute the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sT1pj5mDp6Qf"
   },
   "source": ">$Accuracy = \\frac{#correct\\:predictions}{#total\\:predictions}$"
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
