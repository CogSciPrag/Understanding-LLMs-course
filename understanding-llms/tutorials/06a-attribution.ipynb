{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sheet 6.1 LLM probing & attribution\n",
    "========\n",
    "**Author:** Polina Tsvilodub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* using attention visualization, \n",
    "* extracting various representations within a model, \n",
    "* understanding how / why to train a classifier for probing   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this sheet, we will familiarize ourselves with some methods of looking \"under the hood\" of LLMs. \n",
    "**TODO:** Learning goals:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention visualization\n",
    "\n",
    "One of the core processing mechanisms in the transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install bertviz ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from transformers import BertTokenizer, BertModel, AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "model_t5 = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertviz import model_view\n",
    "input_ids = tokenizer.encode(\"Translate to French: The brown dog ran.\", return_tensors=\"pt\")\n",
    "target_ids = tokenizer.encode(\"Le chien brun a couru.\", return_tensors=\"pt\")\n",
    "# Run model and get the losses of each token\n",
    "output = model_t5(input_ids=input_ids, labels=target_ids) \n",
    "# look at the output of the model\n",
    "# Retrieve attention from model outputs to see how to access attention scores \n",
    "# TODO explain contents of the output\n",
    "print(output)\n",
    "\n",
    "# access attention \n",
    "attention = output[-1]\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])  # Convert input ids to token strings\n",
    "model_view(attention, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**\n",
    "* inspect encoder, decoder, cross attentions separately, if possible\n",
    "* ideally, some example where different attention heads do different things: e.g., Jack's world cappital or IOI task?\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 6.1.1: Interpreting attention scores</span></strong>\n",
    ">\n",
    "> Consider the sentence XXX (e.g. world capital). Intuitively, which token do you think will receive high attention scores, from which tokens? Complete the code below and inspect the output. Do the results match your intuiution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient tracing\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probing\n",
    "\n",
    "**TODO**\n",
    "\n",
    "Resources:\n",
    "* https://github.com/rycolab/probing-via-prompting"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
