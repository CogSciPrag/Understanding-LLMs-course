{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sheet 2.5: Introduction to HuggingFace\n",
    "\n",
    "* HF intro so that homework can be worked on\n",
    "  * use Trainer to train the model\n",
    "  * use of DataLoader with datasets\n",
    "  * setting up finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous sheets, we have learned how to explicitly define neural networks and language models. With this background, given sufficient time and access to information (PyTorch docs, details of the architecture, maybe some concepts from the next lecture), in principle, you should be able to put together any famous neural network yourself (e.g., implement Llama-2 or GPT-3)!\n",
    "\n",
    "However, implementing the architecture (i.e., specifying all the layers, their sizes, dimensions etc.) does *not* mean being able to use the model for predictions in practice. For that, we need the **trained** model, i.e., optimal values of all the weights. This requires training, which is quite costly and data-hungry (at least for state-of-the-art actually useful models).\n",
    "\n",
    "TODO: make note on difference between architecture (carcas) and model (actual instantiation with weights)\n",
    "\n",
    "Luckily, there are many popular *open-source* architectures and models, i.e., those for which the ingredients of architecture and the weights have been made freely accessible by the developers (e.g., GPT-2, the LLama models, BERT etc.; in contrast to, e.g., GPT-4 which is closed-source).\n",
    "Open-source models are frequently used by the community and have been made available in packages which provide an easy interface for loading pretrained models, generating predictions from these pretrained models, or training models from scratch. The by-far most used package which we will heavily use in this course is [**HuggingFace**](https://huggingface.co/). HuggingFace provides infrastructure for working with many different types of deep larning models (audio, vision, language), but, as you might guess, we will be focusing on functionality relevant to language models.\n",
    "\n",
    "TODO: define \"pretrained\"\n",
    "\n",
    "* HF overview: tutorials, HuggingFace Hub account; two packages: [`transformers`](https://huggingface.co/docs/transformers/en/index) and [`datasets`](https://huggingface.co/docs/datasets/index), respectively, docs for that. There are also more packages for metrics, tokenization, acceleration and parallelization on GPUs and more. Actual models and datasets stored on the hub that can be loaded. Spaces / organizations are accounts of users or organization which provide these resources (maybe in spaces grouped by topic) shipping these. \n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 2.5.1: Familiarization with HuggingFace </span></strong>\n",
    ">\n",
    "> 1. Fill me.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning: https://huggingface.co/docs/transformers/training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
