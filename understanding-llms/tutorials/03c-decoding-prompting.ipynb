{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sheet 3.3: Prompting & Decoding\n",
    "=======\n",
    "**Author**: Polina Tsvilodub & Michael Franke"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sheet provides more details on concepts that have been mentioned in passing in the previous sheets, and provides some practical examples and exercises for prompting techniques that have been covered in lecture four. Therefore, the learning goals for this sheet are:\n",
    "* take a closer look and understand various decoding schemes,\n",
    "* understand the temperature parameter,\n",
    "* see a few practical examples of prompting techniques from the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding schemes\n",
    "\n",
    "This part of this sheet is a close replication of [this](https://michael-franke.github.io/npNLG/06-LSTMs/06d-decoding-GPT2.html) sheet.\n",
    "\n",
    "This topic addresses the following question: Given a language model that outputs a next-word probability, how do we use this to actually generate naturally sounding text? For that, we need to choose a single next token from the distribution, which we will then feed back to the model, together with the preceding tokens, so that it can generate the next one. This inference procedure is repeated, until the EOS token is chosen, or a maximal sequence length is achieved. The procedure of how exactly to get that single token from the distribution is call *decoding scheme*. Note that \"decoding schemes\" and \"decoding strategies\" refer to the same concept and are used interchangeably. \n",
    "\n",
    "We have already discussed decoding schemes in lecture 02 (slide 25). The following introduces these schemes in more detail again and provides example code for configuring some of them. \n",
    "\n",
    "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 3.3.1: Decoding schemes</span></strong>\n",
    ">\n",
    "> Please read through the following introduction and look at the provided code. \n",
    "> 1. With the help of the example and the documentation, please complete the code (where it says \"### YOUR CODE HERE ####\") for all the decoding schemes.\n",
    "\n",
    "Common decoding strategies are:\n",
    "* **pure sampling**: In a pure sampling approach, we just sample each next word with exactly the probability assigned to it by the LM. Notice that this process, therefore, is non-determinisitic. We can force replicable results, though, by setting a *seed*.\n",
    "* **Softmax sampling**: In soft-max sampling, the probablity of sampling word $w_i$ is $P_{LM} (w_i \\mid w_{1:i-1}) \\propto \\exp(\\frac{1}{\\tau} P_{LM}(w_i \\mid w_{1:i-1}))$, where $\\tau$ is a *temperature parameter*.\n",
    "  * The *temperature parameter* is also often available for closed-source models like the GPT family. It is often said to change the \"creativity\" of the output.\n",
    "* **greedy sampling**: In greedy sampling, we donâ€™t actually sample but just take the most likely next-word at every step. Greedy sampling is equivalent to setting $\\tau = 0$ for soft-max sampling. It is also sometimes referred to as *argmax* decoding.\n",
    "* **beam search**: In simplified terms, beam search is a parallel search procedure that keeps a number $k$ of path probabilities open at each choice point, dropping the least likely as we go along. (There is actually no unanimity in what exactly beam search means for NLG.)\n",
    "* **top-$k$ sampling**: his sampling scheme looks at the $k$ most likely next-words and samples from so that: $$P_{\\text{sample}}(w_i  \\mid w_{1:i-1}) \\propto \\begin{cases} P_{M}(w_i \\mid w_{1:i-1}) & \\text{if} \\; w_i \\text{ in top-}k \\\\ 0 & \\text{otherwise} \\end{cases}$$\n",
    "* **top-$p$ sampling**: Top-$p$ sampling is similar to top-$k$ sampling, but restricts sampling not to the top-$k$ most likely words (so always the same number of words), but the set of most likely words the summed probability of which does not exceed threshold $p$.\n",
    "\n",
    "The within the `transformers` package, for all causal LMs, the `.generate()` function is available which allows to sample text from the model (remember the brief introduction in [sheet 2.5](https://cogsciprag.github.io/Understanding-LLMs-course/tutorials/02e-intro-to-hf.html)). Configuring this function via different values and combinations of various parameters allows to sample text with the different decoding schemes described above. The respective documentation can be found [here](https://huggingface.co/docs/transformers/v4.40.2/en/generation_strategies#decoding-strategies). The same configurations can be passed to the `pipeline` endpoint which we have seen in the same sheet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out [this](https://medium.com/@harshit158/softmax-temperature-5492e4007f71) blog post for very noce visualizations and more detials on the *temperature* parameter.\n",
    "\n",
    "Please complete the code below. GPT-2 is used as an example model, but this works exactly the same with any other causal LM from HF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant packages\n",
    "import torch \n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# convenience function for nicer output\n",
    "def pretty_print(s):\n",
    "    print(\"Output:\\n\" + 100 * '-')\n",
    "    print(tokenizer.decode(s, skip_special_tokens=True))\n",
    "\n",
    "# encode context the generation is conditioned on\n",
    "input_ids = tokenizer.encode('I enjoy walking with my cute dog', return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a seed for reproducibility (if you want)\n",
    "torch.manual_seed(199)\n",
    "\n",
    "# below, greedy decoding is implemented\n",
    "# NOTE: while it is the default for .generate(), it is NOT for pipeline()\n",
    "\n",
    "greedy_output = model.generate(input_ids, max_new_tokens=10)\n",
    "print(pretty_print(greedy_output[0]))\n",
    "\n",
    "# here, beam search is shown\n",
    "# option `early_stopping` implies stopping when all beams reach the end-of-sentence token\n",
    "beam_output = model.generate(\n",
    "    input_ids, \n",
    "    max_new_tokens=10, \n",
    "    num_beams=3, \n",
    "    early_stopping=True\n",
    ") \n",
    "\n",
    "pretty_print(beam_output[0])\n",
    "\n",
    "\n",
    "#  pure sampling\n",
    "sample_output = model.generate(\n",
    "    input_ids,        # context to continue\n",
    "    #### YOUR CODE HERE ####\n",
    "    max_new_tokens=10, # return maximally 10 new tokens (following the input)\n",
    ")\n",
    "\n",
    "pretty_print(sample_output[0])\n",
    "\n",
    "# same as pure sampling before but with `temperature`` parameter\n",
    "SM_sample_output = model.generate(\n",
    "    input_ids,        # context to continue\n",
    "    #### YOUR CODE HERE ####\n",
    "    max_new_tokens=10,\n",
    ")\n",
    "\n",
    "pretty_print(SM_sample_output[0])\n",
    "\n",
    "# top-k sampling \n",
    "top_k_output = model.generate(\n",
    "    input_ids, \n",
    "    ### YOUR CODE HERE #### \n",
    "    max_new_tokens=10,\n",
    ")\n",
    "\n",
    "pretty_print(top_k_output[0])\n",
    "\n",
    "# top-p sampling\n",
    "top_p_output = model.generate(\n",
    "    input_ids, \n",
    "    ### YOUR CODE HERE #### \n",
    "    max_length=50, \n",
    ")\n",
    "\n",
    "pretty_print(top_p_output[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 3.3.2: Understanding decoding schemes</span></strong>\n",
    ">\n",
    "> Think about the following questions about the different decoding schemes.\n",
    ">  \n",
    "> 1. Why is the temperature parameter in softmax sampling sometimes referred to as a creativity parameter? Hint: Think about the shape distribution and from which the next word is sampled, and how it compares to the \"pure\" distribution when the temperature parameter is varied.\n",
    "> 2. Just for yourself, draw a diagram of how beam decoding that starts with the BOS token and results in the sentence \"BOS Attention is all you need\" might work, assuming k=3 and random other tokens of your choice.\n",
    "> 3. Which decoding scheme seems to work best for GPT-2? \n",
    "> 4. Which of the decoding schemes included in this work sheet is a special case of which other decoding scheme(s)? E.g., X is a special case of Y if the behavior of Y is obtained when we set certain paramters of X to specific values.\n",
    "> 5. Can you see pros and cons to using some of these schemes over others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outlook** \n",
    "\n",
    "There are also other more recent schemes, e.g., [locally typical sampling](https://arxiv.org/abs/2202.00666) introduced by Meister et al. (2022)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting strategies\n",
    "\n",
    "The lecture introduced different prompting techniques. (Note: \"prompting technique\" and \"prompting strategy\" refer to the same concept and are used interchangeably) \n",
    "Prompting techniques refer to the way (one could almost say -- the art) of constructing the inputs to the LM, so as to get optimal outputs for your task at hand. Note that prompting is complementary to choosing the right decoding scheme -- one still has to choose the decoding scheme for predicting the completion, given the prompt constructed via a particulat prompting strategy.\n",
    "\n",
    "Below, a practical example of a simple prompting strategy, namely *few-shot prompting* (which is said to elicit *in-context learning*), and a more advanced example, namely *generated knowledge prompting* are provided. These should serve as inspiration for your own implementations and explorations of other prompting schemes out there. Also, feel free to play around with the examples below to build your intuitions! \n",
    "\n",
    "**TODO** note on using this large model on Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run in your environment / on Colab, if you haven't installed these packages yet\n",
    "# !pip install \"transformers[torch]\" \"huggingface_hub[inference]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# few shot prompting \n",
    "\n",
    "few_shot_prompt = \"\"\"\n",
    "Input: This class is awesome. Sentiment: positive\n",
    "Input: This class is terrible. Sentiment: neutral\n",
    "Input: The class is informative. Sentiment: neutral\n",
    "\"\"\"\n",
    "input_text = \"The class is my favourite!\"\n",
    "\n",
    "full_prompt = few_shot_prompt + \"\\nInput: \" + input_text + \" Sentiment: \"\n",
    "\n",
    "input_ids = tokenizer(full_prompt, return_tensors=\"pt\").input_ids\n",
    "few_shot_prediction = model.generate(\n",
    "    input_ids, max_new_tokens=10, \n",
    "    do_smaple=True, \n",
    "    temperature=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated knowledge prompting \n",
    "# TODO NOTE: the code below is deprecated and uses an old LangChain version -- it will be updated\n",
    "# better explanations will also be added\n",
    "# for now, it's for the vibe\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "import numpy as np\n",
    "from langchain import PromptTemplate, FewShotPromptTemplate\n",
    "from langchain.prompts.example_selector.base import BaseExampleSelector\n",
    "from langchain.example_generator import generate_example\n",
    "from langchain.chains import TransformChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "import openai\n",
    "\n",
    "from utils import init_model\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "class RandomExampleSelector(BaseExampleSelector):\n",
    "    \"\"\"\n",
    "    Convenience class for loading few-shot examples from a file.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, examples: pd.DataFrame, num_facts=1):\n",
    "        self.examples = examples\n",
    "        self.num_facts = num_facts\n",
    "       \n",
    "        self.keys = [\"input\", \"knowledge\"]\n",
    "\n",
    "    def add_example(self, example: pd.DataFrame) -> None:\n",
    "        \"\"\"Add new example to store for a key.\"\"\"\n",
    "        self.examples.append(example)\n",
    "\n",
    "    def select_examples(self, num_examples: int = 5) -> list[dict]:\n",
    "        \"\"\"Select which examples to use based on the inputs.\"\"\"\n",
    "        num_examples = min(num_examples, len(self.examples))\n",
    "        selected_rows = self.examples.sample(n=num_examples, replace=False, random_state=42)\n",
    "        \n",
    "        selected_examples = []\n",
    "        for _, row in selected_rows.iterrows():\n",
    "            example = {}\n",
    "            for key in self.keys:\n",
    "                example[key] = row[key]\n",
    "            selected_examples.append(example)\n",
    "\n",
    "        return selected_examples\n",
    "\n",
    "def transform_fct(inputs: dict) -> list:\n",
    "    \"\"\"\n",
    "    Transform raw output of the facts sampler (string)\n",
    "    into a list of strings for easier further processing\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    inputs: dict\n",
    "        Dict with the key \"knowledge\" containing the proposed facts.\n",
    "    Returns:\n",
    "    --------\n",
    "    facts: dict\n",
    "        Dict with they key \"facts\" with a list of facts.\n",
    "    \"\"\"\n",
    "\n",
    "    if \"\\n\" in inputs[\"knowledge\"]:\n",
    "        facts = [\n",
    "            re.sub(\"^-|(\\d)+\", \"\", u).strip() \n",
    "            for u \n",
    "            in inputs[\"knowledge\"].split(\"\\n\") \n",
    "            if len(u) > 3\n",
    "        ]\n",
    "    else:\n",
    "        facts = [\n",
    "            re.sub(\"^-|(\\d)+\", \"\", u).strip().replace(\"\\n\", \"\") \n",
    "            for u \n",
    "            in inputs[\"knowledge\"].split(\".\") \n",
    "            if len(u) > 3\n",
    "        ]\n",
    "    return {\"facts\": facts}   \n",
    "\n",
    "\n",
    "def sample_knowledge(model_name, temperature, num_facts=2, **kwargs):\n",
    "    \"\"\"\n",
    "    Function for retrieving knowledge statements\n",
    "    for generated knowledge prompting.\n",
    "    Includes a string postrprocessing step.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_name: str\n",
    "        Name of the model to be used.\n",
    "    temperature: float\n",
    "        Temperature for sampling.\n",
    "    num_facts: int\n",
    "        Number of facts to be sampled per input question.\n",
    "    **kwargs:\n",
    "        Args for the backbone LLM.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    knowledge_chain: langchain.chains.llm.LLMChain\n",
    "        Chain for generating knowledge statements. Takes question as input.\n",
    "    transform_facts_chain: langchain.chains.TransformChain\n",
    "        Chain for parsing the raw output of the knowledge sampler.\n",
    "    \"\"\"\n",
    "    # read in instructions\n",
    "    instructions_text = f\"\"\"Instructions:\n",
    "    Generate {str(num_facts)} numerical fact(s) about obejcts. Please provide the facts in a bullet list.\n",
    "    \n",
    "    Examples: \n",
    "    \"\"\"\n",
    "    # define template for few-shot examples\n",
    "    example_template = \"\"\"\n",
    "    Input: {input}\n",
    "    Knowledge: {knowledge}    \n",
    "    \"\"\"\n",
    "\n",
    "    # read in examples\n",
    "    examples = pd.read_csv(\"data/session5/knowledge_examples.csv\", sep = \"|\")\n",
    "    # sample random examples from file\n",
    "    example_selector = RandomExampleSelector(examples)\n",
    "    selected_examples = example_selector.select_examples(num_examples=2)\n",
    "    # instantiate the LLM backbone\n",
    "    if model_name == \"text-davinci-003\" or model_name == \"gpt-4\":\n",
    "        model = init_model(\n",
    "            model_name=model_name, \n",
    "            temperature=temperature, \n",
    "         )\n",
    "    elif \"flan-t5\" in model_name:\n",
    "        print(\"Initting HF model\")\n",
    "        model = init_model(\n",
    "            model_name=model_name, \n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Model {model_name} cannot be used for knowledge based QA.\")\n",
    "\n",
    "    # parse few-shot examples into template\n",
    "    example_prompt = PromptTemplate(\n",
    "        template = example_template,\n",
    "        input_variables = ['input', 'knowledge'],\n",
    "    )\n",
    "    input_template = \"\"\"\n",
    "    Input: {input}\n",
    "    Knowledge:\n",
    "    \"\"\"\n",
    "    # format the few_shot prompt\n",
    "    few_shot_prompt = FewShotPromptTemplate(\n",
    "        prefix=instructions_text, \n",
    "        examples=selected_examples,\n",
    "        example_prompt=example_prompt, \n",
    "        input_variables=[\"input\"],\n",
    "        suffix=input_template,\n",
    "        example_separator=\"\\n\\n\",\n",
    "    )    \n",
    "    # define the LLM\n",
    "    knowledge_chain = LLMChain(\n",
    "        llm=model, \n",
    "        prompt=few_shot_prompt, \n",
    "        verbose=True,\n",
    "        output_key=\"knowledge\"\n",
    "    )\n",
    "\n",
    "    # parse the outputs into list\n",
    "    transform_facts_chain = TransformChain(\n",
    "        input_variables=[\"knowledge\"], \n",
    "        output_variables=[\"facts\"], \n",
    "        transform=transform_fct, \n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    return knowledge_chain, transform_facts_chain\n",
    "\n",
    "def answer_question(question, answers, knowledge, model_name, temperature, **kwargs):\n",
    "    \"\"\"\n",
    "    Function for answering multiple choice questions\n",
    "    based on question and knowledge statements (rough replication of system by Liu et al, 2022).\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    question: str\n",
    "        Question to be answered and for which facts are generated.\n",
    "    answers: list\n",
    "        List of possible answers.\n",
    "    knowledge: list\n",
    "        List of knowledge facts (all are used for answer scoring).\n",
    "    model_name: str\n",
    "        Name of the model to be used.\n",
    "    temperature: float\n",
    "        Temperature for sampling.\n",
    "    Returns:\n",
    "    --------\n",
    "    answer: str\n",
    "        Answer with highest probability when conditioned on the facts.\n",
    "    \"\"\"\n",
    "    answer_logprobs = []\n",
    "    # define the template for scoring answers based on facts\n",
    "    template = \"{knowledge} {question} {answer}\"\n",
    "    # instantiate the LLM backbone\n",
    "    # in particular, add params for retrieving log probs\n",
    "    if model_name == \"text-davinci-003\":\n",
    "        model = init_model(\n",
    "            model_name=model_name, \n",
    "            temperature=temperature, \n",
    "            logprobs=0,\n",
    "            max_tokens=0,\n",
    "            echo=True,\n",
    "        )\n",
    "    elif \"flan-t5\" in model_name or \"gpt-4\" in model_name:\n",
    "        model = init_model(\n",
    "            model_name=\"text-davinci-003\", \n",
    "            temperature=temperature, \n",
    "            logprobs=0,\n",
    "            max_tokens=0,\n",
    "            echo=True,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Model {model_name} cannot be used for knowledge based QA.\")\n",
    "    \n",
    "    # format the prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables = ['question', 'answer', 'knowledge'],\n",
    "    )\n",
    "    qa_chain = LLMChain(   \n",
    "        llm=model,\n",
    "        prompt=prompt,\n",
    "        verbose=True,\n",
    "    )\n",
    "    # plain model request in order to get answer probabilities\n",
    "    for answer in answers:\n",
    "        # note that all facts are used for scoring\n",
    "        result = qa_chain.generate(input_list=[{\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"knowledge\": knowledge,\n",
    "            }]\n",
    "        )\n",
    "        # retrieve log probs from LLM results object       \n",
    "        log_p = result.generations[0][0].generation_info[\"logprobs\"][\"token_logprobs\"]\n",
    "        # cut off none probability of first token\n",
    "        answer_logprobs.append(np.sum(np.array(log_p[1:])))\n",
    "    # renormalize\n",
    "    answer_logprobs = np.array(answer_logprobs)/np.sum(np.array(answer_logprobs))\n",
    "    # find max probability\n",
    "    max_prob_idx = np.argmax(answer_logprobs)\n",
    "    # return answer with max probability\n",
    "    print(\"All answers \", answers)\n",
    "    print(\"Answer probabilities \", answer_logprobs)\n",
    "    print(\"Selected answer \", answers[max_prob_idx])\n",
    "    return answers[max_prob_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 3.3.3: Prompting techniques</span></strong>\n",
    ">\n",
    "> For the following exercises, use the same model as used above.\n",
    "> 1. Implement an example of a few-shot chain-of-thought prompt.\n",
    "> 2. Try to vary the few-shot and the chain-of-thought prompt by introducing mistakes and inconsistencies. Do these mistakes affect the result of your prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outlook**\n",
    "\n",
    "**TODO** to finish.\n",
    "\n",
    "* prompting webbook\n",
    "* some utils for already using pre-built utils for stuff like ToT (LangChain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
