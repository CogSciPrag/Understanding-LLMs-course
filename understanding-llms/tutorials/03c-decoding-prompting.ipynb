{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sheet 3.3: Prompting & Decoding\n",
    "=======\n",
    "**Author**: Polina Tsvilodub & Michael Franke"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sheet provides more details on concepts that have been mentioned in passing in the previous sheets, and provides some practical examples and exercises for prompting techniques that have been covered in lecture four. Therefore, the learning goals for this sheet are:\n",
    "* take a closer look and understand various decoding schemes,\n",
    "* understand the temperature parameter,\n",
    "* see a few practical examples of prompting techniques from the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding schemes\n",
    "\n",
    "This part of this sheet is a close replication of [this](https://michael-franke.github.io/npNLG/06-LSTMs/06d-decoding-GPT2.html) sheet.\n",
    "\n",
    "This topic addresses the following question: Given a language model that outputs a next-word probability, how do we use this to actually generate naturally sounding text? For that, we need to choose a single next token from the distribution, which we will then feed back to the model, together with the preceding tokens, so that it can generate the next one. This inference procedure is repeated, until the EOS token is chosen, or a maximal sequence length is achieved. The procedure of how exactly to get that single token from the distribution is call *decoding scheme*. Note that \"decoding schemes\" and \"decoding strategies\" refer to the same concept and are used interchangeably. \n",
    "\n",
    "We have already discussed decoding schemes in lecture 02 (slide 25). The following introduces these schemes in more detail again and provides example code for configuring some of them. \n",
    "\n",
    "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 3.3.1: Decoding schemes</span></strong>\n",
    ">\n",
    "> Please read through the following introduction and look at the provided code. \n",
    "> 1. With the help of the example and the documentation, please complete the code (where it says \"### YOUR CODE HERE ####\") for all the decoding schemes.\n",
    "\n",
    "Common decoding strategies are:\n",
    "* **pure sampling**: In a pure sampling approach, we just sample each next word with exactly the probability assigned to it by the LM. Notice that this process, therefore, is non-determinisitic. We can force replicable results, though, by setting a *seed*.\n",
    "* **Softmax sampling**: In soft-max sampling, the probablity of sampling word $w_i$ is $P_{LM} (w_i \\mid w_{1:i-1}) \\propto \\exp(\\frac{1}{\\tau} P_{LM}(w_i \\mid w_{1:i-1}))$, where $\\tau$ is a *temperature parameter*.\n",
    "  * The *temperature parameter* is also often available for closed-source models like the GPT family.\n",
    "* **greedy sampling**: In greedy sampling, we donâ€™t actually sample but just take the most likely next-word at every step. Greedy sampling is equivalent to setting $\\tau = 0$ for soft-max sampling. (see below) It is also sometimes referred to as *argmax* decoding.\n",
    "* **beam search**: In simplified terms, beam search is a parallel search procedure that keeps a number of path probabilities open at each choice point, dropping the least likely as we go along. (There is actually no unanimity in what exactly beam search means for NLG.)\n",
    "* **top-k sampling**:\n",
    "* **top-p sampling**:\n",
    "\n",
    "The within the `transformers` package, for all causal LMs, the `.gnerate()` function is available which allows to sample text from the model (remember the brief introduction in [sheet 2.5](https://cogsciprag.github.io/Understanding-LLMs-course/tutorials/02e-intro-to-hf.html)). Configuring this function via different values and combinations of various parameters allows to sample text with the different decoding schemes described above. The respective documentation can be found [here](https://huggingface.co/docs/transformers/v4.40.2/en/generation_strategies#decoding-strategies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentencepiece\n",
    "# !pip install datasets\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant packages\n",
    "import torch \n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# convenience function for nicer output\n",
    "def pretty_print(s):\n",
    "    print(\"Output:\\n\" + 100 * '-')\n",
    "    print(tokenizer.decode(s, skip_special_tokens=True))\n",
    "\n",
    "# encode context the generation is conditioned on\n",
    "input_ids = tokenizer.encode('I enjoy walking with my cute dog', return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Just for yourself, draw a diagram of how beam decoding that starts with the BOS token and results in the sentence \"BOS Attention is all you need\" might work, assuming k=3.\n",
    "> 2. Write code for some of these.\n",
    "> 3. Think about pros and cons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** outlook to more advanced stuff like locally optimal (?) scheme by Meister et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Check out [this](https://medium.com/@harshit158/softmax-temperature-5492e4007f71) blog post for very noce visualizations and more detials.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting strategies"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
