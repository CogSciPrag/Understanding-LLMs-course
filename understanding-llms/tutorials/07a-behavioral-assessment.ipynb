{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sheet 7.1: Behavioral assessment & Evaluation\n",
    "======\n",
    "**Author**: Polina Tsvilodub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sheet focuses on evaluating the input-output (I/O) behavior of LLMs. Inspired by experimental paradigms / the terminology in cognitive science and psychology which investigate a blackbox (the human mind) via looking at the behavior across different interesting conditions (inputs), such assessment of LLMs (also blackboxes) can be called \"behavioral assessment\". This approach can be seen as one piece that should work in combination with attribution methods discussed in the previous sheet in order to provide fuller understanding of what LLMs can or cannot do (I/O testing) and how they do it (attributions). \n",
    "Following the structure of the lecture, we will first look at practical aspects of benchmark testing, and then look at \"machine psychology\", which often draws on the same methods but addresses somewhat different research questions.\n",
    "\n",
    "Therefore, the learning goals of this sheet are:\n",
    "* look at examples of a few different benchmarks and how they are usually constructed\n",
    "* become familiar with standard evaluation metrics and methods used for evaluating LLMs on benchmarks (these include PPL, log probability based scores, accuracy, F1, free generation etc)\n",
    "* become familiar with different more advanced eval measures HELM  \n",
    "* look at examples of machine psychology and how, in practice, LLM performance can be compared to various human data\n",
    "* how LLMs can be tested as \"psychology subjects\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark testing\n",
    "\n",
    "Such I/O evaluations are the most common approach to LLM evaluation. Taking a more technical / engineering-oriented perspective which aims at building LLMs for specific application, it is very common to make use of large benchmark datasets which are designed to test models’ performance on a variety of tasks in an automated way. This is often done by checking the models’ outputs against ground truth answers or by computing standard scores for certain datasets. Therefore, quality of LLMs is measured by their scores on these benchmarks. Initially, these benchmarks were designed to test LLMs’ linguistic performance. TODO: finish.\n",
    "However, more recently, work more informed by human language use and processing has compared LLMs’ performance to aspects of human behavior. Here, the assessment of LLMs is guided more by the question of how human-like certain aspects of its performance are. Finally, recent LLMs have shown impressive generalization to tasks which seem to require more than linguistic fluency, like solving math and reasoning problems. Therefore, more recent benchmarks incorporate tests of various tasks going beyond linguistic capability. Given that SOTA LLMs are also often designed as assisstants and embedded in user-facing applications, it also became crucial to evaluate potenital social impacts that LLMs might exhibit with their outputs, like assessing biases and toxicity of the generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perplexity evaluation\n",
    "# import Huggingface package managing open source datasets\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "test = load_dataset(\"wikitext\", 'wikitext-2-raw-v1', split=\"test\")\n",
    "encodings = tokenizer(\n",
    "    \"\\n\\n\".join(test[\"text\"]), \n",
    "    return_tensors=\"pt\",\n",
    ").input_ids\n",
    "\n",
    "input_tokens = encodings[:,10:50]\n",
    "\n",
    "pretty_print(input_tokens[0])\n",
    "\n",
    "output = model(input_tokens, labels = input_tokens)\n",
    "print(\"Average NLL for wikipedia chunk\", output.loss.item())\n",
    "\n",
    "### your code for computing the perplexity goes here ###\n",
    "# perplexity ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLG scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the implementation of the bleu score computation\n",
    "from torchtext.data.metrics import bleu_score\n",
    "# load model and tokenizer\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "tokenizer_t5 = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model_t5 = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "# define example sentences for translating from English to German\n",
    "text_en = \"All of the others were of a different opinion.\"\n",
    "text_de = \"Alle anderen waren anderer Meinung.\"\n",
    "# define task \n",
    "prefix = \"translate English to German: \"\n",
    "\n",
    "# define helper function taking prediction and gold standard\n",
    "def compute_bleu(prediction, target, n):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    ----------\n",
    "    prediction: str\n",
    "        Predicted translation.\n",
    "    target: str\n",
    "        Gold standard translation.\n",
    "    n: int\n",
    "        n-gram over which to compute BLEU.\n",
    "    Returns:\n",
    "    -------\n",
    "    score: float\n",
    "        BLEU-n score.\n",
    "    \"\"\"\n",
    "    score = bleu_score(\n",
    "        [prediction.split()], \n",
    "        [[target.split()]], \n",
    "        max_n=n, \n",
    "        weights = [1/n] * n,\n",
    "    )\n",
    "    return score \n",
    "\n",
    "# encode the source and the target sentences\n",
    "encoding_en = tokenizer_t5(\n",
    "    [prefix + text_en],\n",
    "    return_tensors=\"pt\",\n",
    ").input_ids\n",
    "# we don't need the task prefix before the target\n",
    "encoding_de = tokenizer_t5(\n",
    "    [text_de],\n",
    "    return_tensors=\"pt\",\n",
    ").input_ids\n",
    "\n",
    "# predict with model\n",
    "predicted_de = model_t5.generate(encoding_en)\n",
    "\n",
    "print(\"Predicted translation: \", predicted_decoded_de)\n",
    "\n",
    "# decode the prediction\n",
    "predicted_decoded_de = tokenizer_t5.decode(\n",
    "    predicted_de[0],\n",
    "    skip_special_tokens=True,\n",
    ")\n",
    "\n",
    "# compute BLEU-1 for the prediction\n",
    "### YOUR CODE CALLING THE HELPER ABOVE GOES HERE ###\n",
    "# bleu1 = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import argparse \n",
    "import json\n",
    "from pprint import pprint\n",
    "# set openAI key in separate .env file w/ content\n",
    "load_dotenv() \n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# read test cases with single token prediction\n",
    "grammaticality_test_cases = pd.read_csv(\"data/grammaticality_tests.csv\")\n",
    "\n",
    "def get_surprisal(\n",
    "        masked_sequence, \n",
    "        full_sequence,\n",
    "        preface = 'start ', \n",
    "        model_name =  \"text-davinci-003\", \n",
    "        mask_token = \"[MASK]\",\n",
    "        return_region_surprisal=True,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Helper for retrieving surprisal of different response types from GPT-3.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    masked_sequence: str\n",
    "        Sequence with masked critical region.\n",
    "    full_sequence: str\n",
    "        Full sequence with crticial region.\n",
    "    preface: str\n",
    "        Preface (instructions or few-shot) to be added to the sequence.\n",
    "    model_name: str\n",
    "        Name of the GPT-3 model to be used.\n",
    "    mask_token: str\n",
    "        Token used for masking the critical region.\n",
    "    return_region_surprisal: bool\n",
    "        Whether to return surprisal of the critical region only or average for full sequence.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    mask_surprisals: list\n",
    "        Surprisal of the critical region or average for full sentence.\n",
    "    \"\"\"\n",
    "    # get log probs for sequence\n",
    "    if model_name not in [\"gpt-3.5-turbo\", \"gpt-4\"]:\n",
    "        response = openai.Completion.create(\n",
    "                engine      = model_name, \n",
    "                prompt      = preface + full_sequence,\n",
    "                max_tokens  = 0, # sample 0 new tokens, i.e., only get scores of input sentence\n",
    "                temperature = 1, \n",
    "                logprobs    = 0, \n",
    "                echo        = True,\n",
    "            ) \n",
    "        pprint(response)\n",
    "    else:\n",
    "        raise ValueError(\"GPT-4 and turbo cannot return log probs!\")\n",
    "\n",
    "    text_offsets = response.choices[0]['logprobs']['text_offset']\n",
    "    # allow to use few shot examples\n",
    "    if preface != '':\n",
    "        cutIndex = text_offsets.index(max(i for i in text_offsets if i <= len(preface))) \n",
    "        endIndex = response.usage.total_tokens\n",
    "    else:\n",
    "        cutIndex = 0\n",
    "        endIndex = len(response.choices[0][\"logprobs\"][\"tokens\"])\n",
    "    answerTokens = response.choices[0][\"logprobs\"][\"tokens\"][cutIndex:endIndex]\n",
    "    answerTokenLogProbs = response.choices[0][\"logprobs\"][\"token_logprobs\"][cutIndex:endIndex] \n",
    "    # retrieve critical region surprisal\n",
    "    if return_region_surprisal:\n",
    "        # get target region surprisal\n",
    "        # for grammaticality judgment comparison\n",
    "        # retrieve the target region which is masked in the masked sequence\n",
    "        # get its index in the full sentence\n",
    "        mask_ind = [i for i, e in \n",
    "                    enumerate(masked_sequence.replace(\".\", \" .\").split(\" \")) \n",
    "                    if e == mask_token\n",
    "                    ]\n",
    "        # get target region\n",
    "        masked_words = [full_sequence.replace(\".\", \" .\").split(\" \")[mask_i] for mask_i in mask_ind]\n",
    "        # get tokens corresponding to the target region\n",
    "        # and handle subword tokenization of GPT\n",
    "        mask_log_probs = []\n",
    "        mask_log_prob = np.nan\n",
    "        for masked_word in masked_words:\n",
    "            for i, t in enumerate(answerTokens):\n",
    "                if t.strip() == masked_word.strip():\n",
    "                    mask_log_prob = answerTokenLogProbs[i]\n",
    "                    mask_log_probs.append(mask_log_prob)\n",
    "                elif t.strip() in masked_word:\n",
    "                    if t.strip() + answerTokens[i+1] in masked_word:\n",
    "                        mask_log_prob = answerTokenLogProbs[i]\n",
    "                        mask_log_probs.append(mask_log_prob)\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "        mask_surprisals = [-m for m in mask_log_probs]\n",
    "    # get full sentence surprisal\n",
    "    else:\n",
    "        mask_surprisals = [- np.mean(\n",
    "            np.asarray(answerTokenLogProbs)\n",
    "        )]\n",
    "\n",
    "    return mask_surprisals\n",
    "\n",
    "def compare_surprisals(row, return_region_surprisal):\n",
    "    \"\"\"\n",
    "    Helper for comparing surprisals of grammatical and ungrammatical sentences.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    row: pd.Series\n",
    "        Row of the dataframe containing the test case.\n",
    "    return_region_surprisal: bool\n",
    "        Whether to return surprisal of the critical region only or average for full sentence.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    is_grammatical: bool\n",
    "        Whether the grammatical sentence has lower surprisal than the ungrammatical one.\n",
    "    \"\"\"\n",
    "    # get surprisal of grammatical sentence\n",
    "    grammatical_surprisal = get_surprisal(\n",
    "        row[\"masked_sentence\"],\n",
    "        row[\"grammatical_sentence\"],\n",
    "        return_region_surprisal=return_region_surprisal,\n",
    "    )\n",
    "    print(f\"--- Surprisal of grammatical sentence {row['grammatical_sentence']}: {grammatical_surprisal} ---\\n\\n\")\n",
    "    # get surprisal of ungrammatical sentence\n",
    "    ungrammatical_surprisal = get_surprisal(\n",
    "        row[\"masked_sentence\"],\n",
    "        row[\"ungrammatical_sentence\"],\n",
    "        return_region_surprisal=return_region_surprisal,\n",
    "    )\n",
    "    print(f\"--- Surprisal of ungrammatical sentence {row['ungrammatical_sentence']}: {ungrammatical_surprisal} ---\\n\\n\")\n",
    "    \n",
    "    # check LM accuracy (in terms of surprisal)\n",
    "    is_grammatical = all([g < u for g, u in zip(grammatical_surprisal, ungrammatical_surprisal)])\n",
    "    return is_grammatical\n",
    "\n",
    "print(\"Gramamticality test cases: \\n\\n\", grammaticality_test_cases.head(10))\n",
    "# call surprisal computation for single test cases from the slides\n",
    "print(\"--- Agreement test case --- \\n Is grammatical sentence less surprising than ungrammatical one?\", \n",
    "      compare_surprisals(grammaticality_test_cases.iloc[0], return_region_surprisal=False), \"\\n\\n\")\n",
    "\n",
    "print(\"--- Reflexive test case --- \\n Is grammatical sentence less surprising than ungrammatical one?\", \n",
    "      compare_surprisals(grammaticality_test_cases.iloc[11], return_region_surprisal=False), \"\\n\\n\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Runs all test cases.\n",
    "    \"\"\"\n",
    "    for _, r in grammaticality_test_cases.iterrows():\n",
    "        print(\"--------------------\")\n",
    "        is_grammatical = compare_surprisals(r, return_region_surprisal=False)\n",
    "        print(f\"Grammatical sentence: {r['grammatical_sentence']} \\n\\n\")\n",
    "        print(f\"Ungrammatical sentence: {r['ungrammatical_sentence']} \\n\\n\")\n",
    "        # check LM accuracy (in terms of surprisal)\n",
    "        print(\"Is the grammatical sentence more likely than the ungrammatical one under LM?\", \n",
    "              is_grammatical)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SuperGLUE\n",
    "\n",
    "Finally, we will get our hands dirty with evaluating LLMs which already have been trained. In this task, we will use a few tasks from one of the most-used LM benchmarks, the SuperGLUE benchmark:\n",
    "\n",
    "    a natural language inference (NLI) task “rte”,\n",
    "\n",
    "        a task wherein the model has to predict whether a second sentence is entailed by the first one (i.e., predict the label ‘entailment’ or ‘no entailment’)\n",
    "\n",
    "    a question answering task “boolq”,\n",
    "\n",
    "        a task wherein the model has to predict an answer (yes/no) to a question, given context\n",
    "\n",
    "    and a sentence continuation task “copa”.\n",
    "\n",
    "        a task wherein the model has to select one of two sentences as the more plausible continuation given an input sentence.\n",
    "\n",
    "We will be using (subset of) the validation splits of the tasks for our evaluation.\n",
    "\n",
    "With the introduction of first language models like BERT, a common approach to using benchmarks like SuperGLUE was to fine-tune the pretrained model on the train split of the benchmark datasets, and then use the test splits for evaluation. With SOTA LLMs, it is more common to do zero- or few-shot evaluation where the model has to, e.g., predict labels or select answer options without special fine-tuning, just given instructions.\n",
    "\n",
    "We are also not going to fine-tune our model on these specific tasks. Instead, as introduced in class, we are going to compare the log probabilities of different answer options (e.g., log probabilities of “entailment” vs. “no entailment” following a pair of sentences from the RTE-task). With this method, the assumption is that a model’s output prediction for a particular trial is correct iff: \n",
    "\n",
    "$$\\log P_{LM}(\\text{correct label} \\mid \\text{context}) >  \\log P_{LM}(\\text{incorrect label} \\mid \\text{context})$$\n",
    "\n",
    "For tasks like “copa” where there is no single label but instead a sentence continuation, we are going to compute the average token log probability as a single-number representation of the continuation. Here, the model’s prediction will count as correct iff the average log probability of the correct continuation sentence will be higher, given the input, than for the incorrect continuation. We will not using task instructions in our evaluation since the model wasn’t fine-tuned on instruction-following.\n",
    "\n",
    "Your job is to complete the code below, evaluate the model which you have fine-tuned above and summarize the results you find in a few words (see below for more detailed step-by-step instructions). If you have issues with the previous task and cannot use your own fine-tuned model, please use the initial IMDB fine-tuned GPT-2 with which we initialized the policy in exercise 2. Please indicate which model you are testing on Moodle in the respective exercise responses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**\n",
    "* Calibration\n",
    "* implementing various bias corrections\n",
    "* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine psychology\n",
    "\n",
    "**TODO**\n",
    "* compute SG scores with some example\n",
    "* coming up with testing some psychological aspect\n",
    "* hands-on methods of comparing quantitatively human and machine data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
