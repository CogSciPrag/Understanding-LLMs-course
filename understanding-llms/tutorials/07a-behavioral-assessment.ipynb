{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sheet 7.1: Behavioral assessment & Evaluation\n",
    "======\n",
    "**Author**: Polina Tsvilodub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sheet focuses on evaluating the input-output (I/O) behavior of LLMs. Inspired by experimental paradigms / the terminology in cognitive science and psychology which investigate a blackbox (the human mind) via looking at the behavior across different interesting conditions (inputs), such assessment of LLMs (also blackboxes) can be called \"behavioral assessment\". This approach can be seen as one piece that should work in combination with attribution methods discussed in the previous sheet in order to provide fuller understanding of what LLMs can or cannot do (I/O testing) and how they do it (attributions). \n",
    "Following the structure of the lecture, we will first look at practical aspects of benchmark testing, and then look at \"machine psychology\", which often draws on the same methods but addresses somewhat different research questions.\n",
    "\n",
    "Therefore, the learning goals of this sheet are:\n",
    "* look at examples of a few different benchmarks and how they are usually constructed\n",
    "* become familiar with standard evaluation metrics and methods used for evaluating LLMs on benchmarks (these include PPL, log probability based scores, accuracy, F1, free generation etc)\n",
    "* become familiar with different more advanced eval measures HELM  \n",
    "* look at examples of machine psychology and how, in practice, LLM performance can be compared to various human data\n",
    "* how LLMs can be tested as \"psychology subjects\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark testing\n",
    "\n",
    "Such I/O evaluations are the most common approach to LLM evaluation. Taking a more technical / engineering-oriented perspective which aims at building LLMs for specific application, it is very common to make use of large benchmark datasets which are designed to test models’ performance on a variety of tasks in an automated way. This is often done by checking the models’ outputs against ground truth answers or by computing standard scores for certain datasets. Therefore, quality of LLMs is measured by their scores on these benchmarks. \n",
    "\n",
    "Initially, these benchmarks were designed to test LLMs’ linguistic performance since the goal of building the model is a system that predict grammatical and fluent natural language. Therefore, some first benchmarks (or, commonly used textual datasets) are, for instance, Wikipedia texts, the Penn Treebank, and the GLUE benchmark. Wikipedia texts are often used for measuring the perplexity of the model on this standard text (see below for details). The Penn Treebank was often used for fine-tuning or evaluating models, e.g., on part-of-speech tagging as an approximation of syntactic performance, while the GLUE benchmark contains tasks which are supposed to approximate (semantic) natural language understanding in the form of paraphrase tasks, sentiment classification, natural language inference tasks etc.\n",
    "\n",
    "Now recent LLMs have shown perhaps unexpectedly impressive generalization to tasks which seem to require more than linguistic fluency, like solving math and reasoning problems. Therefore, more recent benchmarks incorporate tests of various tasks going beyond linguistic capability. Two of the most widely used benchmarks include the [MMLU](https://arxiv.org/abs/2009.03300) and the [BIG-Bench](https://arxiv.org/abs/2206.04615) datasets.\n",
    "Given that SOTA LLMs are also often designed as assisstants and embedded in user-facing applications, it also became crucial to evaluate potenital social impacts that LLMs might exhibit with their outputs, like assessing biases and toxicity of the generations. To this end, specialized benchmarks like [RealToxicityPrompts](https://arxiv.org/abs/2009.11462) or [WinoGender](https://arxiv.org/abs/1804.09301) were created.\n",
    "\n",
    "One crucial assumption behind benchmark evaluation is that benchmarks are representative of tasks and covers a wide variety of data that the model should perform well on in order to count as a good model for its target deployment. Although benchmarks arguably provide a wide coverage (they commonly contain thousands of inputs and answers), they often test only an approximation of what the model does in deployment (i.e., free text generation). \n",
    "Furthermore, with newer models trained on newer crawls of the internet, there  are increasing worries of so-called *contamination*, i.e., actually including the test datasets in the training data of the models, thereby potentially inflating the models' true generalization scores. For instance, Wikipedia is included in the training data of most of the modern models. \n",
    "\n",
    "Scalably evaluating longer generated texts is quite a difficult task. This is because, intuitively, there is no single \"ground truth answer\" when it comes to writing; there are many equally good ways of writing summary of a text, or even potentially multiple ways of translating a sentence. This makes text evaluation difficult to evaluate automatically. This is still a largely unsolved issue (!), so that human or machine evaluation is often used. The available methods for automated text scoring are rooted in work on summarization and machine translation, and require (human-written) gold-standard texts. \n",
    "\n",
    "Note that when mentioning a *model* in the explanations, we refer to trained models which are evaluated with respect to their performance, i.e., in *inference mode*. If one wanted to track the performance on certain benchmarks during training, one could also run evaluations on intermediate model checkpoints during training, too. Just note that the model is \"frozen\" and runs in inference mode during all of the testing described in this sheet.\n",
    "\n",
    "In sum, the reasons why benchmarks are so widely used are a few core advantages: \n",
    "* the availability if a few well-known datasets leads to (somewhat of a) standardization of the evaluation procedure across different work. \n",
    "* their large scale often provides high coverage, more reliable results (although coverage might not always mean consistent quality or variability expected, e.g., by linguists). \n",
    "* **crucially**: they are design to be evaluated with easy to compute *automatic evaluation metrics*. You have heard about them in the lecture; we will recap these below and then work with them in practice.\n",
    "\n",
    "### Metrics\n",
    "\n",
    "**Perplexity**: It is computed as:\n",
    "$$PPL_{LM}(x_0 ... x_n) = \\exp(\\frac{1}{n}\\sum_{i=0}^n - \\log P_{LM}(x_i \\mid x_{<i})) $$\n",
    "\n",
    "Note that this is only applicable to causal language models. This is the metric commonly used, e.g., on the Wikipedia texts. For instance, the PPL of GPT-2 on the Penn Treebank dataset is 35.76, while the perplexity of GPT-3 on the same dataset is 20.50. \n",
    "The idea is that an ideal model should have a perplexity as close to 0 as possible for a naturally occurring text that it has learned, thereby approximating good fit to the \"ground truth distribution of natural language\".\n",
    "\n",
    "Below is some code for computing the perplexity of different sizes of GPT-2 for an exerpt from Wkipedia.\n",
    "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 7.1.1: Calculating perplexity</span></strong>\n",
    ">\n",
    "> 1. Please complete the code below. (Hint: only one simple transformation is required in order to calculate the perplexity from the NLL loss)\n",
    "> 2. Compare the results for the models of different sizes. Does their comparison (ordering) match your intuition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average NLL for wikipedia chunk under small model  9.524829864501953\n",
      "Average NLL for wikipedia chunk under xl model  12.172213554382324\n",
      "PPL of smaller model: 13695.599618637785, PPL of larger model: 193341.5425429966\n"
     ]
    }
   ],
   "source": [
    "# perplexity evaluation\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "test = load_dataset(\"wikitext\", 'wikitext-2-raw-v1', split=\"test\")\n",
    "\n",
    "input_tokens = tokenizer(\n",
    "    \"\\n\\n\".join(test[\"text\"][:10]), \n",
    "    return_tensors=\"pt\",\n",
    ").input_ids.to(device)\n",
    "\n",
    "# select a part of the text\n",
    "# input_tokens = encodings[:,10:50]\n",
    "\n",
    "# load models of different sizes\n",
    "model_s = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
    "model_xl = AutoModelForCausalLM.from_pretrained(\"gpt2-xl\").to(device)\n",
    "\n",
    "output_s = model_s(input_tokens, labels = input_tokens)\n",
    "output_xl = model_xl(input_tokens, labels = input_tokens)\n",
    "print(\"Average NLL for wikipedia chunk under small model \", output_s.loss.item())\n",
    "print(\"Average NLL for wikipedia chunk under xl model \", output_xl.loss.item())\n",
    "\n",
    "### your code for computing the perplexity goes here ###\n",
    "perplexity_s = np.exp( output_s.loss.item())\n",
    "perplexity_xl = np.exp( output_xl.loss.item())\n",
    "\n",
    "print(f\"PPL of smaller model: {perplexity_s}, PPL of larger model: {perplexity_xl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[This](https://huggingface.co/docs/transformers/en/perplexity) blogpost provides an interesting outlook to dealing with the issue of fixed length of the context window of LMs when trying to compute the perplexity of longer texts (e.g., Wikipedia)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Accuracy**: this is a standard metric widely used in many domains, not only NLP. It computes the proportion of correct responses in a set of tasks. Presupposes that there is a single correct answer for a given input. We have seen in the lecture that one way to compute accuracy is to score each answer option, given the input, under the LLM, and retreive the predicted options via $argmax$; i.e., take the option for which the model assigned the highest (log) probability to be the chosen option. If this option is the ground truth option, the model's prediction is correct for this test item (i.e., correctness = 1); otherwise, correctness = 0. Accuracy is then the average correctness across all the test items in the benchmark. The lecture pointed out limitations of the argmax approach. Just as a recap, the underlying assumption is that a model that can perform a task correctly will predict:\n",
    "$$\\log P_{LM}(\\text{correct label} \\mid \\text{context}) >  \\log P_{LM}(\\text{incorrect label} \\mid \\text{context})$$\n",
    "\n",
    "The advantage of this approach is that it makes sure to score only the available answer options under the model, which is an especially important constraint for weaker models. However, SOTA more powerful LLMs, especially if they are instruction-tuned are often also tested via *text generation*. I.e., the input is given with an appropriate instruction, and the model's generated text is evaluated via string matching (e.g., regex of simple matching). If the correct answer option was generated, the model's correctness is 1 for this trial, and 0 otherwise. \n",
    "\n",
    "Below is some code exemplifying evaluating a model on a question answering benchmark CommonsenseQA which we have already used in the homework, via scoring answers under the model. This now provides an automatic implementation of the last task of HW1 / task 2 in HW2. For retrieving conditional log probabilities of different options, given a context, we will be using the package [`minicons`](https://github.com/kanishkamisra/minicons).\n",
    "\n",
    "Note that here we are interested in scoring the different response options, given the questions, under the model, rather prompting the model with a list of possible options and letting it generate the option label. Therefore, the wrangling of the dataset is slightly different than in the homework.\n",
    "\n",
    "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 7.1.2: Calculating accuracy</span></strong>\n",
    ">\n",
    "> 1. Please complete the code below.\n",
    "> 2. Compare the results to your results from the homework. Which are better? Do you think the log probability based evaluation is better than the strategy we used in the homework? Why (not)?\n",
    "> 3. What is the expected chance accuracy on this dataset? Why is it important to consider chance accuracy when interpreting the results of a system?\n",
    "> 4. The lecture mentioned effects of various bias corrections that can be applied to the raw scores. In the code below, by default, a length correction is applied (i.e., average log probabilities are used). use the docs / examples of the minicons package [here](https://github.com/kanishkamisra/minicons/blob/master/examples/surprisals.md) to retrieve \"raw\" log probabilities of the completions (i.e., sums over the token probabilities) and use those to calculate the accuracy. Do the results change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/understanding_llms/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load dataset \n",
    "dataset = load_dataset(\"tau/commonsense_qa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def massage_input_text(example):\n",
    "    \"\"\"\n",
    "    Helper for converting labels, answer options\n",
    "    into a single string.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    example: dict\n",
    "        Sample input from the dataset which contains the \n",
    "        question, answer labels (e.g. A, B, C, D),\n",
    "        the answer options for the question, and which \n",
    "        of the answers is correct.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    answer_options: list[str]\n",
    "        Formatted list of answer options (e.g., 'A. <option 1> B. <option 2>' etc)\n",
    "        and the ground truth answer.\n",
    "    \"\"\"\n",
    "    # combine each label with its corresponding text\n",
    "    answer_options_list = list(zip(\n",
    "        example[\"choices\"][\"label\"],\n",
    "        example[\"choices\"][\"text\"]\n",
    "    ))\n",
    "    # join each label and text with . and space\n",
    "    answer_options = [f\"{label}. {text}\" for label, text in answer_options_list]\n",
    "\n",
    "    return answer_options\n",
    "\n",
    "# process input texts of validation dataset\n",
    "massaged_dataset_val = dataset[\"validation\"].map(\n",
    "    lambda example: {\n",
    "        \"text\": example[\"question\"],\n",
    "        \"answers\": massage_input_text(example),\n",
    "        # get the index of the correct answer\n",
    "        \"label\": example[\"choices\"][\"label\"].index(example[\"answerKey\"])\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1afa02df02c908a558b4036e80242fac',\n",
       " 'question': 'A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?',\n",
       " 'question_concept': 'revolving door',\n",
       " 'choices': {'label': ['A', 'B', 'C', 'D', 'E'],\n",
       "  'text': ['bank', 'library', 'department store', 'mall', 'new york']},\n",
       " 'answerKey': 'A',\n",
       " 'text': 'A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?',\n",
       " 'answers': ['A. bank',\n",
       "  'B. library',\n",
       "  'C. department store',\n",
       "  'D. mall',\n",
       "  'E. new york'],\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "massaged_dataset_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over part of the validation set an compute accuracy \n",
    "# (the test set doesn't have ground truth labels)\n",
    "\n",
    "# set up a scorer \n",
    "from minicons import scorer \n",
    "\n",
    "lm_scorer = scorer.IncrementalLMScorer(\n",
    "    'gpt2',\n",
    "    device=device,\n",
    ")\n",
    "# initialize list for storing the correctness of the model predictions\n",
    "correctness = []\n",
    "\n",
    "for i in range(100):\n",
    "    # get the ith example from the validation set\n",
    "    example = massaged_dataset_val[i]\n",
    "    # get the text of the question\n",
    "    question = example['text']\n",
    "    # get the list of answer options\n",
    "    answer_options = example['answers']\n",
    "    # get the ground truth label\n",
    "    label = example['label']\n",
    "    \n",
    "    # pass a list of contexts and a list of continuations to be scored\n",
    "    answer_scores = lm_scorer.conditional_score(\n",
    "        # format the question into a list of same length as the number of answer options\n",
    "        [question] * len(answer_options), \n",
    "        answer_options,\n",
    "    ) \n",
    "    # get the predicted answer (Hint: check above how we determine what the model predicts is the correct answer)\n",
    "    predicted_label = ### YOUR CODE HERE ###\n",
    "    # check if the prediction is correct\n",
    "    is_correct = predicted_label == label\n",
    "    correctness.append(is_correct)\n",
    "\n",
    "# compute the accuracy\n",
    "print(\"Accuracy: \", np.mean(correctness))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F1-score**:\n",
    "\n",
    "This is a score that is commonly used on *binary* tasks (i.e., tasks with only two possible answer options) instead of accuracy. It is calculated from the *precision* and *recall* of the test results.  The precision is the number of true positive results divided by the number of all samples predicted to be positive, including those not identified correctly. The recall is the number of true positive results divided by the number of all samples that should have been identified as positive. Here, positive and negative results refer to predictions in each of the two answer categories, respectively. \n",
    "\n",
    "The F1 score is the harmonic mean of the precision and recall. It thus symmetrically represents both precision and recall in one metric:\n",
    "$$F1 = 2 \\times \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}$$\n",
    "The more generic $F_{\\beta}$ score applies additional weights, valuing one of precision or recall more than the other.\n",
    "The highest possible value of an F-score is 1.0, indicating perfect precision and recall, and the lowest possible value is 0, if precision and recall are zero. \n",
    "\n",
    "We will use the BoolQ dataset from the SuperGLUE benchmark and evaluate GPT-2's performance in terms of F1 scores on it. This is a task wherein the model has to predict an answer (true/false) to a question, given context. Therefore, the positive prediction here will be \"true\", and the negative \"false\".\n",
    "\n",
    "You can find the test dataset [here](https://github.com/CogSciPrag/Understanding-LLMs-course/tree/main/understanding-llms/tutorials/files/super_glue_boolq.csv).\n",
    "We will retrieve the model's predictions similarly to the accuracy evaluation above. Specifically, we will retrieve the probabilities of \"true\" and \"false\", given the context and the question.\n",
    "\n",
    "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 7.1.3: Calculating F1 scores</span></strong>\n",
    ">\n",
    "> 1. Please complete the code below.\n",
    "> 2. Calculate the results. Does GPT-2 do well in this task? \n",
    "> 3. Evaluate the performance of the model using accuracy. What is the conceptual difference between the two results? Which one might be more reliable and why?\n",
    "> 4. Find out how to compute the F1 score with the `sklearn.metrics` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_boolq = pd.read_csv(\"files/super_glue_boolq.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>is_true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>English Football League play-offs -- Before th...</td>\n",
       "      <td>do away goals count in the league 2 playoffs</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1986 NBA Playoffs -- Second-year player Michae...</td>\n",
       "      <td>did the bulls get swept by the celtics</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Water table -- The groundwater may be from pre...</td>\n",
       "      <td>is the depth of the water table always the same</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FIFA World Cup qualification -- The hosts of t...</td>\n",
       "      <td>does the host country for the world cup get an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bad Lip Reading -- In December of 2015, Bad Li...</td>\n",
       "      <td>is seagulls stop it now a real song</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence1  \\\n",
       "0  English Football League play-offs -- Before th...   \n",
       "1  1986 NBA Playoffs -- Second-year player Michae...   \n",
       "2  Water table -- The groundwater may be from pre...   \n",
       "3  FIFA World Cup qualification -- The hosts of t...   \n",
       "4  Bad Lip Reading -- In December of 2015, Bad Li...   \n",
       "\n",
       "                                           sentence2  is_true  \n",
       "0       do away goals count in the league 2 playoffs        0  \n",
       "1             did the bulls get swept by the celtics        1  \n",
       "2    is the depth of the water table always the same        0  \n",
       "3  does the host country for the world cup get an...        1  \n",
       "4                is seagulls stop it now a real song        1  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect the dataset to understand its structure\n",
    "# if is_true = 1, it means that the answer to the question is \"True\"\n",
    "df_boolq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[-17.42737579345703, -16.241905212402344]\n",
      "1\n",
      "[-15.057735443115234, -12.546226501464844]\n",
      "2\n",
      "[-21.462608337402344, -17.94011688232422]\n",
      "3\n",
      "[-17.27320098876953, -13.855903625488281]\n",
      "4\n",
      "[-16.0611572265625, -13.052642822265625]\n",
      "5\n",
      "[-21.04007339477539, -17.46932601928711]\n",
      "6\n",
      "[-20.496910095214844, -18.105430603027344]\n",
      "7\n",
      "[-21.313457489013672, -16.074993133544922]\n",
      "8\n",
      "[-19.694324493408203, -16.635204315185547]\n",
      "9\n",
      "[-20.120223999023438, -16.774398803710938]\n",
      "10\n",
      "[-19.766246795654297, -16.45175552368164]\n",
      "11\n",
      "[-12.953720092773438, -10.639785766601562]\n",
      "12\n",
      "[-19.742599487304688, -17.128097534179688]\n",
      "13\n",
      "[-18.70870590209961, -14.96548080444336]\n",
      "14\n",
      "[-18.65872573852539, -15.077144622802734]\n",
      "15\n",
      "[-15.560127258300781, -14.01436996459961]\n",
      "16\n",
      "[-17.079673767089844, -13.116645812988281]\n",
      "17\n",
      "[-16.172325134277344, -12.018722534179688]\n",
      "18\n",
      "[-6.572422027587891, -4.708042144775391]\n",
      "19\n",
      "[-19.05229949951172, -14.953254699707031]\n",
      "20\n",
      "[-17.088207244873047, -13.424301147460938]\n",
      "21\n",
      "[-16.65937042236328, -14.122299194335938]\n",
      "22\n",
      "[-20.477622985839844, -16.771408081054688]\n",
      "23\n",
      "[-14.610404968261719, -11.105857849121094]\n",
      "24\n",
      "[-14.012039184570312, -12.632701873779297]\n",
      "25\n",
      "[-18.047531127929688, -12.973003387451172]\n",
      "26\n",
      "[-13.352188110351562, -11.440387725830078]\n",
      "27\n",
      "[-11.962120056152344, -9.965423583984375]\n",
      "28\n",
      "[-19.69597625732422, -16.76921844482422]\n",
      "29\n",
      "[-10.785560607910156, -9.131507873535156]\n",
      "30\n",
      "[-19.964595794677734, -19.589458465576172]\n",
      "31\n",
      "[-17.292922973632812, -12.3150634765625]\n",
      "32\n",
      "[-17.82465362548828, -14.675613403320312]\n",
      "33\n",
      "[-20.043743133544922, -17.162593841552734]\n",
      "34\n",
      "[-17.239776611328125, -12.720954895019531]\n",
      "35\n",
      "[-18.09362030029297, -13.978317260742188]\n",
      "36\n",
      "[-18.73880386352539, -14.9378662109375]\n",
      "37\n",
      "[-18.65554428100586, -14.260086059570312]\n",
      "38\n",
      "[-13.936920166015625, -11.569416046142578]\n",
      "39\n",
      "[-14.891647338867188, -11.013587951660156]\n",
      "40\n",
      "[-14.441810607910156, -11.268638610839844]\n",
      "41\n",
      "[-17.126510620117188, -12.641082763671875]\n",
      "42\n",
      "[-20.883712768554688, -16.64873504638672]\n",
      "43\n",
      "[-14.618003845214844, -11.437545776367188]\n",
      "44\n",
      "[-18.564163208007812, -15.432167053222656]\n",
      "45\n",
      "[-18.493553161621094, -15.753707885742188]\n",
      "46\n",
      "[-15.044010162353516, -13.169330596923828]\n",
      "47\n",
      "[-13.195945739746094, -10.841705322265625]\n",
      "48\n",
      "[-16.996444702148438, -14.891914367675781]\n",
      "49\n",
      "[-18.950836181640625, -16.479934692382812]\n",
      "50\n",
      "[-21.244091033935547, -16.39752960205078]\n",
      "51\n",
      "[-16.964462280273438, -13.166847229003906]\n",
      "52\n",
      "[-16.24566650390625, -11.460220336914062]\n",
      "53\n",
      "[-21.039867401123047, -16.14822006225586]\n",
      "54\n",
      "[-14.367713928222656, -10.457756042480469]\n",
      "55\n",
      "[-13.769309997558594, -9.90020751953125]\n",
      "56\n",
      "[-19.714218139648438, -14.811027526855469]\n",
      "57\n",
      "[-19.185298919677734, -16.052906036376953]\n",
      "58\n",
      "[-16.945903778076172, -11.918437957763672]\n",
      "59\n",
      "[-23.373245239257812, -18.3267822265625]\n",
      "60\n",
      "[-18.51396942138672, -13.988174438476562]\n",
      "61\n",
      "[-15.362049102783203, -12.548992156982422]\n",
      "62\n",
      "[-20.837547302246094, -16.188232421875]\n",
      "63\n",
      "[-8.051902770996094, -5.96734619140625]\n",
      "64\n",
      "[-17.085914611816406, -13.160751342773438]\n",
      "65\n",
      "[-17.074256896972656, -12.985671997070312]\n",
      "66\n",
      "[-16.35248565673828, -13.425788879394531]\n",
      "67\n",
      "[-21.37548828125, -19.3397216796875]\n",
      "68\n",
      "[-17.313377380371094, -14.401741027832031]\n",
      "69\n",
      "[-19.276268005371094, -15.045181274414062]\n",
      "70\n",
      "[-19.04431915283203, -15.583488464355469]\n",
      "71\n",
      "[-10.482040405273438, -8.354846954345703]\n",
      "72\n",
      "[-4.014129638671875, -3.08367919921875]\n",
      "73\n",
      "[-16.358566284179688, -13.471839904785156]\n",
      "74\n",
      "[-15.683181762695312, -11.894363403320312]\n",
      "75\n",
      "[-19.893524169921875, -18.042701721191406]\n",
      "76\n",
      "[-20.546951293945312, -17.486000061035156]\n",
      "77\n",
      "[-14.614448547363281, -9.968147277832031]\n",
      "78\n",
      "[-18.90819549560547, -13.551345825195312]\n",
      "79\n",
      "[-18.912139892578125, -16.021217346191406]\n",
      "80\n",
      "[-18.64044952392578, -15.55072021484375]\n",
      "81\n",
      "[-17.946563720703125, -15.719406127929688]\n",
      "82\n",
      "[-14.612627029418945, -10.604692459106445]\n",
      "83\n",
      "[-15.291641235351562, -11.985816955566406]\n",
      "84\n",
      "[-17.483783721923828, -15.652759552001953]\n",
      "85\n",
      "[-18.200592041015625, -14.324928283691406]\n",
      "86\n",
      "[-16.130905151367188, -12.729721069335938]\n",
      "87\n",
      "[-15.392131805419922, -11.934200286865234]\n",
      "88\n",
      "[-14.778404235839844, -12.993888854980469]\n",
      "89\n",
      "[-16.63009262084961, -14.325092315673828]\n",
      "90\n",
      "[-13.417922973632812, -11.575004577636719]\n",
      "91\n",
      "[-12.859764099121094, -10.925956726074219]\n",
      "92\n",
      "[-15.260292053222656, -12.442466735839844]\n",
      "93\n",
      "[-13.964763641357422, -12.041851043701172]\n",
      "94\n",
      "[-17.408763885498047, -12.933784484863281]\n",
      "95\n",
      "[-19.331531524658203, -15.93661117553711]\n",
      "96\n",
      "[-18.630706787109375, -12.72500991821289]\n",
      "97\n",
      "[-16.519378662109375, -13.642608642578125]\n",
      "98\n",
      "[-11.223617553710938, -9.46249008178711]\n",
      "99\n",
      "[-19.06757354736328, -14.309574127197266]\n",
      "100\n",
      "[-18.699806213378906, -15.147247314453125]\n",
      "101\n",
      "[-13.960700988769531, -11.122329711914062]\n",
      "102\n",
      "[-17.13622283935547, -14.08148193359375]\n",
      "103\n",
      "[-21.014606475830078, -17.656681060791016]\n",
      "104\n",
      "[-19.707870483398438, -14.320297241210938]\n",
      "105\n",
      "[-18.437545776367188, -13.394500732421875]\n",
      "106\n",
      "[-16.103111267089844, -13.032943725585938]\n",
      "107\n",
      "[-15.40341567993164, -11.426151275634766]\n",
      "108\n",
      "[-16.366600036621094, -13.773689270019531]\n",
      "109\n",
      "[-17.354053497314453, -15.11752700805664]\n",
      "110\n",
      "[-15.91389274597168, -13.455747604370117]\n",
      "111\n",
      "[-17.628761291503906, -13.579872131347656]\n",
      "112\n",
      "[-19.781463623046875, -16.379234313964844]\n",
      "113\n",
      "[-20.757640838623047, -17.02474594116211]\n",
      "114\n",
      "[-16.720191955566406, -12.598007202148438]\n",
      "115\n",
      "[-18.621665954589844, -15.120162963867188]\n",
      "116\n",
      "[-11.99224853515625, -9.61697006225586]\n",
      "117\n",
      "[-23.29724884033203, -18.182388305664062]\n",
      "118\n",
      "[-18.33428192138672, -16.008705139160156]\n",
      "119\n",
      "[-13.780296325683594, -10.9896240234375]\n",
      "120\n",
      "[-13.817855834960938, -10.902637481689453]\n",
      "121\n",
      "[-13.277290344238281, -9.656326293945312]\n",
      "122\n",
      "[-17.660362243652344, -12.859664916992188]\n",
      "123\n",
      "[-18.8538818359375, -15.515480041503906]\n",
      "124\n",
      "[-16.334762573242188, -12.685928344726562]\n",
      "125\n",
      "[-14.819366455078125, -10.509048461914062]\n",
      "126\n",
      "[-7.011222839355469, -4.947059631347656]\n",
      "127\n",
      "[-16.937171936035156, -14.030342102050781]\n",
      "128\n",
      "[-17.33946990966797, -14.341297149658203]\n",
      "129\n",
      "[-11.124208450317383, -9.050552368164062]\n",
      "130\n",
      "[-13.988105773925781, -12.357337951660156]\n",
      "131\n",
      "[-17.63681411743164, -15.154857635498047]\n",
      "132\n",
      "[-22.47441864013672, -16.655921936035156]\n",
      "133\n",
      "[-17.124794006347656, -14.74200439453125]\n",
      "134\n",
      "[-16.577587127685547, -12.26192855834961]\n",
      "135\n",
      "[-17.5372314453125, -15.083229064941406]\n",
      "136\n",
      "[-17.353641510009766, -12.04001235961914]\n",
      "137\n",
      "[-13.95998764038086, -12.295356750488281]\n",
      "138\n",
      "[-16.974624633789062, -14.93484115600586]\n",
      "139\n",
      "[-16.31673812866211, -13.873340606689453]\n",
      "140\n",
      "[-16.391883850097656, -13.910049438476562]\n",
      "141\n",
      "[-19.08356475830078, -15.889518737792969]\n",
      "142\n",
      "[-17.0601806640625, -12.285118103027344]\n",
      "143\n",
      "[-13.160537719726562, -10.576026916503906]\n",
      "144\n",
      "[-17.35700225830078, -14.172462463378906]\n",
      "145\n",
      "[-16.590545654296875, -13.416786193847656]\n",
      "146\n",
      "147\n",
      "[-22.303001403808594, -16.998680114746094]\n",
      "148\n",
      "[-15.868148803710938, -12.88885498046875]\n",
      "149\n",
      "[-8.237396240234375, -6.344413757324219]\n",
      "150\n",
      "[-16.890300750732422, -13.62948226928711]\n",
      "151\n",
      "[-20.25031280517578, -14.763603210449219]\n",
      "152\n",
      "[-19.192649841308594, -14.912628173828125]\n",
      "153\n",
      "[-22.308177947998047, -20.18062973022461]\n",
      "154\n",
      "[-17.852783203125, -14.265174865722656]\n",
      "155\n",
      "[-16.109596252441406, -13.638427734375]\n",
      "156\n",
      "[-14.641952514648438, -11.593704223632812]\n",
      "157\n",
      "[-16.512714385986328, -13.98422622680664]\n",
      "158\n",
      "[-12.834674835205078, -10.558582305908203]\n",
      "159\n",
      "[-15.806312561035156, -12.503364562988281]\n",
      "160\n",
      "[-19.341888427734375, -16.227943420410156]\n",
      "161\n",
      "[-20.920928955078125, -18.037403106689453]\n",
      "162\n",
      "[-16.26563262939453, -14.655281066894531]\n",
      "163\n",
      "[-14.978218078613281, -14.404998779296875]\n",
      "164\n",
      "[-12.176506042480469, -9.195236206054688]\n",
      "165\n",
      "[-19.60102081298828, -16.420387268066406]\n",
      "166\n",
      "[-12.859664916992188, -10.344795227050781]\n",
      "167\n",
      "[-14.3695068359375, -10.367652893066406]\n",
      "168\n",
      "[-14.48956298828125, -11.595573425292969]\n",
      "169\n",
      "[-14.136646270751953, -11.752605438232422]\n",
      "170\n",
      "[-18.847198486328125, -16.00091552734375]\n",
      "171\n",
      "[-16.125957489013672, -13.130084991455078]\n",
      "172\n",
      "[-15.959575653076172, -12.94918441772461]\n",
      "173\n",
      "[-15.269180297851562, -11.699958801269531]\n",
      "174\n",
      "[-13.618045806884766, -11.005485534667969]\n",
      "175\n",
      "[-18.881698608398438, -17.084938049316406]\n",
      "176\n",
      "[-17.193256378173828, -13.976776123046875]\n",
      "177\n",
      "[-17.769485473632812, -14.574653625488281]\n",
      "178\n",
      "[-8.909923553466797, -7.197353363037109]\n",
      "179\n",
      "180\n",
      "[-14.975181579589844, -12.571826934814453]\n",
      "181\n",
      "[-13.545047760009766, -9.780536651611328]\n",
      "182\n",
      "[-15.852352142333984, -11.26153564453125]\n",
      "183\n",
      "[-14.738655090332031, -11.337867736816406]\n",
      "184\n",
      "[-19.045166015625, -15.276039123535156]\n",
      "185\n",
      "[-7.010898590087891, -6.031829833984375]\n",
      "186\n",
      "[-15.728515625, -11.659950256347656]\n",
      "187\n",
      "[-0.0859375, -0.042346954345703125]\n",
      "188\n",
      "[-15.374181747436523, -14.542078018188477]\n",
      "189\n",
      "[-22.033355712890625, -16.549896240234375]\n",
      "190\n",
      "[-17.784320831298828, -14.55813217163086]\n",
      "191\n",
      "[-18.58056640625, -14.229923248291016]\n",
      "192\n",
      "[-17.11669921875, -14.053070068359375]\n",
      "193\n",
      "[-16.03753662109375, -12.079986572265625]\n",
      "194\n",
      "[-19.133041381835938, -14.399658203125]\n",
      "195\n",
      "[-18.821537017822266, -15.280792236328125]\n",
      "196\n",
      "[-17.987903594970703, -13.18087387084961]\n",
      "197\n",
      "[-15.882564544677734, -12.683570861816406]\n",
      "198\n",
      "[-20.90009307861328, -17.661582946777344]\n",
      "199\n",
      "[-17.418678283691406, -14.193099975585938]\n"
     ]
    }
   ],
   "source": [
    "predicted_answer= []\n",
    "true_answers = []\n",
    "\n",
    "for i, r in df_boolq[:200].iterrows():\n",
    "    # get the context for the question\n",
    "    context = r['sentence1']\n",
    "    # get the text of the question\n",
    "    question = r['sentence2']\n",
    "    # construct the list of answer options\n",
    "    answer_options = [\"False\", \"True\"]\n",
    "    # get the ground truth label\n",
    "    true_answer = r[\"is_true\"]\n",
    "    \n",
    "    # pass a list of contexts and a list of continuations to be scored\n",
    "    try:\n",
    "        answer_scores = lm_scorer.conditional_score(\n",
    "            # format the context + question into a list of same length as the number of answer options\n",
    "            [context + \" \" + question + \"?\"] * len(answer_options), \n",
    "            answer_options,\n",
    "        ) \n",
    "    except:\n",
    "        continue\n",
    "    # get the predicted answer (Hint: check above how we determine what the model predicts is the correct answer)\n",
    "    predicted_label = ### YOUR CODE HERE ###\n",
    "    # record the predicted answer\n",
    "    predicted_answer.append(predicted_label)\n",
    "    true_answers.append(true_answer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positive:  130\n",
      "False positive:  68\n",
      "F1 score:  0.7926829268292683\n"
     ]
    }
   ],
   "source": [
    "# compute the F1 score\n",
    "true_positive = sum([(i == j) & (i == 1) for i, j in zip(predicted_answer, true_answers)])\n",
    "print(\"True positive: \", true_positive)\n",
    "false_positive = sum([(i != j) & (i == 1) for i, j in zip(predicted_answer, true_answers)]) \n",
    "print(\"False positive: \", false_positive)\n",
    "false_negative = sum([(i != j) & (i == 0) for i, j in zip(predicted_answer, true_answers)])\n",
    "f1_score = # YOUR CODE HERE\n",
    "print(\"F1 score: \", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLG metrics**: The lecture discussed the common metrics for generation evaluation: BLEU, ROUGE and METEOR. We already used ROUGE in task 2 of HW 3. These metrics all check whether the predicted text overlaps with ground truth texts. Often different overlap measures are used; for instance, overlaps of unigrams, bigrams or trigrams can be computed. \n",
    "These metrics originate from summarization and machine translation, where corpora of reference human summaries or translations. These are also applied to any other generation tasks, too, as long as reference texts are available. \n",
    "\n",
    "Below is space for trying out the BLEU score, in order to evaluate the translation predicted by FLAN-T5 small. \n",
    "\n",
    "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 7.1.3: Calculating NLG scores</span></strong>\n",
    ">\n",
    "> 1. Please complete the code below by referring to the docs [here](https://huggingface.co/spaces/evaluate-metric/bleu).\n",
    "> 2. Calculate the results. What happens if you change the values of the `max_order` parameter, for this example and in general?\n",
    "> 3. If possible, try this out with a different language pair / a different sentence pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the implementation of the bleu score computation\n",
    "from torchtext.data.metrics import bleu_score\n",
    "# load model and tokenizer\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "tokenizer_t5 = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "model_t5 = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\")\n",
    "\n",
    "# define example sentences for translating from English to German\n",
    "text_en = \"All of the others were of a different opinion.\"\n",
    "text_de = \"Alle anderen waren anderer Meinung.\"\n",
    "# define task \n",
    "prefix = \"Translate to German: \"\n",
    "\n",
    "# encode the source and the target sentences\n",
    "encoding_en = tokenizer_t5(\n",
    "    [prefix + text_en],\n",
    "    return_tensors=\"pt\",\n",
    ").input_ids\n",
    "# we don't need the task prefix before the target\n",
    "encoding_de = tokenizer_t5(\n",
    "    [text_de],\n",
    "    return_tensors=\"pt\",\n",
    ").input_ids\n",
    "\n",
    "# predict with model\n",
    "predicted_de = model_t5.generate(encoding_en)\n",
    "\n",
    "\n",
    "\n",
    "# decode the prediction\n",
    "predicted_decoded_de = tokenizer_t5.decode(\n",
    "    predicted_de[0],\n",
    "    skip_special_tokens=True,\n",
    ")\n",
    "print(\"Predicted translation: \", predicted_decoded_de)\n",
    "\n",
    "# compute BLEU for the prediction\n",
    "### YOUR CODE CALLING THE HELPER ABOVE GOES HERE ###\n",
    "bleu = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**\n",
    "* Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlook\n",
    "\n",
    "**TODO**:\n",
    "* Pseudo-LL for masked LMs\n",
    "* Jenn's metalinguistic paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine psychology\n",
    "\n",
    "However, more recently, work more informed by human language use and processing has compared LLMs’ performance to aspects of human behavior. Here, the assessment of LLMs is guided more by the question of how human-like certain aspects of its performance are.\n",
    "\n",
    "**TODO**\n",
    "* compute grammaticality judgements and own intution -- interpret\n",
    "* hands-on methods of comparing quantitatively human and machine data: Ethan's code and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import minicons\n",
    "import openai\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import argparse \n",
    "import json\n",
    "from pprint import pprint\n",
    "# set openAI key in separate .env file w/ content\n",
    "load_dotenv() \n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# read test cases with single token prediction\n",
    "grammaticality_test_cases = pd.read_csv(\"data/grammaticality_tests.csv\")\n",
    "\n",
    "def get_surprisal(\n",
    "        masked_sequence, \n",
    "        full_sequence,\n",
    "        preface = 'start ', \n",
    "        model_name =  \"text-davinci-003\", \n",
    "        mask_token = \"[MASK]\",\n",
    "        return_region_surprisal=True,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Helper for retrieving surprisal of different response types from GPT-3.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    masked_sequence: str\n",
    "        Sequence with masked critical region.\n",
    "    full_sequence: str\n",
    "        Full sequence with crticial region.\n",
    "    preface: str\n",
    "        Preface (instructions or few-shot) to be added to the sequence.\n",
    "    model_name: str\n",
    "        Name of the GPT-3 model to be used.\n",
    "    mask_token: str\n",
    "        Token used for masking the critical region.\n",
    "    return_region_surprisal: bool\n",
    "        Whether to return surprisal of the critical region only or average for full sequence.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    mask_surprisals: list\n",
    "        Surprisal of the critical region or average for full sentence.\n",
    "    \"\"\"\n",
    "    # get log probs for sequence\n",
    "    if model_name not in [\"gpt-3.5-turbo\", \"gpt-4\"]:\n",
    "        response = openai.Completion.create(\n",
    "                engine      = model_name, \n",
    "                prompt      = preface + full_sequence,\n",
    "                max_tokens  = 0, # sample 0 new tokens, i.e., only get scores of input sentence\n",
    "                temperature = 1, \n",
    "                logprobs    = 0, \n",
    "                echo        = True,\n",
    "            ) \n",
    "        pprint(response)\n",
    "    else:\n",
    "        raise ValueError(\"GPT-4 and turbo cannot return log probs!\")\n",
    "\n",
    "    text_offsets = response.choices[0]['logprobs']['text_offset']\n",
    "    # allow to use few shot examples\n",
    "    if preface != '':\n",
    "        cutIndex = text_offsets.index(max(i for i in text_offsets if i <= len(preface))) \n",
    "        endIndex = response.usage.total_tokens\n",
    "    else:\n",
    "        cutIndex = 0\n",
    "        endIndex = len(response.choices[0][\"logprobs\"][\"tokens\"])\n",
    "    answerTokens = response.choices[0][\"logprobs\"][\"tokens\"][cutIndex:endIndex]\n",
    "    answerTokenLogProbs = response.choices[0][\"logprobs\"][\"token_logprobs\"][cutIndex:endIndex] \n",
    "    # retrieve critical region surprisal\n",
    "    if return_region_surprisal:\n",
    "        # get target region surprisal\n",
    "        # for grammaticality judgment comparison\n",
    "        # retrieve the target region which is masked in the masked sequence\n",
    "        # get its index in the full sentence\n",
    "        mask_ind = [i for i, e in \n",
    "                    enumerate(masked_sequence.replace(\".\", \" .\").split(\" \")) \n",
    "                    if e == mask_token\n",
    "                    ]\n",
    "        # get target region\n",
    "        masked_words = [full_sequence.replace(\".\", \" .\").split(\" \")[mask_i] for mask_i in mask_ind]\n",
    "        # get tokens corresponding to the target region\n",
    "        # and handle subword tokenization of GPT\n",
    "        mask_log_probs = []\n",
    "        mask_log_prob = np.nan\n",
    "        for masked_word in masked_words:\n",
    "            for i, t in enumerate(answerTokens):\n",
    "                if t.strip() == masked_word.strip():\n",
    "                    mask_log_prob = answerTokenLogProbs[i]\n",
    "                    mask_log_probs.append(mask_log_prob)\n",
    "                elif t.strip() in masked_word:\n",
    "                    if t.strip() + answerTokens[i+1] in masked_word:\n",
    "                        mask_log_prob = answerTokenLogProbs[i]\n",
    "                        mask_log_probs.append(mask_log_prob)\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "        mask_surprisals = [-m for m in mask_log_probs]\n",
    "    # get full sentence surprisal\n",
    "    else:\n",
    "        mask_surprisals = [- np.mean(\n",
    "            np.asarray(answerTokenLogProbs)\n",
    "        )]\n",
    "\n",
    "    return mask_surprisals\n",
    "\n",
    "def compare_surprisals(row, return_region_surprisal):\n",
    "    \"\"\"\n",
    "    Helper for comparing surprisals of grammatical and ungrammatical sentences.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    row: pd.Series\n",
    "        Row of the dataframe containing the test case.\n",
    "    return_region_surprisal: bool\n",
    "        Whether to return surprisal of the critical region only or average for full sentence.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    is_grammatical: bool\n",
    "        Whether the grammatical sentence has lower surprisal than the ungrammatical one.\n",
    "    \"\"\"\n",
    "    # get surprisal of grammatical sentence\n",
    "    grammatical_surprisal = get_surprisal(\n",
    "        row[\"masked_sentence\"],\n",
    "        row[\"grammatical_sentence\"],\n",
    "        return_region_surprisal=return_region_surprisal,\n",
    "    )\n",
    "    print(f\"--- Surprisal of grammatical sentence {row['grammatical_sentence']}: {grammatical_surprisal} ---\\n\\n\")\n",
    "    # get surprisal of ungrammatical sentence\n",
    "    ungrammatical_surprisal = get_surprisal(\n",
    "        row[\"masked_sentence\"],\n",
    "        row[\"ungrammatical_sentence\"],\n",
    "        return_region_surprisal=return_region_surprisal,\n",
    "    )\n",
    "    print(f\"--- Surprisal of ungrammatical sentence {row['ungrammatical_sentence']}: {ungrammatical_surprisal} ---\\n\\n\")\n",
    "    \n",
    "    # check LM accuracy (in terms of surprisal)\n",
    "    is_grammatical = all([g < u for g, u in zip(grammatical_surprisal, ungrammatical_surprisal)])\n",
    "    return is_grammatical\n",
    "\n",
    "print(\"Gramamticality test cases: \\n\\n\", grammaticality_test_cases.head(10))\n",
    "# call surprisal computation for single test cases from the slides\n",
    "print(\"--- Agreement test case --- \\n Is grammatical sentence less surprising than ungrammatical one?\", \n",
    "      compare_surprisals(grammaticality_test_cases.iloc[0], return_region_surprisal=False), \"\\n\\n\")\n",
    "\n",
    "print(\"--- Reflexive test case --- \\n Is grammatical sentence less surprising than ungrammatical one?\", \n",
    "      compare_surprisals(grammaticality_test_cases.iloc[11], return_region_surprisal=False), \"\\n\\n\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Runs all test cases.\n",
    "    \"\"\"\n",
    "    for _, r in grammaticality_test_cases.iterrows():\n",
    "        print(\"--------------------\")\n",
    "        is_grammatical = compare_surprisals(r, return_region_surprisal=False)\n",
    "        print(f\"Grammatical sentence: {r['grammatical_sentence']} \\n\\n\")\n",
    "        print(f\"Ungrammatical sentence: {r['ungrammatical_sentence']} \\n\\n\")\n",
    "        # check LM accuracy (in terms of surprisal)\n",
    "        print(\"Is the grammatical sentence more likely than the ungrammatical one under LM?\", \n",
    "              is_grammatical)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SuperGLUE\n",
    "\n",
    "Finally, we will get our hands dirty with evaluating LLMs which already have been trained. In this task, we will use a few tasks from one of the most-used LM benchmarks, the SuperGLUE benchmark:\n",
    "\n",
    "    a natural language inference (NLI) task “rte”,\n",
    "\n",
    "        a task wherein the model has to predict whether a second sentence is entailed by the first one (i.e., predict the label ‘entailment’ or ‘no entailment’)\n",
    "\n",
    "    a question answering task “boolq”,\n",
    "\n",
    "        a task wherein the model has to predict an answer (yes/no) to a question, given context\n",
    "\n",
    "    and a sentence continuation task “copa”.\n",
    "\n",
    "        a task wherein the model has to select one of two sentences as the more plausible continuation given an input sentence.\n",
    "\n",
    "We will be using (subset of) the validation splits of the tasks for our evaluation.\n",
    "\n",
    "With the introduction of first language models like BERT, a common approach to using benchmarks like SuperGLUE was to fine-tune the pretrained model on the train split of the benchmark datasets, and then use the test splits for evaluation. With SOTA LLMs, it is more common to do zero- or few-shot evaluation where the model has to, e.g., predict labels or select answer options without special fine-tuning, just given instructions.\n",
    "\n",
    "We are also not going to fine-tune our model on these specific tasks. Instead, as introduced in class, we are going to compare the log probabilities of different answer options (e.g., log probabilities of “entailment” vs. “no entailment” following a pair of sentences from the RTE-task). With this method, the assumption is that a model’s output prediction for a particular trial is correct iff: \n",
    "\n",
    "$$\\log P_{LM}(\\text{correct label} \\mid \\text{context}) >  \\log P_{LM}(\\text{incorrect label} \\mid \\text{context})$$\n",
    "\n",
    "For tasks like “copa” where there is no single label but instead a sentence continuation, we are going to compute the average token log probability as a single-number representation of the continuation. Here, the model’s prediction will count as correct iff the average log probability of the correct continuation sentence will be higher, given the input, than for the incorrect continuation. We will not using task instructions in our evaluation since the model wasn’t fine-tuned on instruction-following.\n",
    "\n",
    "Your job is to complete the code below, evaluate the model which you have fine-tuned above and summarize the results you find in a few words (see below for more detailed step-by-step instructions). If you have issues with the previous task and cannot use your own fine-tuned model, please use the initial IMDB fine-tuned GPT-2 with which we initialized the policy in exercise 2. Please indicate which model you are testing on Moodle in the respective exercise responses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "understanding_llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
