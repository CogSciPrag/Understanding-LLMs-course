{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sheet 7.1: Behavioral assessment & Evaluation\n",
    "======\n",
    "**Author**: Polina Tsvilodub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sheet focuses on evaluating the input-output (I/O) behavior of LLMs. Inspired by experimental paradigms / the terminology in cognitive science and psychology which investigate a blackbox (the human mind) via looking at the behavior across different interesting conditions (inputs), such assessment of LLMs (also blackboxes) can be called \"behavioral assessment\". This approach can be seen as one piece that should work in combination with attribution methods discussed in the previous sheet in order to provide fuller understanding of what LLMs can or cannot do (I/O testing) and how they do it (attributions). \n",
    "Following the structure of the lecture, we will first look at practical aspects of benchmark testing, and then look at \"machine psychology\", which often draws on the same methods but addresses somewhat different research questions.\n",
    "\n",
    "Therefore, the learning goals of this sheet are:\n",
    "* look at examples of a few different benchmarks and how they are usually constructed\n",
    "* become familiar with standard evaluation metrics and methods used for evaluating LLMs on benchmarks (these include PPL, log probability based scores, accuracy, F1, free generation etc)\n",
    "* become familiar with different more advanced eval measures HELM  \n",
    "* look at examples of machine psychology and how, in practice, LLM performance can be compared to various human data\n",
    "* how LLMs can be tested as \"psychology subjects\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark testing\n",
    "\n",
    "Such I/O evaluations are the most common approach to LLM evaluation. Taking a more technical / engineering-oriented perspective which aims at building LLMs for specific application, it is very common to make use of large benchmark datasets which are designed to test models’ performance on a variety of tasks in an automated way. This is often done by checking the models’ outputs against ground truth answers or by computing standard scores for certain datasets. Therefore, quality of LLMs is measured by their scores on these benchmarks. \n",
    "\n",
    "Initially, these benchmarks were designed to test LLMs’ linguistic performance since the goal of building the model is a system that predict grammatical and fluent natural language. Therefore, some first benchmarks (or, commonly used textual datasets) are, for instance, Wikipedia texts, the Penn Treebank, and the GLUE benchmark. Wikipedia texts are often used for measuring the perplexity of the model on this standard text (see below for details). The Penn Treebank was often used for fine-tuning or evaluating models, e.g., on part-of-speech tagging as an approximation of syntactic performance, while the GLUE benchmark contains tasks which are supposed to approximate (semantic) natural language understanding in the form of paraphrase tasks, sentiment classification, natural language inference tasks etc.\n",
    "\n",
    "Now recent LLMs have shown perhaps unexpectedly impressive generalization to tasks which seem to require more than linguistic fluency, like solving math and reasoning problems. Therefore, more recent benchmarks incorporate tests of various tasks going beyond linguistic capability. Two of the most widely used benchmarks include the MMLU and the BIG-Bench datasets.\n",
    "Given that SOTA LLMs are also often designed as assisstants and embedded in user-facing applications, it also became crucial to evaluate potenital social impacts that LLMs might exhibit with their outputs, like assessing biases and toxicity of the generations. To this end, specialized benchmarks like RealToxicity or WinoGender were created.\n",
    "**TODO:** refs.\n",
    "\n",
    "**TODO** NLG benchmarks.\n",
    "\n",
    "**TODO:** complete: model refers to trained models which are evaluated with respect to their performance. However, if one wanted to track the performance during training, one could also run evaluations on intermediate model checkpoints during training too. Just note that the model is \"frozen\" and runs in inference mode during all of the testing described in this sheet.\n",
    "\n",
    "The core advantage of benchmarks: (somewhat of a) standardization of the evaluation procedure, large scale (higher coverage, more reliable results), and design to be evaluated with easy to compute automatically evaluation metrics. You have heard about them in the lecture; we will recap these below and then work with them in practice.\n",
    "\n",
    "### Metrics\n",
    "\n",
    "**Perplexity**: avg surprisal that the trained model assigns to some piece of text. It is computed as:\n",
    "$$PPL_{LM}(x_0 ... x_n) = \\frac{1}{n}\\sum_i=0^n -P_{LM}(x_i \\mid x_{<i}) $$\n",
    "\n",
    "Note that this is only applicable to causal language models. This is the metric commonly used, e.g., on the Wikipedia texts. For instance, the PPL of GPT-2 on XXXX is YYY **TODO**. The idea is that an ideal model should have a perplexity as close to 0 as possible for a naturally occurring text that it has learned, thereby approximating good fit to the \"ground truth distribution of natural language\".\n",
    "\n",
    "Below is some code for computing the perplexity of different sizes of GPT-2 for an exerpt from Wkipedia.\n",
    "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 7.1.1: Calculating perplexity</span></strong>\n",
    ">\n",
    "> 1. Please complete the code below. (Hint: only one simple transformation is required in order to calculate the perplexity from the NLL loss)\n",
    "> 2. Compare the results for the models of different sizes. Does their comparison (ordering) match your intuition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perplexity evaluation\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "test = load_dataset(\"wikitext\", 'wikitext-2-raw-v1', split=\"test\")\n",
    "encodings = tokenizer(\n",
    "    \"\\n\\n\".join(test[\"text\"]), \n",
    "    return_tensors=\"pt\",\n",
    ").input_ids\n",
    "# select a part of the text\n",
    "input_tokens = encodings[:,10:50]\n",
    "\n",
    "# load models of different sizes\n",
    "model_s = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "model_xl = AutoModelForCausalLM.from_pretrained(\"gpt2-xl\")\n",
    "\n",
    "output_s = model_s(input_tokens, labels = input_tokens)\n",
    "output_xl = model_s(input_tokens, labels = input_tokens)\n",
    "print(\"Average NLL for wikipedia chunk under small model \", output_s.loss.item())\n",
    "print(\"Average NLL for wikipedia chunk under xl model \", output_xl.loss.item())\n",
    "\n",
    "### your code for computing the perplexity goes here ###\n",
    "perplexity_s =\n",
    "perplexity_xl =\n",
    "\n",
    "print(f\"PPL of smaller model: {perplexity_s}, PPL of larger model: {perplexity_xl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Accuracy**: this is a standard metric widely used in many domains, not only NLP. It computes the proportion of correct responses in a set of tasks. Presupposes that there is a single correct answer for a given input. We have seen in the lecture that one way to compute accuracy is to score each answer option, given the input, under the LLM, and retreive the predicted options via $argmax$; i.e., take the option for which the model assigned the highest (log) probability to be the chosen option. If this option is the ground truth option, the model's prediction is correct for this test item (i.e., correctness = 1); otherwise, correctness = 0. Accuracy is then the average correctness across all the test items in the benchmark. The lecture pointed out limitations of the argmax approach.\n",
    "\n",
    "The advantage of this approach is that it makes sure to score only the available answer options under the model, which is an especially important constraint for weaker models. However, SOTA more powerful LLMs, especially if they are instruction-tuned are often also tested via text generation. I.e., the input is given with an appropriate instruction, and the model's generated text is evaluated via string matching. If the correct answer option was generated, the model's correctness is 1 for this trial, and 0 otherwise. \n",
    "\n",
    "Below is some code exemplifying evaluating a model on a question answering benchmark which we have already used in the homework, via scoring answers under the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F1-score**:\n",
    "\n",
    "**correlation**:\n",
    "\n",
    "Better explain some QA benchmarks which we have seen in the HW tasks already. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlook\n",
    "\n",
    "* Pseudo-LL for masked LMs\n",
    "* Jenn's metalinguistic paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLG scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the implementation of the bleu score computation\n",
    "from torchtext.data.metrics import bleu_score\n",
    "# load model and tokenizer\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "tokenizer_t5 = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model_t5 = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "# define example sentences for translating from English to German\n",
    "text_en = \"All of the others were of a different opinion.\"\n",
    "text_de = \"Alle anderen waren anderer Meinung.\"\n",
    "# define task \n",
    "prefix = \"translate English to German: \"\n",
    "\n",
    "# define helper function taking prediction and gold standard\n",
    "def compute_bleu(prediction, target, n):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    ----------\n",
    "    prediction: str\n",
    "        Predicted translation.\n",
    "    target: str\n",
    "        Gold standard translation.\n",
    "    n: int\n",
    "        n-gram over which to compute BLEU.\n",
    "    Returns:\n",
    "    -------\n",
    "    score: float\n",
    "        BLEU-n score.\n",
    "    \"\"\"\n",
    "    score = bleu_score(\n",
    "        [prediction.split()], \n",
    "        [[target.split()]], \n",
    "        max_n=n, \n",
    "        weights = [1/n] * n,\n",
    "    )\n",
    "    return score \n",
    "\n",
    "# encode the source and the target sentences\n",
    "encoding_en = tokenizer_t5(\n",
    "    [prefix + text_en],\n",
    "    return_tensors=\"pt\",\n",
    ").input_ids\n",
    "# we don't need the task prefix before the target\n",
    "encoding_de = tokenizer_t5(\n",
    "    [text_de],\n",
    "    return_tensors=\"pt\",\n",
    ").input_ids\n",
    "\n",
    "# predict with model\n",
    "predicted_de = model_t5.generate(encoding_en)\n",
    "\n",
    "print(\"Predicted translation: \", predicted_decoded_de)\n",
    "\n",
    "# decode the prediction\n",
    "predicted_decoded_de = tokenizer_t5.decode(\n",
    "    predicted_de[0],\n",
    "    skip_special_tokens=True,\n",
    ")\n",
    "\n",
    "# compute BLEU-1 for the prediction\n",
    "### YOUR CODE CALLING THE HELPER ABOVE GOES HERE ###\n",
    "# bleu1 = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install minicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import minicons\n",
    "import openai\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import argparse \n",
    "import json\n",
    "from pprint import pprint\n",
    "# set openAI key in separate .env file w/ content\n",
    "load_dotenv() \n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# read test cases with single token prediction\n",
    "grammaticality_test_cases = pd.read_csv(\"data/grammaticality_tests.csv\")\n",
    "\n",
    "def get_surprisal(\n",
    "        masked_sequence, \n",
    "        full_sequence,\n",
    "        preface = 'start ', \n",
    "        model_name =  \"text-davinci-003\", \n",
    "        mask_token = \"[MASK]\",\n",
    "        return_region_surprisal=True,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Helper for retrieving surprisal of different response types from GPT-3.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    masked_sequence: str\n",
    "        Sequence with masked critical region.\n",
    "    full_sequence: str\n",
    "        Full sequence with crticial region.\n",
    "    preface: str\n",
    "        Preface (instructions or few-shot) to be added to the sequence.\n",
    "    model_name: str\n",
    "        Name of the GPT-3 model to be used.\n",
    "    mask_token: str\n",
    "        Token used for masking the critical region.\n",
    "    return_region_surprisal: bool\n",
    "        Whether to return surprisal of the critical region only or average for full sequence.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    mask_surprisals: list\n",
    "        Surprisal of the critical region or average for full sentence.\n",
    "    \"\"\"\n",
    "    # get log probs for sequence\n",
    "    if model_name not in [\"gpt-3.5-turbo\", \"gpt-4\"]:\n",
    "        response = openai.Completion.create(\n",
    "                engine      = model_name, \n",
    "                prompt      = preface + full_sequence,\n",
    "                max_tokens  = 0, # sample 0 new tokens, i.e., only get scores of input sentence\n",
    "                temperature = 1, \n",
    "                logprobs    = 0, \n",
    "                echo        = True,\n",
    "            ) \n",
    "        pprint(response)\n",
    "    else:\n",
    "        raise ValueError(\"GPT-4 and turbo cannot return log probs!\")\n",
    "\n",
    "    text_offsets = response.choices[0]['logprobs']['text_offset']\n",
    "    # allow to use few shot examples\n",
    "    if preface != '':\n",
    "        cutIndex = text_offsets.index(max(i for i in text_offsets if i <= len(preface))) \n",
    "        endIndex = response.usage.total_tokens\n",
    "    else:\n",
    "        cutIndex = 0\n",
    "        endIndex = len(response.choices[0][\"logprobs\"][\"tokens\"])\n",
    "    answerTokens = response.choices[0][\"logprobs\"][\"tokens\"][cutIndex:endIndex]\n",
    "    answerTokenLogProbs = response.choices[0][\"logprobs\"][\"token_logprobs\"][cutIndex:endIndex] \n",
    "    # retrieve critical region surprisal\n",
    "    if return_region_surprisal:\n",
    "        # get target region surprisal\n",
    "        # for grammaticality judgment comparison\n",
    "        # retrieve the target region which is masked in the masked sequence\n",
    "        # get its index in the full sentence\n",
    "        mask_ind = [i for i, e in \n",
    "                    enumerate(masked_sequence.replace(\".\", \" .\").split(\" \")) \n",
    "                    if e == mask_token\n",
    "                    ]\n",
    "        # get target region\n",
    "        masked_words = [full_sequence.replace(\".\", \" .\").split(\" \")[mask_i] for mask_i in mask_ind]\n",
    "        # get tokens corresponding to the target region\n",
    "        # and handle subword tokenization of GPT\n",
    "        mask_log_probs = []\n",
    "        mask_log_prob = np.nan\n",
    "        for masked_word in masked_words:\n",
    "            for i, t in enumerate(answerTokens):\n",
    "                if t.strip() == masked_word.strip():\n",
    "                    mask_log_prob = answerTokenLogProbs[i]\n",
    "                    mask_log_probs.append(mask_log_prob)\n",
    "                elif t.strip() in masked_word:\n",
    "                    if t.strip() + answerTokens[i+1] in masked_word:\n",
    "                        mask_log_prob = answerTokenLogProbs[i]\n",
    "                        mask_log_probs.append(mask_log_prob)\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "        mask_surprisals = [-m for m in mask_log_probs]\n",
    "    # get full sentence surprisal\n",
    "    else:\n",
    "        mask_surprisals = [- np.mean(\n",
    "            np.asarray(answerTokenLogProbs)\n",
    "        )]\n",
    "\n",
    "    return mask_surprisals\n",
    "\n",
    "def compare_surprisals(row, return_region_surprisal):\n",
    "    \"\"\"\n",
    "    Helper for comparing surprisals of grammatical and ungrammatical sentences.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    row: pd.Series\n",
    "        Row of the dataframe containing the test case.\n",
    "    return_region_surprisal: bool\n",
    "        Whether to return surprisal of the critical region only or average for full sentence.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    is_grammatical: bool\n",
    "        Whether the grammatical sentence has lower surprisal than the ungrammatical one.\n",
    "    \"\"\"\n",
    "    # get surprisal of grammatical sentence\n",
    "    grammatical_surprisal = get_surprisal(\n",
    "        row[\"masked_sentence\"],\n",
    "        row[\"grammatical_sentence\"],\n",
    "        return_region_surprisal=return_region_surprisal,\n",
    "    )\n",
    "    print(f\"--- Surprisal of grammatical sentence {row['grammatical_sentence']}: {grammatical_surprisal} ---\\n\\n\")\n",
    "    # get surprisal of ungrammatical sentence\n",
    "    ungrammatical_surprisal = get_surprisal(\n",
    "        row[\"masked_sentence\"],\n",
    "        row[\"ungrammatical_sentence\"],\n",
    "        return_region_surprisal=return_region_surprisal,\n",
    "    )\n",
    "    print(f\"--- Surprisal of ungrammatical sentence {row['ungrammatical_sentence']}: {ungrammatical_surprisal} ---\\n\\n\")\n",
    "    \n",
    "    # check LM accuracy (in terms of surprisal)\n",
    "    is_grammatical = all([g < u for g, u in zip(grammatical_surprisal, ungrammatical_surprisal)])\n",
    "    return is_grammatical\n",
    "\n",
    "print(\"Gramamticality test cases: \\n\\n\", grammaticality_test_cases.head(10))\n",
    "# call surprisal computation for single test cases from the slides\n",
    "print(\"--- Agreement test case --- \\n Is grammatical sentence less surprising than ungrammatical one?\", \n",
    "      compare_surprisals(grammaticality_test_cases.iloc[0], return_region_surprisal=False), \"\\n\\n\")\n",
    "\n",
    "print(\"--- Reflexive test case --- \\n Is grammatical sentence less surprising than ungrammatical one?\", \n",
    "      compare_surprisals(grammaticality_test_cases.iloc[11], return_region_surprisal=False), \"\\n\\n\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Runs all test cases.\n",
    "    \"\"\"\n",
    "    for _, r in grammaticality_test_cases.iterrows():\n",
    "        print(\"--------------------\")\n",
    "        is_grammatical = compare_surprisals(r, return_region_surprisal=False)\n",
    "        print(f\"Grammatical sentence: {r['grammatical_sentence']} \\n\\n\")\n",
    "        print(f\"Ungrammatical sentence: {r['ungrammatical_sentence']} \\n\\n\")\n",
    "        # check LM accuracy (in terms of surprisal)\n",
    "        print(\"Is the grammatical sentence more likely than the ungrammatical one under LM?\", \n",
    "              is_grammatical)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SuperGLUE\n",
    "\n",
    "Finally, we will get our hands dirty with evaluating LLMs which already have been trained. In this task, we will use a few tasks from one of the most-used LM benchmarks, the SuperGLUE benchmark:\n",
    "\n",
    "    a natural language inference (NLI) task “rte”,\n",
    "\n",
    "        a task wherein the model has to predict whether a second sentence is entailed by the first one (i.e., predict the label ‘entailment’ or ‘no entailment’)\n",
    "\n",
    "    a question answering task “boolq”,\n",
    "\n",
    "        a task wherein the model has to predict an answer (yes/no) to a question, given context\n",
    "\n",
    "    and a sentence continuation task “copa”.\n",
    "\n",
    "        a task wherein the model has to select one of two sentences as the more plausible continuation given an input sentence.\n",
    "\n",
    "We will be using (subset of) the validation splits of the tasks for our evaluation.\n",
    "\n",
    "With the introduction of first language models like BERT, a common approach to using benchmarks like SuperGLUE was to fine-tune the pretrained model on the train split of the benchmark datasets, and then use the test splits for evaluation. With SOTA LLMs, it is more common to do zero- or few-shot evaluation where the model has to, e.g., predict labels or select answer options without special fine-tuning, just given instructions.\n",
    "\n",
    "We are also not going to fine-tune our model on these specific tasks. Instead, as introduced in class, we are going to compare the log probabilities of different answer options (e.g., log probabilities of “entailment” vs. “no entailment” following a pair of sentences from the RTE-task). With this method, the assumption is that a model’s output prediction for a particular trial is correct iff: \n",
    "\n",
    "$$\\log P_{LM}(\\text{correct label} \\mid \\text{context}) >  \\log P_{LM}(\\text{incorrect label} \\mid \\text{context})$$\n",
    "\n",
    "For tasks like “copa” where there is no single label but instead a sentence continuation, we are going to compute the average token log probability as a single-number representation of the continuation. Here, the model’s prediction will count as correct iff the average log probability of the correct continuation sentence will be higher, given the input, than for the incorrect continuation. We will not using task instructions in our evaluation since the model wasn’t fine-tuned on instruction-following.\n",
    "\n",
    "Your job is to complete the code below, evaluate the model which you have fine-tuned above and summarize the results you find in a few words (see below for more detailed step-by-step instructions). If you have issues with the previous task and cannot use your own fine-tuned model, please use the initial IMDB fine-tuned GPT-2 with which we initialized the policy in exercise 2. Please indicate which model you are testing on Moodle in the respective exercise responses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**\n",
    "* Calibration\n",
    "* implementing various bias corrections\n",
    "* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine psychology\n",
    "\n",
    "However, more recently, work more informed by human language use and processing has compared LLMs’ performance to aspects of human behavior. Here, the assessment of LLMs is guided more by the question of how human-like certain aspects of its performance are.\n",
    "\n",
    "**TODO**\n",
    "* compute SG scores with some example\n",
    "* coming up with testing some psychological aspect\n",
    "* hands-on methods of comparing quantitatively human and machine data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
