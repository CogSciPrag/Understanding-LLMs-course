# Prompting & Current LMs

Slides from the fourth lecture introducing prompting strategies in the context of base (or, foundational) language models, as well as some of the current large language models can be found [here](https://github.com/CogSciPrag/Understanding-LLMs-course/tree/main/understanding-llms/lectures/slides/04-LLMs-prompting.pdf).

## Additional materials

If you want to dig a bit deeper, here are (optional!) supplementary readings introducing the prompting strategies and the mentioned LMs that were covered in class:

* [Wei et al. (2023) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/pdf/2201.11903)
* [Kojima et al. (2023) Large Language Models are Zero-Shot Reasoners](https://arxiv.org/pdf/2205.11916)
* [Webson & Pavlick (2022) Do Prompt-Based Models Really Understand the Meaning of Their Prompts?](https://aclanthology.org/2022.naacl-main.167.pdf)
* [Nye et al. (2022) Show Your Work: Scratchpads for Intermediate Computation with Language Models](https://openreview.net/pdf?id=HBlx2idbkbq) 
* [Reynolds & McDonnell (2021) Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm](https://dl.acm.org/doi/10.1145/3411763.3451760)
* [Wang et al. (2023) Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/pdf/2203.11171)
* [Liu et al. (2022) Generated Knowledge Prompting for Commonsense Reasoning](https://aclanthology.org/2022.acl-long.225.pdf)
* [Yao et al. (2023) Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/pdf/2305.10601)
* [Xie et al. (2022) An Explanation of In-context Learning as Implicit Bayesian Inference](https://arxiv.org/pdf/2111.02080)
* [Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?](https://aclanthology.org/2022.emnlp-main.759.pdf)
* [Lampinen et al. (2022) Can language models learn from explanations in context?](https://aclanthology.org/2022.findings-emnlp.38/)
* [Touvron et al. (2023) LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/pdf/2302.13971)
* [Touvron et al. (2023) Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/pdf/2307.09288)
