# Fine-tuning and RLHF

Slides of the session diving deeper into fine-tuning of LMs and, in particular, reinforcement learning from human feedback (RLHF), can be found [here](https://github.com/CogSciPrag/Understanding-LLMs-course/tree/main/understanding-llms/lectures/slides/05-Finetuning-RLHF.pdf).

## Additional materials

If you want to dig a bit deeper, here are (optional!) supplementary readings on some of the topics covered in class:

Supervised fine-tuning:

* [Ding et al. (2023) Parameter-efficient fine-tuning of large-scale pre-trained language models](https://www.nature.com/articles/s42256-023-00626-4)
* [Howard et al. (2018) Universal Language Model Fine-tuning for Text Classification](https://arxiv.org/abs/1801.06146)
  * background paper rather than PEFT-related

RLHF:
* [Blogpost about learning from human preferences (in the context of robotics) from OpenAI](https://openai.com/index/learning-from-human-preferences/)
* [Ouyang et al. (2022) Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)
* [RLHF overview blogpost from HuggingFace](https://huggingface.co/blog/rlhf)
* [ChatGPT blogpost](https://openai.com/index/chatgpt/)
* [Bai et al. (2022) Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://arxiv.org/pdf/2204.05862)