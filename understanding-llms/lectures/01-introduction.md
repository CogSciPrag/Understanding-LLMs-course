# Background

Stay tuned for slides from the first lecture!

A cheat sheet on mathematical notation and a few basics for the class can be found [here](https://github.com/CogSciPrag/Understanding-LLMs-course/tree/main/understanding-llms/lectures/slides/cheat-sheet-notation-algebra.pdf).

## Additional materials

If you want to dig a bit deeper, here are a few (optional!) readings on the development trends of LLMs, a general textbook on NLP as well as a paper mentioned in the slides:

* [Jurafsky & Martin (2024) Speech and Language Processing (textbook)](https://web.stanford.edu/~jurafsky/slp3/)
* [Zhao et al. (2023) A survey of LLMs](https://arxiv.org/pdf/2303.18223.pdf)
* [Kaplan et al. (2020) Scaling Laws for Neural Language Models](https://arxiv.org/pdf/2001.08361.pdf)
* [Mikolov et al. (2013) Word2Vec paper](https://arxiv.org/pdf/1301.3781.pdf)