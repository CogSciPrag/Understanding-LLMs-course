# LSTMs & Transformers

Slides from the third lecture, introducing attention and transformers, can be found [here](https://github.com/CogSciPrag/Understanding-LLMs-course/tree/main/understanding-llms/lectures/slides/03-Transformers.pdf).
Furthermore, there are some excellent educational materials and talks out there -- references to some original papers and some suggested resources are below.

# Transformers: Part 2

Slides from the second lecture on transformers can be found [here](https://github.com/CogSciPrag/Understanding-LLMs-course/tree/main/understanding-llms/lectures/slides/04-Transformers-02.pdf). The slides on mathematical formalization of transformers are available [here](https://github.com/CogSciPrag/Understanding-LLMs-course/tree/main/understanding-llms/lectures/slides/TransforMechInterp-slides.pdf).


## Additional materials

If you want to dig a bit deeper, here are (optional!) supplementary readings, a general textbook on deep learning and the paper that first introduced backpropagation for training neural networks:

* [Vaswani et al. (2017) Attention is all you need](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
* [Devlin et al. (2019) BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)
* [Radford et al. (2019) Language Models are Unsupervised Multitask Learners (GPT-2)](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
* [Series of lectures and talks on Transformers from Stanford](https://www.youtube.com/playlist?list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM)
* [Video on building GPT from scratch by Andrej Karpathy](https://www.youtube.com/watch?v=kCc8FmEb1nY)

Supplementary materials for the second transformers session:
* [A blog / website on hardware in deep learning and LLM](https://semianalysis.com/)
* [Rahwan et al. (2019) Machine behvaior](https://www.nature.com/articles/s41586-019-1138-y)
* [Paper on positional information in transformers](https://direct.mit.edu/coli/article/48/3/733/111478/Position-Information-in-Transformers-An-Overview)