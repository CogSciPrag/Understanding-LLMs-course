# LSTMs & Transformers

Slides from the third lecture, introducing attention and transformers, can be found [here](https://github.com/CogSciPrag/Understanding-LLMs-course/tree/main/understanding-llms/lectures/slides/03-Transformers.pdf).
Furthermore, there are some excellent educational materials and talks out there -- references to some original papers and some suggested resources are below.

## Additional materials

If you want to dig a bit deeper, here are (optional!) supplementary readings, a general textbook on deep learning and the paper that first introduced backpropagation for training neural networks:

* [Vaswani et al. (2017) Attention is all you need](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
* [Devlin et al. (2019) BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)
* [Radford et al. (2019) Language Models are Unsupervised Multitask Learners (GPT-2)](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
* [Series of lectures and talks on Transformers from Stanford](https://www.youtube.com/playlist?list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM)
* [Video on building GPT from scratch by Andrej Karpathy](https://www.youtube.com/watch?v=kCc8FmEb1nY)