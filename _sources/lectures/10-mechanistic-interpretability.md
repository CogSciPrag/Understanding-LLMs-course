# Mechanistic Interpretability

This session covers a cutting-edge topic -- mechanistic interpretability -- which works towatds identifying computational mechanisms within the transformer architecture which support performance on various tasks. 
Slides of the session can be found [here](https://github.com/CogSciPrag/Understanding-LLMs-course/tree/main/understanding-llms/lectures/slides/10-Mechanistic-Interpretability.pdf).

## Additional materials

If you want to dig a bit deeper, here are (optional!) supplementary readings. More papers discussed in the lecture are provided in the slides.

* [The logit lens blogpost](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens)
* [Merullo et al. (2024) Language Models Implement Simple Word2Vec-style Vector Arithmetic](https://arxiv.org/pdf/2305.16130)
* [Elhage et al. (2021) A Mathematical Framework for Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html)
* [Meng et al. (2022) Locating and Editing Factual Associations in GPT, aka ROME paper mentioned in the lecture](https://arxiv.org/abs/2202.05262)
* [Vig et al. (2020) Causal Mediation Analysis for Interpreting Neural NLP: The Case of Gender Bias](https://arxiv.org/abs/2004.12265)
* [Heimersheim & Nanda (2024) How to use and interpret activation patching](https://arxiv.org/abs/2404.15255)
* [Wang et al. (2023) Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small](https://arxiv.org/abs/2211.00593)
* [Causal Scrubbing: a method for rigorously testing interpretability hypotheses](https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing)
* [Merullo et al. (2024) Circuit component reuse across tasks in transformer language models](https://health-nlp.com/files/pubs/iclr24a.pdf)
* [Yu et al. (2023) Characterizing Mechanisms for Factual Recall in Language Models](https://health-nlp.com/files/pubs/emnlp23d.pdf)