{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sheet 8.1: Mechanistic interpretability\n",
    "==========\n",
    "**Author**: Polina Tsvilodub\n",
    "\n",
    "One criticism often raised in context of LLMs is their blackbox nature, i.e., the inscrutability of the mechanics of the models and how or why they arrive at predictions, given the input.\n",
    "We have seen some methods that help to map out how models behave on certain tasks ([sheet 7.1](https://cogsciprag.github.io/Understanding-LLMs-course/tutorials/07a-behavioral-assessment.html)), what aspects of the input critically affect the output and which information the models process ([sheet 6.1](https://cogsciprag.github.io/Understanding-LLMs-course/tutorials/06a-attribution.html)).\n",
    "In this sheet, we will look at methods for identifying the *computational mechanisms* that lead to the outputs, i.e., at mechanistic interpretability. It can be seen as trying to reverse-engineer the computational algorithms the mdoel has learned during training and that are active during certain tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early decoding\n",
    "\n",
    "First, we will look at early decoding, i.e., at applying the \"unembedding\" layer (projecting hidden representations into vocabulary space by applying a linear and a softmax layer) to representations in layers throughout the model (not just the last layer). For this, we will need to output results of the calculations in the layers. There are various parameters that can be passed to \n",
    "Furthermore, it is helpful to be able to access different weights of the model, which we did in [sheet 3.1](https://cogsciprag.github.io/Understanding-LLMs-course/tutorials/03a-tokenization-transformers.html).\n",
    "\n",
    "The code is largely based on [this](https://github.com/jmerullo/lm_vector_arithmetic) repository which accompanies the paper by [Merullo et al. (2024)](https://arxiv.org/pdf/2305.16130)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import BloomTokenizerFast \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "\n",
    "def get_device():\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    return device\n",
    "\n",
    "def load_gpt2(version):\n",
    "    device= get_device()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(version)\n",
    "    model = AutoModelForCausalLM.from_pretrained(version, torch_dtype=torch.float16).to(device)\n",
    "    return model, tokenizer\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "class ModelWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, model, tokenizer):\n",
    "        super().__init__()\n",
    "        self.model = model.eval()\n",
    "        self.model.activations_ = {}\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = get_device()\n",
    "        self.num_layers = len(self.model.transformer.h)\n",
    "        self.hooks  = []\n",
    "        self.layer_pasts = {}\n",
    "\n",
    "    def tokenize(self, s):\n",
    "        return self.tokenizer.encode(s, return_tensors='pt').to(self.device)\n",
    "\n",
    "    def list_decode(self, inpids):\n",
    "        return [self.tokenizer.decode(s) for s in inpids]\n",
    "\n",
    "    def layer_decode(self, hidden_states):\n",
    "        raise Exception(\"Layer decode has to be implemented!\")\n",
    "\n",
    "    def get_layers(self, tokens, **kwargs):\n",
    "        outputs = self.model(input_ids=tokens, output_hidden_states=True, **kwargs)\n",
    "        hidden_states, true_logits = outputs.hidden_states, outputs.logits\n",
    "        logits = self.layer_decode(hidden_states)\n",
    "        #logits[-1] = true_logits.squeeze(0)[-1].unsqueeze(-1) #we used to just replace the last logits because we were applying ln_f twice\n",
    "        return torch.stack(logits).squeeze(-1)#, true_logits.squeeze(0)\n",
    "\n",
    "    def get_layers_w_attns(self, tokens, **kwargs):\n",
    "        outputs = self.model(input_ids=tokens, output_hidden_states=True, output_attentions=True, **kwargs)\n",
    "        hidden_states, true_logits = outputs.hidden_states, outputs.logits\n",
    "        logits = self.layer_decode(hidden_states)\n",
    "        #logits[-1] = true_logits.squeeze(0)[-1].unsqueeze(-1)\n",
    "        return torch.stack(logits).squeeze(-1), outputs.attentions#, true_logits.squeeze(0)\n",
    "\n",
    "    def rr_per_layer(self, logits, answer, debug=False):\n",
    "        #reciprocal rank of the answer at each layer\n",
    "        answer_id = self.tokenizer.encode(answer)[0]\n",
    "        if debug:\n",
    "            print(\"Answer id\", answer_id, answer)\n",
    "\n",
    "        rrs = []\n",
    "        for i,layer in enumerate(logits):\n",
    "            soft = F.softmax(layer,dim=-1)\n",
    "            sorted_probs = soft.argsort(descending=True)\n",
    "            rank = float(np.where(sorted_probs.cpu().numpy()==answer_id)[0][0])\n",
    "            rrs.append(1/(rank+1))\n",
    "\n",
    "        return np.array(rrs)\n",
    "\n",
    "    def prob_of_answer(self, logits, answer, debug=False):\n",
    "        answer_id = self.tokenizer.encode(answer)[0]\n",
    "        if debug:\n",
    "            print(\"Answer id\", answer_id, answer)\n",
    "        answer_probs = []\n",
    "        first_top = -1\n",
    "        mrrs = []\n",
    "        for i,layer in enumerate(logits):\n",
    "            soft = F.softmax(layer,dim=-1)\n",
    "            answer_prob = soft[answer_id].item()\n",
    "            sorted_probs = soft.argsort(descending=True)\n",
    "            if debug:\n",
    "                print(f\"{i}::\", answer_prob)\n",
    "            answer_probs.append(answer_prob)\n",
    "        #is_top_at_end = sorted_probs[0] == answer_id\n",
    "        return np.array(answer_probs)\n",
    "\n",
    "    def print_top(self, logits, k=10):\n",
    "        for i,layer in enumerate(logits):\n",
    "            print(f\"{i}\", self.tokenizer.decode(F.softmax(layer,dim=-1).argsort(descending=True)[:k]) )\n",
    "\n",
    "    def topk_per_layer(self, logits, k=10):\n",
    "        topk = []\n",
    "        for i,layer in enumerate(logits):\n",
    "            topk.append([self.tokenizer.decode(s) for s in F.softmax(layer,dim=-1).argsort(descending=True)[:k]])\n",
    "        return topk\n",
    "\n",
    "    def get_activation(self, name):\n",
    "        #https://github.com/mega002/lm-debugger/blob/01ba7413b3c671af08bc1c315e9cc64f9f4abee2/flask_server/req_res_oop.py#L57\n",
    "        def hook(module, input, output):\n",
    "            if \"in_sln\" in name:\n",
    "                num_tokens = list(input[0].size())[1]\n",
    "                self.model.activations_[name] = input[0][:, num_tokens - 1].detach()\n",
    "            elif \"mlp\" in name or \"attn\" in name or \"m_coef\" in name:\n",
    "                if \"attn\" in name:\n",
    "                    num_tokens = list(output[0].size())[1]\n",
    "                    self.model.activations_[name] = output[0][:, num_tokens - 1].detach()\n",
    "                    self.model.activations_['in_'+name] = input[0][:, num_tokens - 1].detach()\n",
    "                elif \"mlp\" in name:\n",
    "                    num_tokens = list(output[0].size())[0]  # [num_tokens, 3072] for values;\n",
    "                    self.model.activations_[name] = output[0][num_tokens - 1].detach()\n",
    "                elif \"m_coef\" in name:\n",
    "                    num_tokens = list(input[0].size())[1]  # (batch, sequence, hidden_state)\n",
    "                    self.model.activations_[name] = input[0][:, num_tokens - 1].detach()\n",
    "            elif \"residual\" in name or \"embedding\" in name:\n",
    "                num_tokens = list(input[0].size())[1]  # (batch, sequence, hidden_state)\n",
    "                if name == \"layer_residual_\" + str(self.num_layers-1):\n",
    "                    self.model.activations_[name] = self.model.activations_[\n",
    "                                                        \"intermediate_residual_\" + str(final_layer)] + \\\n",
    "                                                    self.model.activations_[\"mlp_\" + str(final_layer)]\n",
    "\n",
    "                else:\n",
    "                    if 'out' in name:\n",
    "                        self.model.activations_[name] = output[0][num_tokens-1].detach()\n",
    "                    else:\n",
    "                        self.model.activations_[name] = input[0][:,\n",
    "                                                            num_tokens - 1].detach()\n",
    "\n",
    "        return hook\n",
    "\n",
    "    def reset_activations(self):\n",
    "        self.model.activations_ = {}\n",
    "\n",
    "\n",
    "class GPT2Wrapper(ModelWrapper):\n",
    "\n",
    "    def layer_decode(self, hidden_states):\n",
    "        logits = []\n",
    "        for i,h in enumerate(hidden_states):\n",
    "            h=h[:, -1, :] #(batch, num tokens, embedding size) take the last token\n",
    "            if i == len(hidden_states)-1:\n",
    "                normed = h #ln_f would already have been applied\n",
    "            else:\n",
    "                normed = self.model.transformer.ln_f(h)\n",
    "            l = torch.matmul(self.model.lm_head.weight, normed.T)\n",
    "            logits.append(l)\n",
    "        return logits\n",
    "\n",
    "    def add_hooks(self):\n",
    "        for i in range(self.num_layers):\n",
    "            #intermediate residual between\n",
    "            #print('saving hook') \n",
    "            self.hooks.append(self.model.transformer.h[i].ln_1.register_forward_hook(self.get_activation(f'in_sln_{i}')))\n",
    "            self.hooks.append(self.model.transformer.h[i].attn.register_forward_hook(self.get_activation('attn_'+str(i))))\n",
    "            self.hooks.append(self.model.transformer.h[i].ln_2.register_forward_hook(self.get_activation(\"intermediate_residual_\" + str(i))))\n",
    "            self.hooks.append(self.model.transformer.h[i].ln_2.register_forward_hook(self.get_activation(\"out_intermediate_residual_\" + str(i))))\n",
    "            self.hooks.append(self.model.transformer.h[i].mlp.register_forward_hook(self.get_activation('mlp_'+str(i))))\n",
    "            #print(self.model.activations_)\n",
    "\n",
    "\n",
    "    def get_pre_wo_activation(self, name):\n",
    "        #wo refers to the output matrix in attention layers. The last linear layer in the attention calculation\n",
    "\n",
    "        def hook(module, input, output):\n",
    "            #use_cache=True (default) and output_attentions=True have to have been passed to the forward for this to work\n",
    "            _, past_key_value, attn_weights = output\n",
    "            value = past_key_value[1]\n",
    "            pre_wo_attn = torch.matmul(attn_weights, value)    \n",
    "            self.model.activations_[name]=pre_wo_attn\n",
    "\n",
    "        return hook\n",
    "\n",
    "    def get_past_layer(self, name):\n",
    "        #wo refers to the output matrix in attention layers. The last linear layer in the attention calculation\n",
    "\n",
    "        def hook(module, input, output):\n",
    "            #use_cache=True (default) and output_attentions=True have to have been passed to the forward for this to work\n",
    "            #print(len(output), output, name)\n",
    "            _, past_key_value, attn_weights = output  \n",
    "            self.layer_pasts[name]=past_key_value\n",
    "\n",
    "        return hook\n",
    "\n",
    "    def add_mid_attn_hooks(self):\n",
    "        for i in range(self.num_layers):\n",
    "            self.hooks.append(self.model.transformer.h[i].attn.register_forward_hook(self.get_pre_wo_activation('mid_attn_'+str(i))))\n",
    "\n",
    "            self.hooks.append(self.model.transformer.h[i].attn.register_forward_hook(self.get_past_layer('past_layer_'+str(i))))\n",
    "\n",
    "    def rm_hooks(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "\n",
    "    def reset_activations(self):\n",
    "        self.activations_ = {}\n",
    "        self.last_pasts = {}\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_gpt2('gpt2-medium')\n",
    "model = model.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper = GPT2Wrapper(model, tokenizer)\n",
    "def tokenize(text):\n",
    "    inp_ids = wrapper.tokenize(text)\n",
    "    str_toks = wrapper.list_decode(inp_ids[0])\n",
    "    return inp_ids, str_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poland_text=\"\"\"Q: What is the capital of France?\n",
    "A: Paris\n",
    "Q: What is the capital of Poland?\n",
    "A:\"\"\"\n",
    "\n",
    "poland_ids, pol_toks = tokenize(poland_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poland_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = wrapper.get_layers(poland_ids)\n",
    "wrapper.print_top(logits[1:]) #skip the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the shape of the logits and try to understand what they represent\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 8.1.1: Early decoding</span></strong>\n",
    ">\n",
    "> 1. (For yourself) Read through the code above and make sure that you understand what it does and why. Make sure to understand the concept of early decoding.\n",
    "> 2. Try a different example (maybe for a different task). Do you see a similar pattern (comparing results between the layers)?\n",
    "\n",
    "\n",
    "## Residual stream\n",
    "\n",
    "Next, the lecture discussed the role of the *residual stream*, i.e., the \"stream\" which passes information between the transformer blocks and, only undergoes linear transformations. Note that there is no block or layers called \"residual stream\" in the transformer architecture (i.e., if you were to print a pre-defined architecture); rather, it is a conceptual interpretation of the transformer architecture, realising that due to residual connections within the transformer blocks, one can look at the flow of information (i.e., the vector representations) as being *read from* by the transformer blocks (i.e., reading results of previous computations), and then *writing* the results of the attention calculations back to the representations (via linear operations applied to previous representations). \n",
    "\n",
    "Below is a small example of accessing the residual stream. The idea behind this analysis is understanding what exactly the mechanistic role of the single computations (i.e., applying attention vs. the FFNN layer).\n",
    "The code below works with so-called hooks, i.e., functions that are \"hooked onto\" the forward pass of through the model and are executed together with the normal computations, when the model is called. They are based on [this](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_forward_hook) native PyTorch functionality. Their implementation is in the code cells above.\n",
    "\n",
    "Specicifically, the question tested in the code below is which computational step promotes \"Warsaw\" over \"Poland\", specifically, whether it is the pass through the FFNN. This is done by using the same early decoding technique and looking at the model's prediction before and after applying the FFNN.\n",
    "\n",
    "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 8.1.2: Residual stream decoding</span></strong>\n",
    ">\n",
    "> 1. (For yourself) Read through the code above and make sure that you understand what it does and why. How is the question above operationalized?\n",
    "> 2. Inspecting the results, would you say that the FFNN is responsinble for prompting \"Warsaw\"?\n",
    "> 3. In case you saw a similar pattern for your other example above, try to adapt the code below to your example.\n",
    "> 4. [Optional] If you are curious to see more, read the excellent paper above and look at the full [demo notebook](https://github.com/jmerullo/lm_vector_arithmetic/blob/main/demo.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We can decode at the residual stream between the attention and FFN to show that \n",
    "it is the FFN update that updates from Poland to Warsaw, but a slightly easier way to do this is to\n",
    "subtract the FFN update that was applied at layer 19 to show the intermediate residual stream state\n",
    "(i.e., between the attention and FFN)\n",
    "\"\"\"\n",
    "\n",
    "wrapper.add_hooks()\n",
    "out = wrapper.model(input_ids = poland_ids, output_hidden_states=True) #run it again to activate hooks\n",
    "logits = out.logits\n",
    "hidden_states = out.hidden_states\n",
    "hidden_states = list(hidden_states)[1:] #skip the embedding layer to stay consistent with our indexing\n",
    "\n",
    "#get the FFN output update at layer 19\n",
    "o_city = wrapper.model.activations_['mlp_19']\n",
    "\n",
    "\n",
    "print(len(hidden_states)) #24\n",
    "\n",
    "layer_logits = wrapper.layer_decode(hidden_states)\n",
    "layer_logits = torch.stack(layer_logits).squeeze(-1)\n",
    "print(\"Original top tokens at layer 19\")\n",
    "wrapper.print_top(layer_logits[19].unsqueeze(0))\n",
    "\n",
    "hidden_states[19]-=o_city \n",
    "\n",
    "\n",
    "layer_logits = wrapper.layer_decode(hidden_states)\n",
    "layer_logits = torch.stack(layer_logits).squeeze(-1)\n",
    "print(\"After subtracting mlp_19 (o_city)\")\n",
    "wrapper.print_top(layer_logits[19].unsqueeze(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation patching\n",
    "\n",
    "Another technique discussed in the lecture is *activation patching*. This can be seen as a method for *causal intervention* on the computational mechanisms of the model. For instance, certain results of the computations are injected, and the effect these results have on the outcome are observed. We could, of course, inject random values, but that might require trying quite many values before being able to observe meaningful differences in the output. Therefore, instead, certain representations computed in one pass through the model are taken (i.e., representations from the *clean run*) and injected into another (the *corrupted run*). These injected representations are called the patch.\n",
    "\n",
    "Patching techniques are quite tricky to work with since patching something in an intermediate layer affects not only the local layer, but also all the downstream computations. To be able to retrieve meaningful results, careful comparison and, e.g., freezing of subsequent representations is required.\n",
    "\n",
    "Below is an example of using activation patching on the Indirect Object Identification task, with the help of the library [`transformer_lens`](https://github.com/TransformerLensOrg/TransformerLens). \n",
    "The idea of the task if simple: given a sentence like \"After John and Mary went to the store, Mary gave a bottle of milk to\", identify the indirect object (i.e., \"John\", rather than \"Mary\"). The goal is to identify which model activations (i.e., representation computed within the transformer ) are important for completing a task. We do this by setting up a clean prompt and a corrupted prompt, for instance:\n",
    "\n",
    "Clean prompt: After John and Mary went to the store, Mary gave a bottle of milk to\n",
    "\n",
    "Corrupted prompt: After John and Mary went to the store, John gave a bottle of milk to\n",
    "\n",
    "Further, we define a metric for performance on the task, e.g., the difference in logits of the correct token, given different inputs (recall our methods for assessing model performance, where one core assumption is that a model can perform a task well if it assigns higher log probabilities (i.e., the logits are higher) for correct predictions that for incorrect ones). \n",
    "\n",
    "We then pick a specific model activation, run the model on the corrupted prompt, but then intervene on that activation and patch in its value when run on the clean prompt. We then apply the metric, and see how much this patch has recovered the clean performance. \n",
    "Essentially, we ask: given a corrupted input, if we inject certain representations from the \"correct\" run, can we \"fix\" the performance? If we can, we know that the injected representations (causally!) contribute to producing the correct output.\n",
    "\n",
    "The code below is taken from [this](https://github.com/TransformerLensOrg/TransformerLens/blob/main/demos/Main_Demo.ipynb) demo.\n",
    "\n",
    "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 8.1.3: Activation patching</span></strong>\n",
    ">\n",
    "> 1. (For yourself) Read through the code above and make sure that you understand what it does and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformer_lens plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "import plotly.express as px\n",
    "import transformer_lens.utils as utils\n",
    "import tqdm \n",
    "from functools import partial\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/understanding_llms/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# load the model within the wrapper of the library which allows to easily access and patch activations\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean logit difference: 4.276\n",
      "Corrupted logit difference: -2.738\n"
     ]
    }
   ],
   "source": [
    "# first, we check if the model can do the task at all\n",
    "# i.e., we compare the difference in logits for the correct and incorrect answer\n",
    "# given different inputs without any interventions\n",
    "\n",
    "clean_prompt = \"After John and Mary went to the store, Mary gave a bottle of milk to\"\n",
    "corrupted_prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "\n",
    "clean_tokens = model.to_tokens(clean_prompt)\n",
    "corrupted_tokens = model.to_tokens(corrupted_prompt)\n",
    "\n",
    "def logits_to_logit_diff(logits, correct_answer=\" John\", incorrect_answer=\" Mary\"):\n",
    "    # model.to_single_token maps a string value of a single token to the token index for that token\n",
    "    # If the string is not a single token, it raises an error.\n",
    "    correct_index = model.to_single_token(correct_answer)\n",
    "    incorrect_index = model.to_single_token(incorrect_answer)\n",
    "    return logits[0, -1, correct_index] - logits[0, -1, incorrect_index]\n",
    "\n",
    "# We run on the clean prompt with the cache so we store activations to patch in later.\n",
    "clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
    "clean_logit_diff = logits_to_logit_diff(clean_logits)\n",
    "print(f\"Clean logit difference: {clean_logit_diff.item():.3f}\")\n",
    "\n",
    "# We don't need to cache on the corrupted prompt.\n",
    "corrupted_logits = model(corrupted_tokens)\n",
    "corrupted_logit_diff = logits_to_logit_diff(corrupted_logits)\n",
    "print(f\"Corrupted logit difference: {corrupted_logit_diff.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a helper\n",
    "\n",
    "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 12/12 [00:23<00:00,  1.97s/it]\n"
     ]
    }
   ],
   "source": [
    "# We define a residual stream patching hook\n",
    "# We choose to act on the residual stream at the start of the layer, so we call it resid_pre\n",
    "# The type annotations are a guide to the reader and are not necessary\n",
    "def residual_stream_patching_hook(\n",
    "    resid_pre,\n",
    "    hook,\n",
    "    position\n",
    "):\n",
    "    # Each HookPoint has a name attribute giving the name of the hook.\n",
    "    clean_resid_pre = clean_cache[hook.name]\n",
    "    # NOTE: this is the key step in the patching process\n",
    "    # where we replace the activations in the residual stream with the same activations from the clean run\n",
    "    resid_pre[:, position, :] = clean_resid_pre[:, position, :]\n",
    "    return resid_pre\n",
    "\n",
    "# We make a tensor to store the results for each patching run. \n",
    "# We put it on the model's device to avoid needing to move things between the GPU and CPU, which can be slow.\n",
    "num_positions = len(clean_tokens[0])\n",
    "ioi_patching_result = torch.zeros((model.cfg.n_layers, num_positions), device=model.cfg.device)\n",
    "\n",
    "for layer in tqdm.tqdm(range(model.cfg.n_layers)):\n",
    "    for position in range(num_positions):\n",
    "        # Use functools.partial to create a temporary hook function with the position fixed\n",
    "        temp_hook_fn = partial(residual_stream_patching_hook, position=position)\n",
    "        # Run the model with the patching hook\n",
    "        patched_logits = model.run_with_hooks(corrupted_tokens, fwd_hooks=[\n",
    "            (utils.get_act_name(\"resid_pre\", layer), temp_hook_fn)\n",
    "        ])\n",
    "        # Calculate the logit difference\n",
    "        patched_logit_diff = logits_to_logit_diff(patched_logits).detach()\n",
    "        # Store the result, normalizing by the clean and corrupted logit difference so it's between 0 and 1 (ish)\n",
    "        ioi_patching_result[layer, position] = (patched_logit_diff - corrupted_logit_diff)/(clean_logit_diff - corrupted_logit_diff)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "coloraxis": "coloraxis",
         "hovertemplate": "Position: %{x}<br>Layer: %{y}<br>color: %{z}<extra></extra>",
         "name": "0",
         "type": "heatmap",
         "x": [
          "<|endoftext|>_0",
          "After_1",
          " John_2",
          " and_3",
          " Mary_4",
          " went_5",
          " to_6",
          " the_7",
          " store_8",
          ",_9",
          " Mary_10",
          " gave_11",
          " a_12",
          " bottle_13",
          " of_14",
          " milk_15",
          " to_16"
         ],
         "xaxis": "x",
         "yaxis": "y",
         "z": [
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.99814785,
           0.0016011463,
           0.00014901973,
           -0.00037118964,
           -0.000021210837,
           -0.00062871096,
           -0.0005155865
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.99805593,
           0.0022837,
           0.00018246759,
           -0.0005049811,
           -0.0002678548,
           -0.00005221129,
           -0.001281896
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.9967372,
           0.0040811826,
           0.000973523,
           0.00004405328,
           -0.00015962514,
           -0.00033583824,
           -0.0019445986
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.99058974,
           0.019986862,
           0.0018951067,
           0.0010134974,
           -0.00006852732,
           0.0009112502,
           -0.0019005453
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.9616513,
           0.0853467,
           0.005204269,
           0.003052185,
           0.00019715201,
           0.0011059548,
           -0.0022845159
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.96309996,
           0.084370725,
           0.0041219727,
           0.00071790523,
           0.000103062914,
           0.0010009883,
           -0.004214974
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.9359183,
           0.11111814,
           0.0077044284,
           0.00037445285,
           0.00036493517,
           0.0013256773,
           0.01874467
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.77015555,
           0.037419453,
           0.0020683284,
           -0.000083211744,
           0.00013460724,
           0.0017248761,
           0.44990605
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.096506044,
           0.025926441,
           0.001971248,
           0.00032931185,
           0.00042394482,
           0.001885589,
           0.8994711
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           -0.023322675,
           0.018537456,
           0.0015872776,
           0.00052700774,
           0.00025371424,
           0.0008737233,
           0.96127474
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           -0.0085583,
           0.006339321,
           0.00058221025,
           -0.00034100498,
           0.00011067706,
           0.0006482902,
           0.9495821
          ]
         ]
        }
       ],
       "layout": {
        "coloraxis": {
         "cmid": 0,
         "colorscale": [
          [
           0,
           "rgb(103,0,31)"
          ],
          [
           0.1,
           "rgb(178,24,43)"
          ],
          [
           0.2,
           "rgb(214,96,77)"
          ],
          [
           0.3,
           "rgb(244,165,130)"
          ],
          [
           0.4,
           "rgb(253,219,199)"
          ],
          [
           0.5,
           "rgb(247,247,247)"
          ],
          [
           0.6,
           "rgb(209,229,240)"
          ],
          [
           0.7,
           "rgb(146,197,222)"
          ],
          [
           0.8,
           "rgb(67,147,195)"
          ],
          [
           0.9,
           "rgb(33,102,172)"
          ],
          [
           1,
           "rgb(5,48,97)"
          ]
         ]
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Normalized Logit Difference After Patching Residual Stream on the IOI Task"
        },
        "xaxis": {
         "anchor": "y",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "scaleanchor": "y",
         "title": {
          "text": "Position"
         }
        },
        "yaxis": {
         "anchor": "x",
         "autorange": "reversed",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Layer"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add the index to the end of the label, because plotly doesn't like duplicate labels\n",
    "token_labels = [f\"{token}_{index}\" for index, token in enumerate(model.to_str_tokens(clean_tokens))]\n",
    "imshow(ioi_patching_result, x=token_labels, xaxis=\"Position\", yaxis=\"Layer\", title=\"Normalized Logit Difference After Patching Residual Stream on the IOI Task\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "understanding_llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
