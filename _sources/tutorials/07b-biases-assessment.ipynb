{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sheet 7.2: Advanced evaluation\n",
    "=======\n",
    "**Author:** Polina Tsvilodub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main learning goal of this sheet is diving deeper into SOTA evaluations of LLMs; specifically, it is the familiarization with benchmarks which evaluate more intricate aspects of LLM I/O behavior. \n",
    "Since recent LLMs exhibit in-context learning behavior and have shown good performance on various tasks going beyond simple text completion, this sheet will zoom in on the more recent benchmarks like [MMLU](https://arxiv.org/abs/2009.03300), and evaluations of aspects connected to safety and fairness of LLMs. As mentioned in the previous sheet, the latter aspect became especially relevant as LLMs are embedded in user facing applications. Furthermore, awareness of questions of algorithmic fairness and biases in ML is increasingly rising in the community.\n",
    "\n",
    "The methods used for evaluating such aspects are already familiar to you from the previous sheet. Therefore, this sheet is more conceptual and aims rather at sharpening your critical thinking and result interpretation skills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge & Problem solving benchmarks\n",
    "\n",
    "Two of the most popular recent benchmarks for LLMs are the [MMLU](https://arxiv.org/abs/2009.03300) and the [BIG-Bench](https://arxiv.org/abs/2206.04615) datasets. \n",
    "The authors of MMLU describe it as a multi-task dataset for text models; according to the paper, in order to obtain high accuracy on it, models must possess world knowledge and problem solving capabilities. It contains tasks like elementary mathematics, US history, computer science, law etc.\n",
    "The BIG Bench also contains various tasks drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development etc. Interestingly, its composition is driven by what is / was believed to be beyond the capabilities of SOTA LMs of 2022. \n",
    "\n",
    "Both of these benchmarks strongly focus on what can be considered rather *factual knowledge* and *formal* problem solving (e.g., puzzle solving, logical reasoning, math tasks) etc. \n",
    "Most recent evaluations, e.g., of [GPT-4](https://arxiv.org/abs/2303.08774), were extended to domain knowledge-intensive tasks like the the US bar exam or coding (ironically, according to the name of one of the recent benchmarks, [HumanEval](https://arxiv.org/abs/2107.03374), coding is apparently an indicator of human-level abilities of LLMs...) \n",
    "\n",
    "These tasks can be seen in contrast to tasks that require more intuitive *common-sense*. There are other datasets which focus more on common-sense, for instance, [CommonsenseQA](https://arxiv.org/abs/1811.00937) or [HellaSwag](https://arxiv.org/abs/1905.07830). All of these are multiple-choice benchmarks. \n",
    "\n",
    "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 7.2.1: Understanding knowledge benchmarks</span></strong>\n",
    ">\n",
    "> 1. Consider the following datasets:\n",
    "> * [LAMBADA](https://arxiv.org/abs/1606.06031)\n",
    "> * [TriviaQA](https://arxiv.org/abs/1705.03551)\n",
    "> * [StoryCloze](https://arxiv.org/pdf/1604.01696v1)\n",
    "> * [NaturalQA](https://research.google/pubs/natural-questions-a-benchmark-for-question-answering-research/)\n",
    "> * [GSM8K](https://arxiv.org/abs/2110.14168)\n",
    "> Which of these would you say test: general world knowledge, common sense, factual knowledge, reasoning (or something else)?\n",
    "> 2. Performance of LLMs on these benchmarks are sometimes taken to support arguments about LMs being models of human (general) [intelligence](https://arxiv.org/pdf/2303.12712). Do you think performance on these benchmarks tests intelligence? Is yes, why? If no, what is (intuitively) missing? (Hint: you might want to think about children and what they would know / how we think about their capabilities)\n",
    "> 3. Consider the following plot (source [here](https://arxiv.org/abs/2206.04615)). What can you conclude about LMs in general from this plot? Does your conclusion play into your answer to the question above?\n",
    "> ![img](./pics/big-scaling.png)\n",
    "> \n",
    "> 3. Now consider this plot (source [here](https://scale-llm-24.pages.dev/pdf/inverse_scaling_prize_paper.pdf)). This plot presents the performance of different models on tasks like the following:\n",
    "> Context: Write a quote that ends in the word “heavy”: Absence makes the heart grow ...\n",
    "> \n",
    "> Classes [“ heavy.”, “ fonder.”]\n",
    "\n",
    "> Answer “heavy.\"\n",
    ">\n",
    "> What can you conclude about LMs in general from this plot? Does your conclusion play into your answer to the question above?\n",
    "> \n",
    "> ![img](./pics/inverse-scaling.png)\n",
    ">\n",
    "\n",
    "\n",
    "### Hallucinations\n",
    "\n",
    "One core problem of LLMs are so-called *hallucinations* -- outputs generated by LLMs which sound *plausible* but are actually *wrong* with respect to real-world facts. This is a problematic phenomenon when we think about LLMs as part of user-facing applications where the developers would want the users to be able to rely on the system's outputs. Additionally, this issue might provide an interesting perspective on the questions regarding human-likeness of LLM performance. \n",
    "\n",
    "Hallucinations have been distinguished into factual hallucinations, i.e., those where LLMs output factually wrong information, and faithfulness hallucinations, where the LLM is inconsistent with user inputs or prior generations. Identifying, minimizing hallucinations and scalably evaluating LLMs' hallucination propensity remains a core challenge for the field. One commonly used benchmark for detecting hallucinations is the [TruthfulQA dataset](https://arxiv.org/abs/2109.07958).\n",
    "[This](https://arxiv.org/pdf/2311.05232) paper provides more details and an overview of recent advances in the field.\n",
    "  * \n",
    "\n",
    "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 7.2.2: Hallucinations</span></strong>\n",
    ">\n",
    "> 1. Skim section 3 of the hallucinations overview paper. What are possible reasons for hallucinations in LLMs? \n",
    "> 2. Look at the TruthfulQA dataset. Which type of hallucination (faithfulness or factuality) would be more likely to evaluate? Do you think this is a robust strategy to evaluate hallucinations? Which metric could be used to evaluate a model on this dataset (see last sheet)?\n",
    "> 3. [Optional] One metric introduced specifically in the context of factuality evaluations is FACTSCORE. It is a metric specifically for long-form text generation, which decomposes the generation content into atomic facts and subsequently computes the percentage of the facts supported by reliable knowledge sources. The knowledge source are provded, e.g., by corpora. If you are interested, you can check the package implementing FACTSCORE [here](https://github.com/shmsw25/FActScore).\n",
    "\n",
    "### Process consistency\n",
    "\n",
    "One aspect of LLM evaluation that is missing for the approaches discussed so far is closer (behvaioral) inspection of *how a model got to its answer*. That is, although it has been shown that explicit solution steps elicited via chain-of-thought prompting and similar techniques are beneficial for model performance, the actual consistency and correctness of these steps is rarely evaluated! In fact, it has been shown that models can be \"right for the wrong reasons\", i.e., their performance might not be sensitive to wrong intermediate steps [(Webson & Pavlick, 2022)](https://aclanthology.org/2022.naacl-main.167/).\n",
    "\n",
    "One interesting approach that attempts to target this problem is [*process supervision*](https://openai.com/index/improving-mathematical-reasoning-with-process-supervision/). This approach is related to RLHF, where a reward model is trained to assign higher rewards to more desirable outcomes. Under process supervision, however, the reward model is trained to also check intermediate solution steps. This is tested on math tasks.\n",
    "\n",
    "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 7.2.3: Process supervision</span></strong>\n",
    ">\n",
    "> 1. Do you think process supervision is transferable to other tasks, e.g., question answering in dialogues?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reasoning benchmarks\n",
    "\n",
    "Another capability that has been recently tested distinctly from knowledge tasks and respective problems is *abstract reasoning*. Specifically, the Abstraction and Reasoning Corpus (ARC) ([Chollet, 2019](https://arxiv.org/pdf/1911.01547)) tests different models (primarily, vision models, but more recently also LLMs) on reasoning about abstract visual shapes and their relations. This remains one of the most challenging tests for machines. [This](https://lab42.global/arc/) blogpost provides a short overview of the benchmark. \n",
    "\n",
    "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 7.2.3: ARC</span></strong>\n",
    ">\n",
    "> 1. Does information about ARC change your perspective on the benchmarks described above, and their bearing on discussions about intelligence of LLMs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Social aspects\n",
    "\n",
    "* gender biases: WInoGrande (bigger version of WinoGender, BBQ)\n",
    "* language biases?\n",
    "* Whose opinion do LMs reflect?\n",
    "* RealToxicityPrompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assistant evals\n",
    "\n",
    "* HHH\n",
    "* red teaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlook\n",
    "\n",
    "* Perez et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
