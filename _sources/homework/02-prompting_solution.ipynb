{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: Prompting & Generation with LMs (50 points)\n",
    "\n",
    "The second homework zooms in on the following skills: on gaining a deeper understanding of different state-of-the-art prompting techniques and training your critical conceptual thinking regarding research on LMs. \n",
    "\n",
    "### Logistics\n",
    "\n",
    "* submission deadline: June 2nd th 23:59 German time via Moodle\n",
    "  * please upload a **SINGLE .IPYNB FILE named Surname_FirstName_HW2.ipynb** containing your solutions of the homework.\n",
    "* please solve and submit the homework **individually**! \n",
    "* if you use Colab, to speed up the execution of the code on Colab, you can use the available GPU (if Colab resources allow). For that, before executing your code, navigate to Runtime > Change runtime type > GPU > Save.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Advanced prompting strategies (16 points)\n",
    "\n",
    "The lecture discussed various sophisticated ways of prompting language models for generating texts. Please answer the following questions about prompting techniques in context of different models, and write down your answers, briefly explaining them (max. 3 sentences). Feel free to actually implement some of the prompting strategies to play around with them and build your intuitions.\n",
    "\n",
    "> Consider the following language models: \n",
    "> * GPT-2, GPT-4, Vicuna (an instruction-tuned version of Llama) and Llama-2-7b-base.\n",
    ">  \n",
    "> Consider the following prompting / generation strategies: \n",
    "> * beam search, tree-of-thought reasoning, zero-shot CoT prompting, few-shot CoT prompting, few-shot prompting.\n",
    "> \n",
    "> For each model, which strategies do you think work well, and why? Do you think there are particular tasks or contexts, in which they work better, than in others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "4p per model. Aspects that can be mentioned include: \n",
    "* GPT-2: \n",
    "  * beam search: it has been shown that it improves results for \"standard\" LLMs\n",
    "  * few-shot prompting: GPT-2 might be able to do in-context learning if the examples are more liek text-completion.\n",
    "  * other strategies are too fancy\n",
    "* GPT-4:\n",
    "  * anything except beam search should work (it is probably too costly). depending on the task, few-shot CoT or tree of thought could be best for reasoning tasks\n",
    "* Vicuna:\n",
    "  * few-shot prompting or zero-shot CoT could work because it was instruction-tuned\n",
    "* Llama-base: \n",
    "  * few-shot prompting  or few-shot CoT could work, ToT or zero-shot might be too advanced because it wasn't instruction- / RL-tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Prompting for NLI & Multiple-choice QA (14 points)\n",
    "\n",
    "In this exercise, you can let your creativity flow -- your task is to come up with prompts for language models such that they achieve maximal accuracy on the following example tasks. Feel free to take inspiration from the in-class examples of the sentiment classification task. Also feel free to play around with the decoding scheme and see how it interacts with the different prompts.\n",
    "\n",
    "**TASK:**\n",
    "> Use the code that was introduced in the Intro to HF sheet to load the model and generate predictions from it with your sample prompts.\n",
    "> \n",
    "> * Please provide your code.\n",
    "> * Please report the best prompt that you found for each model and task (i.e., NLI and multiple choice QA), and the decoding scheme parameters that you used. \n",
    "> * Please write a brief summary of your explorations, stating what you tried, what worked (better), why you think that is.\n",
    "\n",
    "* Models: Pythia-410m, Pythia-1.4b\n",
    "* Tasks: please **test** the model on the following sentences and report the accuracy of the model with your best prompt and decoding configurations.\n",
    "  * Natural language inference: the task is to classify whether two sentences form a \"contradiction\" or an \"entailment\", or the relation is \"neutral\". The gold labels are provided for reference here, but obviously shouldn't be given to the model at test time.\n",
    "    * A person on a horse jumps over a broken down airplane. A person is training his horse for a competition. neutral\n",
    "    * A person on a horse jumps over a broken down airplane. A person is outdoors, on a horse. entailment\n",
    "    * Children smiling and waving at camera. There are children present. entailment\n",
    "    * A boy is jumping on skateboard in the middle of a red bridge. The boy skates down the sidewalk. contradiction\n",
    "    * An older man sits with his orange juice at a small table in a coffee shop while employees in bright colored shirts smile in the background. An older man drinks his juice as he waits for his daughter to get off work. neutral\n",
    "    * High fashion ladies wait outside a tram beside a crowd of people in the city. The women do not care what clothes they wear. contradiction\n",
    "  * Multiple choice QA: the task is to predict the correct answer option for the question, given the question and the options (like in the task of Ex. 3 of homework 1). The gold labels are provided for reference here, but obviously shouldn't be given to the model at test time.\n",
    "    * The only baggage the woman checked was a drawstring bag, where was she heading with it? [\"garbage can\", \"military\", \"jewelry store\", \"safe\", \"airport\"] -- airport\n",
    "    * To prevent any glare during the big football game he made sure to clean the dust of his what? [\"television\", \"attic\", \"corner\", \"they cannot clean corner and library during football match they cannot need that\", \"ground\"] -- television\n",
    "    * The president is the leader of what institution? [\"walmart\", \"white house\", \"country\", \"corporation\", \"government\"] -- country\n",
    "    * What kind of driving leads to accidents? [\"stressful\", \"dangerous\", \"fun\", \"illegal\", \"deadly\"] -- dangerous\n",
    "    * Can you name a good reason for attending school? [\"get smart\", \"boredom\", \"colds and flu\", \"taking tests\", \"spend time\"] -- \"get smart\"\n",
    "    * Stanley had a dream that was very vivid and scary. He had trouble telling it from what? [\"imagination\", \"reality\", \"dreamworker\", \"nightmare\", \"awake\"] -- reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Partial solution suggestion**\n",
    "\n",
    "* 6 pts / model, 2 pts for code\n",
    "  * for each model, there should be: a prompt, decoding parameters, accuracy for NLI, accuracy for QA, conclusion / summary\n",
    "  * the actual accuracies don't matter that much as long as the response sensibly reflects upon what's going on\n",
    "* any kind of code that does what is asked for in this task is of course acceptable, but below is one possibility (for one model). If people manually evaluated the accuracy, it's also fine (code is not required here).\n",
    "* intuition suggests that some kind of few shot prompting should work, especially if the prompt is formatted as text continuation rather than some structured format for the smaller model; for the larger model, even more advanced things might work, e.g., formatting the QA as multiple choice could work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following solution was created by Karahan Sarƒ±ta≈ü. It is much more that what was expected, but shows a very good and systematic approach to ivestigate different combinations of prompting/decoding strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karab\\Desktop\\Intro-LLMs\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# define computational device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Device: {device}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(f\"Device: {device}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-410m\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-410m\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed  # reproducibility\n",
    "torch.manual_seed(42)\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = [\n",
    "    \"Input: A person on a horse jumps over a broken down airplane. A person is training his horse for a competition. Relation:\", # neutral\n",
    "    \"Input: A person on a horse jumps over a broken down airplane. A person is outdoors, on a horse. Relation:\",  # entailment\n",
    "    \"Input: Children smiling and waving at camera. There are children present. Relation:\",  # entailment\n",
    "    \"Input: A boy is jumping on skateboard in the middle of a red bridge. The boy skates down the sidewalk. Relation:\", # contradiction\n",
    "    \"Input: An older man sits with his orange juice at a small table in a coffee shop while employees in bright colored shirts smile in the background. An older man drinks his juice as he waits for his daughter to get off work. Relation:\", # neutral\n",
    "    \"Input: High fashion ladies wait outside a tram beside a crowd of people in the city. The women do not care what clothes they wear. Relation:\" # contradiction\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(s, text = None):\n",
    "    decoded = tokenizer.decode(s, skip_special_tokens=True)\n",
    "    if text is not None:\n",
    "        decoded = decoded.replace(text, \"\")\n",
    "    print(decoded)\n",
    "    print(100 * '-' + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# greedy decoding\n",
    "def greedy_decoding(input_ids, max_new_tokens=2):\n",
    "    output = model.generate(input_ids, max_new_tokens=max_new_tokens, pad_token_id=tokenizer.eos_token_id)\n",
    "    return output\n",
    "\n",
    "def beam_search_decoding(input_ids, max_new_tokens=2, num_beams=5):\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        num_beams=num_beams,\n",
    "        early_stopping=True,   # option `early_stopping` implies stopping when all beams reach the end-of-sentence token\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return output\n",
    "\n",
    "def pure_sampling_decoding(input_ids, max_new_tokens=2):\n",
    "    # # activate sampling and deactivate top_k by setting top_k sampling to 0\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        top_k=0,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return output\n",
    "\n",
    "def softmax_sampling_decoding(input_ids, max_new_tokens=2, temperature=0.7):\n",
    "    # # activate sampling and deactivate top_k by setting top_k sampling to 0\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        top_k=0,\n",
    "        temperature=temperature,  # higher temperature means more randomness\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return output\n",
    "\n",
    "def top_k_sampling_decoding(input_ids, max_new_tokens=2, k=50):\n",
    "    # # activate sampling and deactivate top_k by setting top_k sampling to 0\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        top_k=k,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return output\n",
    "\n",
    "# the set of most likely words the summed probability of which exceeds threshold p  (also called nucleus sampling)\n",
    "def top_p_sampling_decoding(input_ids, max_new_tokens=2, p=0.9):\n",
    "    # # activate sampling and deactivate top_k by setting top_k sampling to 0\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        top_p=p,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return output\n",
    "\n",
    "def contrastive_decoding(input_ids, max_new_tokens=2, penalty_alpha=0.6, top_k=4):\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        penalty_alpha=penalty_alpha,\n",
    "        top_k=top_k,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Scheme\n",
    "In our experiments we worked on the following configurations:\n",
    "* Two models, `pythia-410m` and `pythia-1.4b` are used to generate the output.\n",
    "\n",
    "\n",
    "### Natural Language Inference\n",
    "\n",
    "* We use \"Instruction Prompting\" to make LM better understand user intention and follow the instruction.\n",
    "* Prefix of the input is given as follows:\\\n",
    " \"Please classify whether two sentences form a ‚Äúcontradiction‚Äù or an ‚Äúentailment‚Äù, or the relation is ‚Äúneutral‚Äù.\"\n",
    "* For each model, we test the following prompt engineering techniques:\n",
    "    * $k$-shot learning\n",
    "        * Zero-shot learning (no example is provided)\n",
    "        * One-shot learning (only one example is provided)\n",
    "        * Few-shot learning ($k$ = 3) (1 example for each relation)\n",
    "        * Few-shot learning ($k$ = 9) (3 examples for each relation)\n",
    "    * Self-consistency prompting (softmax sampling) with majority vote\n",
    "* For each $k$-shot learning scenario, we test the following decoding strategies:\n",
    "    * Greedy decoding\n",
    "    * Pure sampling\n",
    "    * Softmax sampling (temperature = 0.7)\n",
    "    * Top-$k$ sampling ($k$ = 50)\n",
    "    * Top-$p$ sampling ($p$ = 0.9)\n",
    "    * Beam search (beam size = 5)\n",
    "    * Contrastive decoding (penalty=0.6, $k$=4)\n",
    "\n",
    "In self-consistency, we apply majority vote on the outputs generate using softmax sampling.\n",
    "\n",
    "Disclaimer: As all these methods have different hyperparameters to tune, therefore the comparison is not completely fair. However, we believe that this comparison can give us a general idea of the performance of these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Inference\n",
    "Natural language inference: the task is to classify whether two sentences form a ‚Äúcontradiction‚Äù or an ‚Äúentailment‚Äù, or the relation is ‚Äúneutral‚Äù.\n",
    "* A person on a horse jumps over a broken down airplane. A person is training his horse for a competition. **neutral**\n",
    "* A person on a horse jumps over a broken down airplane. A person is outdoors, on a horse. **entailment**\n",
    "* Children smiling and waving at camera. There are children present. **entailment**\n",
    "* A boy is jumping on skateboard in the middle of a red bridge. The boy skates down the sidewalk. **contradiction**\n",
    "* An older man sits with his orange juice at a small table in a coffee shop while employees in bright colored shirts smile in the background. An older man drinks his juice as he waits for his daughter to get off work. **neutral**\n",
    "* High fashion ladies wait outside a tram beside a crowd of people in the city. The women do not care what clothes they wear. **contradiction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few-shot examples are collected from the Stanford Natural Language Inference (SNLI) Corpus. \\\n",
    "Paper: https://arxiv.org/pdf/1508.05326\n",
    "\n",
    "We used two sets of few-shot examples to experiment on. First set consists of entirely independent sentences, while the second set consists of sentences that are related to each other. In the second set, three different \"hypothesis\" sentences are created for each \"premise\" sentence. The idea is to prevent the model from relying solely on the first sentence for the relation prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Model | Prompt Engineering | Few-Shot Examples | Best Accuracy | Majority Vote Accuracy |\n",
    "| :---: | :---: | :---: |:---: |:---: |\n",
    "| pythia-410m | Zero-shot learning | -  | 0/6 | 0/6|\n",
    "| pythia-410m | One-shot learning |  - | 2/6 | 2/6 |\n",
    "| pythia-410m | Few-shot learning ($k = 3$) |  Independent | 3/6 | 3/6|\n",
    "| pythia-410m | Few-shot learning ($k = 9$) | Independent | 3/6 | 2/6 |\n",
    "| pythia-410m | Few-shot learning ($k = 3$) |Related | 3/6| 2/6 |\n",
    "| pythia-410m | Few-shot learning ($k = 9$) |  Related | 3/6 | 2/6 |\n",
    "| pythia-410m | Self-consistency  ($k = 3$) |  Independent | 2/6 | - |\n",
    "| pythia-410m | Self-consistency  ($k = 9$) |  Related | 2/6 | - |\n",
    "| pythia-1.4b | Zero-shot learning | -  | 0/6 | 0/6|\n",
    "| pythia-1.4b | One-shot learning |  - | 2/6  | 2/6|\n",
    "| pythia-1.4b | Few-shot learning ($k = 3$) |  Independent | 3/6 | 2/6|\n",
    "| pythia-1.4b | Few-shot learning ($k = 9$) | Independent | **4/6** (softmax sampling) | 2/6 |\n",
    "| pythia-1.4b | Few-shot learning ($k = 3$) |Related | **4/6** (contrastive decoding) | 3/6 |\n",
    "| pythia-1.4b | Few-shot learning ($k = 9$) |  Related | 3/6 | 2/6 |\n",
    "| pythia-1.4b | Self-consistency  ($k = 3$) |  Independent | 2/6 | - |\n",
    "| pythia-1.4b | Self-consistency  ($k = 9$) |  Related | 2/6 | - |\n",
    "\n",
    "* Best Accuracy: For optimal accuracy, we generate outputs for each decoding scheme and select the one with the highest accuracy, disregarding the others.\n",
    "* Majority Vote Accuracy (few-shot learning): Accuracy is determined by considering all outputs from different decoding schemes for a given input, with the final output decided by majority vote."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* When only the input is provided without any example or CoT prompting, the model doesn't even seem to understand the task - although the task is explicitly stated in the prompt. It attempts to complete the sentence with irrelevant information.\n",
    "* We tested all these approaches with and without CoT prompts. In this particular case, using the given models and few-shot examples, there doesn't seem to be any difference between prompting for reasoning and not.\n",
    "* There doesn't appear to be a hierarchy among the different decoding strategies, as the leading one varies from experiment to experiment.\n",
    "* The model seems to achieve the same accuracy for both independent and related few-shot examples.\n",
    "* `pythia-1.4b` appears to outperform `pythia-410m` when the few-shot examples are provided. However, the test set needs to be larger to draw a solid conclusion. Best accuracies are achieved when $k=3$ for both of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# independent\n",
    "few_shot_examples_v1 = [\n",
    "    \"Input: Children smiling and waving at camera. There are children present. Relation: entailment\",\n",
    "    \"Input: A man inspects the uniform of a figure in some East Asian country. The man is sleeping. Relation: contradiction\", # contradiction\n",
    "    \"Input: An older and younger man smiling. Two men are smiling and laughing at the cats playing on the floor. Relation: neutral\", # neutral\n",
    "    \"Input: This church choir sings to the masses as they sing joyous songs from the book at a church. The church is filled with song. Relation: entailment\",  # entailment\n",
    "    \"Input: A black race car starts up in front of a crowd of people. A man is driving down a lonely road. Relation: contradiction\", # contradiction\n",
    "    \"Input: A smiling costumed woman is holding an umbrella. A happy woman in a fairy costume holds an umbrella. Relation: neutral\", # neutral\n",
    "    \"Input: A soccer game with multiple males playing. Some men are playing a sport. Relation: entailment\",  # entailment\n",
    "    \"Input: Four dirty and barefooted children. Four kids won awards for 'cleanest feet'. Relation: contradiction\", # contradiction\n",
    "    \"Input: A woman with a green headscarf, blue shirt and a very big grin.\tThe woman is young. Relation: neutral\", # neutral\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# related\n",
    "few_shot_examples_v2 = [\n",
    "    \"Input: Children smiling and waving at camera. There are children present. Relation: entailment\",  # entailment\n",
    "    \"Input: Children smiling and waving at camera. The kids are frowning. Relation: contradiction\", # contradiction\n",
    "    \"Input: Children smiling and waving at camera. They are smiling at their parents. Relation: neutral\", # neutral\n",
    "    \"Input: This church choir sings to the masses as they sing joyous songs from the book at a church. The church is filled with song. Relation: entailment\",  # entailment\n",
    "    \"Input: This church choir sings to the masses as they sing joyous songs from the book at a church. A choir singing at a baseball game. Relation: contradiction\", # contradiction\n",
    "    \"Input: This church choir sings to the masses as they sing joyous songs from the book at a church. The church has cracks in the ceiling. Relation: neutral\", # neutral\n",
    "    \"Input: A woman with a green headscarf, blue shirt and a very big grin.\tThe woman is very happy. Relation: entailment\", # entailment\n",
    "    \"Input: A woman with a green headscarf, blue shirt and a very big grin.\tThe woman has been shot. Relation: contradiction\", # contradiction\n",
    "    \"Input: A woman with a green headscarf, blue shirt and a very big grin.\tThe woman is young. Relation: neutral\", # neutral\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(k, decoding, few_shot_examples, test_set):\n",
    "    print(f\"Decoding scheme: {decoding.__name__}\\n\")\n",
    "    for idx, test in enumerate(test_set):\n",
    "        suffix = \"\"\n",
    "        if(k):\n",
    "            few_shot = few_shot_examples[:k]\n",
    "            suffix = \"\\n\".join(few_shot) + \"\\n\"\n",
    "        task = \"Please classify whether two sentences form a ‚Äúcontradiction‚Äù or an ‚Äúentailment‚Äù, or the relation is ‚Äúneutral‚Äù. \\n\"\n",
    "        input_text = task + suffix +  test\n",
    "\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "        output = decoding(input_ids)\n",
    "        print(f\"{idx}) \" + test)\n",
    "        pretty_print(output[0], text=input_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_vote(k, few_shot_examples, test_set):\n",
    "  for idx, test in enumerate(test_set):\n",
    "    suffix = \"\"\n",
    "    if(k):\n",
    "        few_shot = few_shot_examples[:k]\n",
    "        suffix = \"\\n\".join(few_shot) + \"\\n\"\n",
    "    task = \"Please classify whether two sentences form a ‚Äúcontradiction‚Äù or an ‚Äúentailment‚Äù, or the relation is ‚Äúneutral‚Äù. \\n\"\n",
    "    input_text = task + suffix +  test\n",
    "\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "    print(f\"{idx}) \" + test)\n",
    "    for decoding in [greedy_decoding, beam_search_decoding, pure_sampling_decoding, softmax_sampling_decoding, top_k_sampling_decoding, top_p_sampling_decoding, contrastive_decoding]:\n",
    "      output = decoding(input_ids)\n",
    "      decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "      decoded = decoded.replace(input_text, \"\").replace(\"\\n\",\"\")\n",
    "      print(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoding strategies: greedy, beam search, pure sampling, softmax sampling, top-k sampling, top-p sampling, contrastive decoding\n",
    "for decoding in [greedy_decoding, beam_search_decoding, pure_sampling_decoding, softmax_sampling_decoding, top_k_sampling_decoding, top_p_sampling_decoding, contrastive_decoding]:\n",
    "    experiment(3, decoding, few_shot_examples_v2, test_set)\n",
    "\n",
    "## GT:  neutral entailment entailment contradiction neutral contradiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_vote(1, few_shot_examples_v1, test_set)\n",
    "## GT:  neutral entailment entailment contradiction neutral contradiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_consistency(k, decoding, few_shot_examples, test_set, n = 5):\n",
    "  for idx, test in enumerate(test_set):\n",
    "    suffix = \"\"\n",
    "    if(k):\n",
    "        few_shot = few_shot_examples[:k]\n",
    "        suffix = \"\\n\".join(few_shot) + \"\\n\"\n",
    "    task = \"Please classify whether two sentences form a ‚Äúcontradiction‚Äù or an ‚Äúentailment‚Äù, or the relation is ‚Äúneutral‚Äù.\\n\"\n",
    "    input_text = task + suffix +  test\n",
    "\n",
    "\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "    print(f\"{idx}) \" + test)\n",
    "\n",
    "    for i in range(n):\n",
    "      output = decoding(input_ids)\n",
    "      decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "      decoded = decoded.replace(input_text, \"\").replace(\"\\n\",\"\")\n",
    "      print(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_consistency(9, softmax_sampling_decoding, few_shot_examples_v1, test_set)\n",
    "## GT:  neutral entailment entailment contradiction neutral contradiction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple-choice QA\n",
    "\n",
    "Multiple-choice QA: the task is to predict the correct answer option for the question, given the question and the options.\n",
    "\n",
    "\n",
    "## Experiment Scheme\n",
    "In our experiments we worked on the following configurations:\n",
    "* Two models, `pythia-410m` and `pythia-1.4b` are used to generate the output.\n",
    "* Few-shot examples are collected from two different datasets: [CommonSenseQA](https://huggingface.co/datasets/tau/commonsense_qa) and [NumerSense](https://inklab.usc.edu/NumerSense/) dataset. CommonsenseQA is a multiple-choice question answering dataset that requires different types of commonsense knowledge to predict the correct answers . It contains 12,102 questions with one correct answer and four distractor answers. NumerSense is a numerical commonsense reasoning probing task, with a diagnostic dataset consisting of 3,145 masked-word-prediction probes.\n",
    "* We primarily explored two prompting techniques: Generated Knowledge Prompting (with few-shot examples) and Few-shot Prompting. In Generated Knowledge Prompting, the approach involves generating knowledge statements pertaining to the given question. Subsequently, we evaluate the log probability of each answer option being generated given these knowledge statements. This methodology enables the model to either support or oppose its own answer. We also employ two output extraction techniques: scoring the answers based on log-probabilities and classical generation. Classical generation utilizes the log-probabilities to generate new tokens. Hence, these approaches initially appear similar. However, the former allows us to constrain the model's output to only one of the options, thereby limiting its output possibilities. We worked on the following combinations:\n",
    "  * Generated Knowledge Prompting with NumerSense dataset + scoring\n",
    "  * Generated Knowledge Prompting with CommonSenseQA dataset + scoring\n",
    "  * Few Shot Learning with CommonSenseQA dataset + scoring\n",
    "  * Few Shot Learning with CommonSenseQA dataset + classical generation (softmax sampling)\n",
    "\n",
    "For the generated knowledge prompting, we generated five knowledge statements for each question. For each answer choice ùëé, we identified the knowledge statement that best supports it by calculating the log probability of generating that answer for each augmented prompt (with the knowledge statement). This process allows us to determine the maximum probability for each answer. Ultimately, we select the answer with the highest maximum probability.\n",
    "\n",
    "All the few-shot prompts can be found below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | Prompt Engineering | Dataset | Generation Technique | Accuracy |\n",
    "| :---: | :---: | :---: |:---: |:---: |\n",
    "| pythia-410m | Generated Knowledge Prompting | NumerSense  |  Log-Probability scoring | 3/6 |\n",
    "| pythia-410m | Generated Knowledge Prompting | CommonSenseQA  |  Log-Probability scoring | **4/6** |\n",
    "| pythia-410m | Few Shot learning | CommonSenseQA  |  Log-Probability scoring | 0/6 |\n",
    "| pythia-410m | Few Shot learning | CommonSenseQA  |  Generation with Softmax Sampling | 1/6 |\n",
    "| pythia-1.4b | Generated Knowledge Prompting | NumerSense  |  Log-Probability scoring | 2/6 |\n",
    "| pythia-1.4b| Generated Knowledge Prompting | CommonSenseQA  |  Log-Probability scoring | 2/6 |\n",
    "| pythia-1.4b | Few Shot learning | CommonSenseQA  |  Log-Probability scoring | 1/6 |\n",
    "| pythia-1.4b | Few Shot learning | CommonSenseQA  |  Generation with Softmax Sampling | 2/6 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* Overall, generated knowledge prompting seems to outperform vanilla few shot learning (no knowledge statement is generated).\n",
    "* Maximum accuracy is achieved using generated knowledge prompting on CommonSenseQA with `pythia-410m`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generated Knowledge Prompting with NumerSense Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = {\n",
    "    \"The only baggage the woman checked was a drawstring bag, where was she heading with it?\": [\"garbage can\", \"military\", \"jewelry store\", \"safe\", \"airport\"],\n",
    "    \"To prevent any glare during the big football game he made sure to clean the dust of his what?\": [\"television\", \"attic\", \"corner\", \"they cannot clean corner and library during football match they cannot need that\", \"ground\"],\n",
    "    \"The president is the leader of what institution?\": [\"walmart\", \"white house\", \"country\", \"corporation\", \"government\"],\n",
    "    \"What kind of driving leads to accidents?\": [\"stressful\", \"dangerous\", \"fun\", \"illegal\", \"deadly\"],\n",
    "    \"Can you name a good reason for attending school?\": [\"get smart\", \"boredom\", \"colds and flu\", \"taking tests\", \"spend time\"],\n",
    "    \"Stanley had a dream that was very vivid and scary. He had trouble telling it from what?\": [\"imagination\", \"reality\", \"dreamworker\", \"nightmare\", \"awake\"]\n",
    "}\n",
    "\n",
    "# GT: airport - television - country - dangerous - get smart - reality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed few shot prompt\n",
      "How many wings do penguins have? We know that birds have two wings. penguin is a kind of bird.\n",
      "How many sides does a parallelogram have? We know that a rectangular is a parallelogram. a square is a parallelogram.\n",
      "What is the number of limbs a typical human being has? We know that human beings have four limbs.\n",
      "How many feet are there in a yard? We know that a yard is three feet.\n"
     ]
    }
   ],
   "source": [
    "# Run this to get NumerSense dataset few shot prompts\n",
    "import pandas as pd\n",
    "\n",
    "inputs = [\n",
    "    \"How many wings do penguins have?\",\n",
    "    \"How many sides does a parallelogram have?\",\n",
    "    \"What is the number of limbs a typical human being has?\",\n",
    "    \"How many feet are there in a yard?\",\n",
    "]\n",
    "\n",
    "knowledges = [\n",
    "    \"Birds have two wings. Penguin is a kind of bird.\",\n",
    "    \"A rectangular is a parallelogram. A square is a parallelogram.\",\n",
    "    \"Human beings have four limbs.\",\n",
    "    \"A yard is three feet.\",\n",
    "]\n",
    "\n",
    "# Creating the dataframe\n",
    "df = pd.DataFrame({\n",
    "    'input': inputs,\n",
    "    'knowledge': knowledges\n",
    "})\n",
    "df\n",
    "\n",
    "\n",
    "few_shot_template = \"\"\"{q} We know that {k}\"\"\"\n",
    "\n",
    "few_shot_prompt = \"\\n\".join([\n",
    "    few_shot_template.format(\n",
    "        q=df.loc[i, \"input\"],\n",
    "        k=df.loc[i, \"knowledge\"].lower()\n",
    "    )\n",
    "    for i in range(len(df))\n",
    "])\n",
    "print(\"Constructed few shot prompt\\n\" + few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed few shot prompt\n",
      "Google Maps and other highway and street GPS services have replaced what? We know that electronic maps are the modern version of paper atlas.\n",
      "The fox walked from the city into the forest, what was it looking for? We know that natural habitats are usually away from cities.\n",
      "You can share files with someone if you have a connection to a what? We know that files can be shared over the internet.\n",
      "Too many people want exotic snakes. The demand is driving what to carry them? We know that some people raise snakes as pets.\n",
      "The bodyguard was good at his duties, he made the person who hired him what? We know that the job of bodyguards is to ensure the safety and security of the employer.\n"
     ]
    }
   ],
   "source": [
    "# Run this to get CommonSenseQA dataset few shot prompts\n",
    "\n",
    "inputs = [\n",
    "    \"Google Maps and other highway and street GPS services have replaced what?\",\n",
    "    \"The fox walked from the city into the forest, what was it looking for?\",\n",
    "    \"You can share files with someone if you have a connection to a what?\",\n",
    "    \"Too many people want exotic snakes. The demand is driving what to carry them?\",\n",
    "    \"The bodyguard was good at his duties, he made the person who hired him what?\"\n",
    "]\n",
    "\n",
    "knowledges = [\n",
    "    \"Electronic maps are the modern version of paper atlas.\",\n",
    "    \"Natural habitats are usually away from cities.\",\n",
    "    \"Files can be shared over the Internet.\",\n",
    "    \"Some people raise snakes as pets.\",\n",
    "    \"The job of bodyguards is to ensure the safety and security of the employer.\"\n",
    "]\n",
    "# Creating the dataframe\n",
    "df = pd.DataFrame({\n",
    "    'input': inputs,\n",
    "    'knowledge': knowledges\n",
    "})\n",
    "df\n",
    "\n",
    "\n",
    "few_shot_template = \"\"\"{q} We know that {k}\"\"\"\n",
    "\n",
    "few_shot_prompt = \"\\n\".join([\n",
    "    few_shot_template.format(\n",
    "        q=df.loc[i, \"input\"],\n",
    "        k=df.loc[i, \"knowledge\"].lower()\n",
    "    )\n",
    "    for i in range(len(df))\n",
    "])\n",
    "print(\"Constructed few shot prompt\\n\" + few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated knowledge:  \n",
      "bags are usually carried by the person carrying them.\n",
      "\n",
      "So,\n"
     ]
    }
   ],
   "source": [
    "# Only for visualizing a generated knowledge:\n",
    "question = list(qa)[0]\n",
    "choices = qa[question]\n",
    "prompt_input_ids = tokenizer(\n",
    "    few_shot_prompt + \"\\n\" + question + \" We know that \",\n",
    "    return_tensors=\"pt\"\n",
    ").input_ids.to(device)\n",
    "\n",
    "knowledge_statements = model.generate(\n",
    "    prompt_input_ids,\n",
    "    max_new_tokens=15,\n",
    "    do_sample=True,\n",
    "    temperature=0.5\n",
    ")\n",
    "# access the knowledge statements (i.e., only text that comes after prompt)\n",
    "knowledge = tokenizer.decode(\n",
    "    knowledge_statements[0, prompt_input_ids.shape[-1]:],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "print(\"Generated knowledge: \", knowledge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated knowledge:  \n",
      "the woman checked her luggage, but she didn't take any of her\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated knowledge:  \n",
      "baggage is usually checked in checked baggage.\n",
      "The man was not\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated knowledge:  \n",
      "women carry small bags, especially when they're on the go.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated knowledge:  \n",
      "the woman checked her bag before she left.\n",
      "What was the reason\n",
      "Generated knowledge:  \n",
      "bags are usually carried in a carrier bag or a briefcase.\n",
      "\n",
      "Answer  garbage can Answer probabilities for each knowledge statement:  [-8.651183128356934, -8.358133316040039, -9.2638578414917, -8.854644775390625, -8.918338775634766]\n",
      "Answer  military Answer probabilities for each knowledge statement:  [-13.746541976928711, -13.27782917022705, -14.510452270507812, -14.339978218078613, -14.297685623168945]\n",
      "Answer  jewelry store Answer probabilities for each knowledge statement:  [-9.776968002319336, -10.095916748046875, -10.659232139587402, -9.95555591583252, -10.486684799194336]\n",
      "Answer  safe Answer probabilities for each knowledge statement:  [-11.570777893066406, -11.832415580749512, -13.639629364013672, -12.748946189880371, -13.790425300598145]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer  airport Answer probabilities for each knowledge statement:  [-11.538839340209961, -9.888652801513672, -12.38808536529541, -11.434816360473633, -11.631083488464355]\n",
      "Selected answer  garbage can with log P  -8.358133316040039\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated knowledge:  umpires are also responsible for cleaning the field.\n",
      "The police officer was\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated knowledge:  icing is used to stop glare on the ice.\n",
      "The computer software was\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated knowledge:  icing is a technique used to prevent glare in the game of football.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated knowledge:  umpires are responsible for preventing any glare.\n",
      "The person who wanted to\n",
      "Generated knowledge:  umpires are the workers who make sure the game goes well.\n",
      "You\n",
      "Answer  television Answer probabilities for each knowledge statement:  [-10.235928535461426, -9.407893180847168, -8.890667915344238, -9.370429039001465, -11.198206901550293]\n",
      "Answer  attic Answer probabilities for each knowledge statement:  [-13.596774101257324, -12.11440658569336, -11.61604118347168, -11.67751693725586, -13.281587600708008]\n",
      "Answer  corner Answer probabilities for each knowledge statement:  [-11.031961441040039, -10.703117370605469, -10.687712669372559, -11.021014213562012, -11.909016609191895]\n",
      "Answer  they cannot clean corner and library during football match they cannot need that Answer probabilities for each knowledge statement:  [-6.271496772766113, -6.655148983001709, -6.51792049407959, -6.4584479331970215, -6.452688217163086]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer  ground Answer probabilities for each knowledge statement:  [-8.554006576538086, -9.23339557647705, -8.114116668701172, -8.754240989685059, -9.520474433898926]\n",
      "Selected answer  they cannot clean corner and library during football match they cannot need that with log P  -6.271496772766113\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated knowledge:  umpires are the people who make the decisions about what to call an \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated knowledge:  \n",
      "presidents can be elected by the people.\n",
      "\n",
      "The president is\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated knowledge:  \n",
      "the president is the leader of what institution? We know that the president\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated knowledge:  \n",
      "the president has a lot of power, but the president is also the\n",
      "Generated knowledge:  umpires are the people who officiate at baseball games.\n",
      "The cat\n",
      "Answer  walmart Answer probabilities for each knowledge statement:  [-8.142420768737793, -8.972271919250488, -8.90455436706543, -8.03041934967041, -7.8204474449157715]\n",
      "Answer  white house Answer probabilities for each knowledge statement:  [-8.612419128417969, -7.63491153717041, -7.091569900512695, -6.645547866821289, -8.576825141906738]\n",
      "Answer  country Answer probabilities for each knowledge statement:  [-10.13504695892334, -12.369648933410645, -13.64605712890625, -11.518067359924316, -11.83149242401123]\n",
      "Answer  corporation Answer probabilities for each knowledge statement:  [-12.276028633117676, -14.06977367401123, -16.650859832763672, -13.893194198608398, -13.85256576538086]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer  government Answer probabilities for each knowledge statement:  [-10.173294067382812, -9.73061466217041, -11.749650001525879, -9.258166313171387, -10.929475784301758]\n",
      "Selected answer  white house with log P  -6.645547866821289\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated knowledge:  \n",
      "driving is a very dangerous job.\n",
      "What kind of car was the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated knowledge:  ####### has a very high accident rate.\n",
      "What do you mean by\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated knowledge:  \n",
      "There are many types of accidents:\n",
      "\n",
      "Faulty brakes\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated knowledge:  \n",
      "- the driver is under the influence of drugs\n",
      "- the driver is\n",
      "Generated knowledge:  \n",
      "accidents happen because of what? We know that we can‚Äôt\n",
      "Answer  stressful Answer probabilities for each knowledge statement:  [-14.708761215209961, -15.258934020996094, -15.46780014038086, -14.419212341308594, -14.7927885055542]\n",
      "Answer  dangerous Answer probabilities for each knowledge statement:  [-10.114299774169922, -11.092577934265137, -11.165120124816895, -9.094646453857422, -10.583547592163086]\n",
      "Answer  fun Answer probabilities for each knowledge statement:  [-14.501969337463379, -14.986560821533203, -16.233652114868164, -14.622293472290039, -13.889074325561523]\n",
      "Answer  illegal Answer probabilities for each knowledge statement:  [-11.216497421264648, -13.336479187011719, -12.792975425720215, -9.515718460083008, -11.635777473449707]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer  deadly Answer probabilities for each knowledge statement:  [-12.690098762512207, -14.63901424407959, -15.035849571228027, -12.710489273071289, -13.199512481689453]\n",
      "Selected answer  dangerous with log P  -9.094646453857422\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated knowledge:  \n",
      "school is expensive.\n",
      "\n",
      "A:\n",
      "\n",
      "There are several reasons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated knowledge:  \n",
      "schools are meant to prepare students for the future.\n",
      "\n",
      "The\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated knowledge:  \n",
      "schools are for learning and education, they do not teach people how\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated knowledge:  \n",
      "schools will be the place for you to learn what? We know\n",
      "Generated knowledge:  ith the education system.\n",
      "\n",
      "A:\n",
      "\n",
      "I'm not sure\n",
      "Answer  get smart Answer probabilities for each knowledge statement:  [-8.589640617370605, -8.743338584899902, -8.013982772827148, -8.673873901367188, -9.343545913696289]\n",
      "Answer  boredom Answer probabilities for each knowledge statement:  [-7.858763694763184, -7.82045841217041, -7.263005256652832, -7.9718918800354, -7.668207168579102]\n",
      "Answer  colds and flu Answer probabilities for each knowledge statement:  [-5.3271989822387695, -5.516082286834717, -5.24860143661499, -5.370843410491943, -5.465941429138184]\n",
      "Answer  taking tests Answer probabilities for each knowledge statement:  [-8.29901123046875, -7.830539703369141, -7.338224411010742, -8.12620735168457, -7.884116172790527]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer  spend time Answer probabilities for each knowledge statement:  [-7.690305233001709, -7.893522262573242, -7.179699897766113, -7.780394554138184, -8.239646911621094]\n",
      "Selected answer  colds and flu with log P  -5.24860143661499\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated knowledge:  \n",
      "Stanley had trouble telling it from what? We know that Stanley had\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated knowledge:  umpires are often called to decide games.\n",
      "A person who is very\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated knowledge:  \n",
      "Stanley had a dream that was very vivid and scary. He had\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated knowledge:  \n",
      "Stanley had an aversion to heights.\n",
      "The man was a\n",
      "Generated knowledge:  icing is used in medicine to treat pain.\n",
      "The computer was a big\n",
      "Answer  imagination Answer probabilities for each knowledge statement:  [-14.361309051513672, -11.466972351074219, -12.520139694213867, -9.94018840789795, -11.04466724395752]\n",
      "Answer  reality Answer probabilities for each knowledge statement:  [-12.76948356628418, -9.612512588500977, -9.671875, -7.55483865737915, -8.412652015686035]\n",
      "Answer  dreamworker Answer probabilities for each knowledge statement:  [-14.165630340576172, -10.419055938720703, -12.055522918701172, -12.118217468261719, -11.015838623046875]\n",
      "Answer  nightmare Answer probabilities for each knowledge statement:  [-15.019416809082031, -11.555357933044434, -11.577406883239746, -10.60135555267334, -11.841059684753418]\n",
      "Answer  awake Answer probabilities for each knowledge statement:  [-15.815916061401367, -12.528425216674805, -13.598419189453125, -13.201595306396484, -13.424176216125488]\n",
      "Selected answer  reality with log P  -7.55483865737915\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 3. Score each answer to the question based on the knowledge statements\n",
    "# as the score, we take the average log probability of the tokens in the answer\n",
    "# iterate over the answer options\n",
    "import numpy as np\n",
    "no_knowledge_statements = 5\n",
    "\n",
    "for question in list(qa):\n",
    "  answers = qa[question]\n",
    "  answer_log_probs = []\n",
    "  prompt_input_ids = tokenizer(\n",
    "    few_shot_prompt + \"\\n\" + question + \" We know that \",\n",
    "    return_tensors=\"pt\"\n",
    "    ).input_ids.to(device)\n",
    "\n",
    "  knowledge_statements = []\n",
    "  for i in range(no_knowledge_statements):\n",
    "    knowledge = model.generate(\n",
    "        prompt_input_ids,\n",
    "        max_new_tokens=15,\n",
    "        do_sample=True,\n",
    "        temperature=0.5\n",
    "    )\n",
    "\n",
    "    # access the knowledge statements (i.e., only text that comes after prompt)\n",
    "    knowledge = tokenizer.decode(\n",
    "        knowledge[0, prompt_input_ids.shape[-1]:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    knowledge_statements.append(knowledge)\n",
    "    print(\"Generated knowledge: \", knowledge)\n",
    "\n",
    "  # now we have knowledge statements in hand\n",
    "  maximizing_answer = None\n",
    "  maximizing_log_prob = -float(\"inf\")\n",
    "  for a in answers:\n",
    "    log_probs_for_a = []\n",
    "    for knowledge in knowledge_statements:\n",
    "        # construct the full prompt\n",
    "        prompt = f\"{knowledge} {question} {a}\"\n",
    "        # construct the prompt without the answer to create a mask which will\n",
    "        # allow to retrieve the token probabilities for tokens in the answer only\n",
    "        context_prompt = f\"{knowledge} {question}\"\n",
    "        # tokenize the prompt\n",
    "        input_ids = tokenizer(prompt,\n",
    "                            return_tensors=\"pt\").input_ids.to(device)\n",
    "        # tokenize the context prompt\n",
    "        context_input_ids = tokenizer(context_prompt,\n",
    "                                    return_tensors=\"pt\").input_ids\n",
    "        # create a mask with -100 for all tokens in the context prompt\n",
    "        # the -100 indicates that the token should be ignored in the loss computation\n",
    "        masked_labels = torch.ones_like(input_ids) * -100\n",
    "        masked_labels[:, context_input_ids.shape[-1]:] = input_ids[:, context_input_ids.shape[-1]:]\n",
    "        # generate the answer\n",
    "        preds = model(\n",
    "            input_ids,\n",
    "            labels=masked_labels\n",
    "        )\n",
    "        # retrieve the average log probability of the tokens in the answer\n",
    "        log_p = preds.loss.item()\n",
    "        log_probs_for_a.append(-log_p)\n",
    "    max_prob = np.max(log_probs_for_a)\n",
    "    if max_prob > maximizing_log_prob:\n",
    "        maximizing_log_prob = max_prob\n",
    "        maximizing_answer = a\n",
    "    print(\"Answer \", a, \"Answer probabilities for each knowledge statement: \", log_probs_for_a)\n",
    "  print(\"Selected answer \", maximizing_answer, \"with log P \", maximizing_log_prob)\n",
    "  print(100 * \"-\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-Shot Prompting with CommonSenseQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Predict the correct answer option for the question provided, considering the available options.\n",
      "Instructions: Review the question and choices provided, then select the option you believe is the correct answer.\n",
      "\n",
      "Question: A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?\n",
      "Options:\n",
      "A. bank\n",
      "B. library\n",
      "C. department store\n",
      "D. mall\n",
      "E. new york\n",
      "Selected Choice: A\n",
      "\n",
      "Question: What do people aim to do at work?\n",
      "Options:\n",
      "A. complete job\n",
      "B. learn from each other\n",
      "C. kill animals\n",
      "D. wear hats\n",
      "E. talk to each other\n",
      "Selected Choice: A\n",
      "\n",
      "Question: Where would you find magazines alongside many other printed works?\n",
      "Options:\n",
      "A. doctor\n",
      "B. bookstore\n",
      "C. market\n",
      "D. train station\n",
      "E. mortuary\n",
      "Selected Choice: B\n",
      "\n",
      "Question: Where are you likely to find a hamburger?\n",
      "Options:\n",
      "A. fast food restaurant\n",
      "B. pizza\n",
      "C. ground up dead cows\n",
      "D. mouth\n",
      "E. cow carcass\n",
      "Selected Choice: A\n",
      "\n",
      "Question: James was looking for a good place to buy farmland. Where might he look?\n",
      "Options:\n",
      "A. midwest\n",
      "B. countryside\n",
      "C. estate\n",
      "D. farming areas\n",
      "E. illinois\n",
      "Selected Choice: A\n",
      "\n",
      "Question: What island country is ferret popular?\n",
      "Options:\n",
      "A. own home\n",
      "B. north carolina\n",
      "C. great britain\n",
      "D. hutch\n",
      "E. outdoors\n",
      "Selected Choice: C\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    ('A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?', ['A. bank', 'B. library', 'C. department store', 'D. mall', 'E. new york'], 'A'),\n",
    "    ('What do people aim to do at work?', ['A. complete job', 'B. learn from each other', 'C. kill animals', 'D. wear hats', 'E. talk to each other'], 'A'),\n",
    "    ('Where would you find magazines alongside many other printed works?', ['A. doctor', 'B. bookstore', 'C. market', 'D. train station', 'E. mortuary'], 'B'),\n",
    "    ('Where are you likely to find a hamburger?', ['A. fast food restaurant', 'B. pizza', 'C. ground up dead cows', 'D. mouth', 'E. cow carcass'], 'A'),\n",
    "    ('James was looking for a good place to buy farmland. Where might he look?', ['A. midwest', 'B. countryside', 'C. estate', 'D. farming areas', 'E. illinois'], 'A'),\n",
    "    ('What island country is ferret popular?', ['A. own home', 'B. north carolina', 'C. great britain', 'D. hutch', 'E. outdoors'], 'C')\n",
    "]\n",
    "df = pd.DataFrame(data, columns=['Question', 'Answer_Options', 'Correct_Answer'])\n",
    "\n",
    "# Define task and instructions prompts\n",
    "task_prompt = \"Task: Predict the correct answer option for the question provided, considering the available options.\"\n",
    "instructions_prompt = \"Instructions: Review the question and choices provided, then select the option you believe is the correct answer.\"\n",
    "\n",
    "# Generate prompt with multiple examples\n",
    "few_shot_prompt = f\"{task_prompt}\\n{instructions_prompt}\\n\"\n",
    "for index, row in df.iterrows():\n",
    "    few_shot_prompt += \"\\n\"\n",
    "    few_shot_prompt += f\"Question: {row['Question']}\\nOptions:\\n\"\n",
    "    for option in row['Answer_Options']:\n",
    "        few_shot_prompt += f\"{option}\\n\"\n",
    "    few_shot_prompt += f\"Selected Choice: {row['Correct_Answer']}\\n\"\n",
    "\n",
    "# Display prompt\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Predict the correct answer option for the question provided, considering the available options.\n",
      "Instructions: Review the question and choices provided, then select the option you believe is the correct answer.\n",
      "\n",
      "Question: A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?\n",
      "Options:\n",
      "A. bank\n",
      "B. library\n",
      "C. department store\n",
      "D. mall\n",
      "E. new york\n",
      "Selected Choice: A\n",
      "\n",
      "Question: What do people aim to do at work?\n",
      "Options:\n",
      "A. complete job\n",
      "B. learn from each other\n",
      "C. kill animals\n",
      "D. wear hats\n",
      "E. talk to each other\n",
      "Selected Choice: A\n",
      "\n",
      "Question: Where would you find magazines alongside many other printed works?\n",
      "Options:\n",
      "A. doctor\n",
      "B. bookstore\n",
      "C. market\n",
      "D. train station\n",
      "E. mortuary\n",
      "Selected Choice: B\n",
      "\n",
      "Question: Where are you likely to find a hamburger?\n",
      "Options:\n",
      "A. fast food restaurant\n",
      "B. pizza\n",
      "C. ground up dead cows\n",
      "D. mouth\n",
      "E. cow carcass\n",
      "Selected Choice: A\n",
      "\n",
      "Question: James was looking for a good place to buy farmland. Where might he look?\n",
      "Options:\n",
      "A. midwest\n",
      "B. countryside\n",
      "C. estate\n",
      "D. farming areas\n",
      "E. illinois\n",
      "Selected Choice: A\n",
      "\n",
      "Question: What island country is ferret popular?\n",
      "Options:\n",
      "A. own home\n",
      "B. north carolina\n",
      "C. great britain\n",
      "D. hutch\n",
      "E. outdoors\n",
      "Selected Choice: C\n",
      "\n",
      "Question: The only baggage the woman checked was a drawstring bag, where was she heading with it?\n",
      "Options:\n",
      "A. garbage can\n",
      "B. military\n",
      "C. jewelry store\n",
      "D. safe\n",
      "E. airport\n",
      "Selected Choice:\n"
     ]
    }
   ],
   "source": [
    "question = list(qa)[0]\n",
    "choices = qa[question]\n",
    "\n",
    "print(few_shot_prompt + \"\\nQuestion: \" + question + \"\\nOptions:\\n\" + \"\\n\".join([chr(ord('A') + i) + \". \" + choices[i] for i in range(len(choices))]) + \"\\nSelected Choice:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All answers  ['garbage can', 'military', 'jewelry store', 'safe', 'airport']\n",
      "Answer probabilities  [-1.149327039718628, -0.8839691281318665, -1.7674624919891357, -2.4614875316619873, -5.297186851501465]\n",
      "Selected answer  military with log P  -0.8839691281318665\n",
      "----------------------------------------------------------------------------------------------------\n",
      "All answers  ['television', 'attic', 'corner', 'they cannot clean corner and library during football match they cannot need that', 'ground']\n",
      "Answer probabilities  [-1.6051456928253174, -1.3155701160430908, -1.452256441116333, -1.5575783252716064, -2.7076313495635986]\n",
      "Selected answer  attic with log P  -1.3155701160430908\n",
      "----------------------------------------------------------------------------------------------------\n",
      "All answers  ['walmart', 'white house', 'country', 'corporation', 'government']\n",
      "Answer probabilities  [-1.0540814399719238, -0.9338192343711853, -1.921186923980713, -2.363598346710205, -4.736490726470947]\n",
      "Selected answer  white house with log P  -0.9338192343711853\n",
      "----------------------------------------------------------------------------------------------------\n",
      "All answers  ['stressful', 'dangerous', 'fun', 'illegal', 'deadly']\n",
      "Answer probabilities  [-0.7589095234870911, -1.0533545017242432, -2.1971399784088135, -2.8056986331939697, -6.040278911590576]\n",
      "Selected answer  stressful with log P  -0.7589095234870911\n",
      "----------------------------------------------------------------------------------------------------\n",
      "All answers  ['get smart', 'boredom', 'colds and flu', 'taking tests', 'spend time']\n",
      "Answer probabilities  [-1.03721284866333, -0.8900933861732483, -1.955305576324463, -2.6002087593078613, -4.708530902862549]\n",
      "Selected answer  boredom with log P  -0.8900933861732483\n",
      "----------------------------------------------------------------------------------------------------\n",
      "All answers  ['imagination', 'reality', 'dreamworker', 'nightmare', 'awake']\n",
      "Answer probabilities  [-0.9054934978485107, -1.1341121196746826, -1.681248426437378, -2.6236846446990967, -5.031479835510254]\n",
      "Selected answer  imagination with log P  -0.9054934978485107\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "answer_log_probs = []\n",
    "# iterate over the answer options\n",
    "# NOTE: This can take a moment\n",
    "\n",
    "for question in list(qa):\n",
    "  answers = qa[question]\n",
    "  answer_log_probs = []\n",
    "\n",
    "  for choice in ['A','B','C','D','E']:\n",
    "    # construct the full prompt\n",
    "    context_prompt = few_shot_prompt + \"\\nQuestion: \" + question + \"\\nOptions:\\n\" + \"\\n\".join([chr(ord('A') + i) + \". \" + answers[i] for i in range(len(answers))]) + \"\\nSelected Choice:\"\n",
    "    prompt = context_prompt + \" \" + choice\n",
    "\n",
    "    # construct the prompt without the answer to create a mask which will\n",
    "    # allow to retrieve the token probabilities for tokens in the answer only\n",
    "    # tokenize the prompt\n",
    "    input_ids = tokenizer(prompt,\n",
    "                          return_tensors=\"pt\").input_ids.to(device)\n",
    "    # tokenize the context prompt\n",
    "    context_input_ids = tokenizer(context_prompt,\n",
    "                                  return_tensors=\"pt\").input_ids\n",
    "    # create a mask with -100 for all tokens in the context prompt\n",
    "    # the -100 indicates that the token should be ignored in the loss computation\n",
    "    masked_labels = torch.ones_like(input_ids) * -100\n",
    "    masked_labels[:, context_input_ids.shape[-1]:] = input_ids[:, context_input_ids.shape[-1]:]\n",
    "    # generate the answer\n",
    "    preds = model(\n",
    "        input_ids,\n",
    "        labels=masked_labels\n",
    "    )\n",
    "    # retrieve the average log probability of the tokens in the answer\n",
    "    log_p = preds.loss.item()\n",
    "    answer_log_probs.append(-log_p)\n",
    "  import numpy as np\n",
    "  print(\"All answers \", answers)\n",
    "  print(\"Answer probabilities \", answer_log_probs)\n",
    "  max_prob_idx = np.argmax(answer_log_probs)\n",
    "  print(\"Selected answer \", answers[max_prob_idx], \"with log P \", answer_log_probs[max_prob_idx])\n",
    "  print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The only baggage the woman checked was a drawstring bag, where was she heading with it?\n",
      " B\n",
      "To prevent any glare during the big football game he made sure to clean the dust of his what?\n",
      " B\n",
      "The president is the leader of what institution?\n",
      " A\n",
      "What kind of driving leads to accidents?\n",
      " B\n",
      "Can you name a good reason for attending school?\n",
      " B\n",
      "Stanley had a dream that was very vivid and scary. He had trouble telling it from what?\n",
      " D\n"
     ]
    }
   ],
   "source": [
    "for question in list(qa):\n",
    "  answers = qa[question]\n",
    "  prompt = few_shot_prompt + \"\\nQuestion: \" + question + \"\\nOptions:\\n\" + \"\\n\".join([chr(ord('A') + i) + \". \" + answers[i] for i in range(len(answers))]) + \"\\nSelected Choice:\"\n",
    "\n",
    "  input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "  output = softmax_sampling_decoding(input_ids)\n",
    "  decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "  decoded = decoded.replace(prompt, \"\").replace(\"\\n\",\"\")\n",
    "  print(question)\n",
    "  print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: First neural LM (20 points)\n",
    "\n",
    "Next to reading and understanding package documentations, a key skill for NLP researchers and practitioners is reading and critically assessing NLP literature. The density, but also the style of NLP literature has undergone a significant shift in the recent years with increasing acceleration of progress. Your task in this exercise is to read a paper about one of the first successful neural langauge models, understand its key architectural components and compare how these key components have evolved in modern systems that were discussed in the lecture. \n",
    "\n",
    "> Specifically, please read this paper and answer the following questions: [Bengio et al. (2003)](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n",
    ">\n",
    "> * How were words / tokens represented? What is the difference / similarity to modern LLMs?\n",
    "> * How was the context represented? What is the difference / similarity to modern LLMs?\n",
    "> * What is the curse of dimensionality? Give a concrete example in the context of language modeling.\n",
    "> * Which training data was used? What is the difference / similarity to modern LLMs?\n",
    "> * Which components of the Bengio et al. (2003) model (if any) can be found in modern LMs?\n",
    "> \n",
    "> * Please formulate one question about the paper (not the same as the questions above) and post it to the dedicated **Forum** space, and **answer 1 other question** about the paper.\n",
    "\n",
    "Furthermore, your task is to carefully dissect the paper by Bengio et al. (2003) and analyse its structure and style in comparison to another more recent paper:  [Devlin et al. (2019) BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805)\n",
    "\n",
    "**TASK:**\n",
    "\n",
    "> For each section of the Bengio et al. (2003) paper, what are key differences between the way it is written, the included contents, to the BERT paper (Devlin et al., 2019)? What are key similarities? Write max. 2 sentences per section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Answers\n",
    "> #### How were words / tokens represented? What is the difference / similarity to modern LLMs?\n",
    ">A continuous real-vector for each word was used to represent similarity between words (instead of using discrete random or deterministic variables). That way, each word is associated with a specific point in the vector space, while the number of features is smaller than the size of the vocabulary.\n",
    "That idea reminds of the feature vectors used in information retrieval. However, here we are looking at the probability distribution of word sequences from natural language text.\n",
    "Like the Bengio et al. paper, LLMs using a vector representation. However, modern LLMs work with context-sensitive embeddings. Also, they divide the word into sub words. \n",
    "> \n",
    "> #### How was the context represented? What is the difference / similarity to modern LLMs?\n",
    ">The vector, that is learnt to represent a word, is based on the preceding context. This follows the intuition that words, which occur in similar contexts, have similar meaning.\n",
    "In contrast Modern LLMs work with self-attention and Transformers. They also use much wider context-windows, than the model decribed here.\n",
    "> \n",
    "> #### What is the curse of dimensionality? Give a concrete example in the context of language modelling.\n",
    ">The curse of dimensionality occurs while analysing data in high-dimensional spaces. When the dimensionality rises, the volume of the space increases exponentially. The Problem is that words, which are similar, will still be different in high dimensial space. This is because even though they appear relatively often in simialar contexts, most of the contexts will still be different, if we consider every possible context. \n",
    "One example is a joint distribution of 10 consecutive words with a vocabulary size of 100,000. Here we have 100,000^10-1 = 10^50-1 free parameters. \n",
    "This model is using a joint probability function of word feature vector sequences which is a smooth function of this feature values with a neural network. \n",
    "Doing so, the method is crucially different to modern LLMs.\n",
    "> \n",
    "> #### Which training data was used? What is the difference / similarity to modern LLMs?\n",
    ">The training set is a sequence of words, the vocabulary large but finite. \n",
    "Comparative experiments were performed on the Brown corpus, where the first 800,000 words were used for the training data set. \n",
    "Furthermore, a experiment was run on the Associated Press News texts, where the training set consist of a stream of about 14 million words.\n",
    "The training data of Modern LLMs is much larger and much more diverse. \n",
    "> \n",
    "> #### Which components of the Bengio et al. (2003) model (if any) can be found in modern LMs?\n",
    "> Bengio et al.:\n",
    "> - Using neural networks with softmax\n",
    "> - generalize unseen words with similarity between word vectors\n",
    "> \n",
    ">LLMs:\n",
    "> - Self-Attention\n",
    "> - contextualised embeddings\n",
    "> - Masking\n",
    "> - Special Tokens \n",
    "> - Fine-Tuning\n",
    "\n",
    "> ### differences per section\n",
    " > The section are selected as the main section of the Bengio et al. paper\n",
    "> #### Abstract\n",
    "> Similar in both papers: introducing problem and solution.\n",
    "> \n",
    "> #### Introduction\n",
    "> Bengio et al. describes the challenges of statistical modelling and offers a neural network solution. A special focus is here the curse of dimensionality.\n",
    "Devlin et al. explains the limitation of existing pre-trained techniques and introduced BERT. \n",
    "> Therefore, Bengio et al. introduce a new theoretical framework whereas Denvio et. al introduces an entire alternative model.\n",
    "> Bengio et al. looks into earlier neural networks and statistical models whereas Devlin et al. focus on feature based models like ELMo.\n",
    "> \n",
    "> #### A neural model\n",
    "> Bengio et al. describes a neural network architecture. Devlin et al. explains a pre-trained transformer with a giant corpus, masked language modelling and finetuning.\n",
    "> Both papers are explaining there architecture detailed and with pictures. But Bengio et al. give a much more detailed description of the theoretical background, while Devlin et al. presuppose knowledge about general transformer architecture and emphasize the diffenrencs and improvements of BERT.\n",
    "> \n",
    "> #### Parallel Implementation\n",
    "> Bengio et al. emphasizes parallel computation where the hardware no longer exists. There is no comaprable section in the paper of Devlin et al.\n",
    "> \n",
    "> #### Experimental Results\n",
    "> Bengio et al. uses only perplexity reduction on the Brown dataset and the AP News Corpora. The paper also compares to SOTA.  Devlin et al. uses different NLP benchmarks.\n",
    "> \n",
    "> #### Extensions and Future Work\n",
    "> Bengio et al. describes different possible improvements that can be tried out in the future. Devlin et al.is keeping this part very short. \n",
    "> \n",
    "> #### Conclusion\n",
    "> Both papers summarize their break throughs. Nonetheless, the conclusion in the BERT paper is shorter than in the paper introducing the neural network solution.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
