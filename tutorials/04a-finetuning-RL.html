

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>4.1 Supervised fine-tuning and RL fine-tuning &#8212; Understanding LLMs</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'tutorials/04a-finetuning-RL';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo-ULM-2024.png" class="logo__image only-light" alt="Understanding LLMs - Home"/>
    <script>document.write(`<img src="../_static/logo-ULM-2024.png" class="logo__image only-dark" alt="Understanding LLMs - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Course overview: Understanding LLMs
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">01 Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../lectures/01-introduction.html">Background</a></li>
<li class="toctree-l1"><a class="reference internal" href="01-introduction.html">Sheet 1.1: Practical set-up &amp; Training data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">02 ANNs &amp; RNNs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../lectures/02-torch-ANNs-RNNs.html">PyTorch, ANNs &amp; LMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="02a-pytorch-intro.html">Sheet 2.1: PyTorch essentials</a></li>
<li class="toctree-l1"><a class="reference internal" href="02b-MLE.html">Sheet 2.2: ML-estimation</a></li>
<li class="toctree-l1"><a class="reference internal" href="02c-MLP-pytorch.html">Sheet 2.3: Non-linear regression (MLP w/ PyTorch modules)</a></li>
<li class="toctree-l1"><a class="reference internal" href="02d-char-level-RNN.html">Sheet 2.4: Character-level sequence modeling w/ RNNs</a></li>
<li class="toctree-l1"><a class="reference internal" href="02e-intro-to-hf.html">Sheet 2.5: Introduction to HuggingFace &amp; LMs</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">03 LSTMs &amp; transformers</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../lectures/03-LSTMs-Transformers.html">LSTMs &amp; Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="03a-tokenization-transformers.html">Sheet 3.1: Tokenization &amp; Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="03b-transformers-heads-training.html">Sheet 3.2: Transformer configurations &amp; Training utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lectures/04-LLMs-Prompting.html">Prompting &amp; Current LMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="03c-decoding-prompting.html">Sheet 3.3: Prompting &amp; Decoding</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../lectures/05-finetuning-RLHF.html">Fine-tuning and RLHF</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Homework</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../homework/01-language-modeling.html">Homework 1: Language models (50 points)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../homework/02-prompting.html">Homework 2: Prompting &amp; Generation with LMs (50 points)</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/CogSciPrag/Understanding-LLMs-course/main?urlpath=tree/understanding-llms/tutorials/04a-finetuning-RL.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onBinder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li><a href="https://colab.research.google.com/github/CogSciPrag/Understanding-LLMs-course/blob/main/understanding-llms/tutorials/04a-finetuning-RL.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="initThebeSBT()"
  class="btn btn-sm btn-launch-thebe dropdown-item"
  title="Launch Thebe"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="btn__text-container">Live Code</span>
</button>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/CogSciPrag/Understanding-LLMs-course" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/CogSciPrag/Understanding-LLMs-course/issues/new?title=Issue%20on%20page%20%2Ftutorials/04a-finetuning-RL.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/tutorials/04a-finetuning-RL.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>4.1 Supervised fine-tuning and RL fine-tuning</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#flavours-of-fine-tuning">Flavours of fine-tuning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#outlook-peft-in-practice">Outlook: PEFT in practice</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rl-fine-tuning">RL fine-tuning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#policy">Policy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#supervised-fine-tuning">Supervised fine-tuning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reward-modeling">Reward modeling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ppo-training">PPO training</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optional-outlook">Optional outlook</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="supervised-fine-tuning-and-rl-fine-tuning">
<h1>4.1 Supervised fine-tuning and RL fine-tuning<a class="headerlink" href="#supervised-fine-tuning-and-rl-fine-tuning" title="Permalink to this heading">#</a></h1>
<p><strong>Author:</strong> Polina Tsvilodub</p>
<ul class="simple">
<li><p>understanding instruction tuning (examples), understanding data collection for reward models, trying out reward models, RL for LMs in practice (packages), (optional: discussing different rule-based approaches to alignment)</p></li>
<li><p>recap distinction between pretraining, task fine-tuning, instruction fine-tuning, assistant fine-tuning</p></li>
<li><p>Understanding RLHF: reward model objective, ways to elicit feedback, issues with various approaches.</p></li>
<li><p>RL: understand the issue with the <strong>differential bottleneck and how RL actually helps to solve it</strong></p></li>
</ul>
<p>This sheet provides an overview of different flavours of fine-tuning of LLMs and their respective use cases. Particular focus is provided for <em>RL based</em> fine-tuning, and specifically, <em>RLHF</em> (reinforcement learning from human feedback).</p>
<p><strong>TODO:</strong> more prose about RLHF seemingly being the secret fairy dust of helpful and nice assistants.</p>
<p>The key learning goals for this sheet are:</p>
<ul class="simple">
<li><p>be able to identify the type of fine-tuning used for a particular model and explain it conceptually</p></li>
<li><p>gain a practical understanding of RLHF components, in particular:</p>
<ul>
<li><p>creation and use of a reward model</p></li>
<li><p>training steps for the policy</p></li>
<li><p>important hyperparameters of RLHF.</p></li>
</ul>
</li>
</ul>
<section id="flavours-of-fine-tuning">
<h2>Flavours of fine-tuning<a class="headerlink" href="#flavours-of-fine-tuning" title="Permalink to this heading">#</a></h2>
<p>In <a class="reference external" href="https://cogsciprag.github.io/Understanding-LLMs-course/tutorials/02e-intro-to-hf.html">sheet 2.5</a>, we already brushed over the distinction between pretrained and fine-tuned models. In particular, fine-tuning was introduced as the training LMs on <em>task-specific</em> datasets, e.g., for movie review generation or classification. For classification, LMs are usually fine-tuned with a classification head (see <a class="reference external" href="https://cogsciprag.github.io/Understanding-LLMs-course/tutorials/03b-transformers-heads-training.html">sheet 3.2</a> for a recap). However, from here on, we will focus on fine-tuning of model for <em>generative</em> tasks, i.e., simply fine-tuning LMs for next-word prediction on a specific task.</p>
<p>We have seen fine-tuning of a generative LM for question answering in homework 1. Here, the specific task the model is supposed to learn is constituted by the specific dataset and the formatting of the questions and the answers. The model was trained on examples of questions with correct answers; i.e., this is <em>supervised fine-tuning</em> where the model was shown what the desired behavior is (i.e., which next tokens it is supposed to predict).</p>
<p>For state-of-the-art LLMs, it has been identified that there is a particular kind of task that <em>general-purpose LLMs</em>, and in particular <em>assistants</em>, should be able to complete: namely, <strong>instruction-following</strong>. That is, rather than creating LMs that can only do QA on a specific dataset with a particular formatting, the community started to build <em>instruction-tuned</em> LLMs, which generate sensible outputs given user instructions ranging from “Provide ten recipes with tofu in a bullet list format” to “Summarize the following scientific paper”.
This has also been achieved with supervised fine-tuning, where the example input and output pairs in the dataset consist of example isntructions, and their respective completions.</p>
<p><strong>TODO</strong>: distinction to self-supervised training. mention catastrophic forgetting. behavioral cloning.</p>
<blockquote>
<div><p><strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 4.1.1: Supervised-finetuning</span></strong></p>
<ol class="arabic simple">
<li><p>Take a look at <a class="reference external" href="https://huggingface.co/datasets/HuggingFaceH4/instruction-dataset?row=23">this</a> dataset. For which kind of fine-tuning is it intended? What kinds of examples are there?</p></li>
<li><p>Consider the following use cases for which you want to build an LM: (a) an assistant tutor model for students for different subjects, (b) a model for answering highly specific questions about a medical knowledge base, (c) a model intended for writing abstracts of scientific papers. What kind of fine-tuning set up would you consider (i.e., what kind of fine-tuning dataset would you ideally choose)?</p></li>
<li><p>Please come up with a prompt for testing whether an LM can follow instructions. Use the following code to test the instruction-following performance of an instruction-tuned model (Phi-3) and a simple small LM (GPT-2). Feel free to play around with the decoding parameters! (<strong>WARNING</strong>: the instruction-tuned model is a large model, so it can take a moment to load on Colab. Please also be aware of it if you execute the notebook locally!)</p></li>
</ol>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># import packages</span>

<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
<span class="kn">import</span> <span class="nn">torch</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/anaconda3/envs/understanding_llms/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer_instruct</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;microsoft/Phi-3-mini-4k-instruct&quot;</span><span class="p">)</span>
<span class="n">model_instruct</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;microsoft/Phi-3-mini-4k-instruct&quot;</span><span class="p">)</span>

<span class="n">tokenizer_lm</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2-small&quot;</span><span class="p">)</span>
<span class="n">model_lm</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2-small&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">instruction_text</span> <span class="o">=</span> <span class="c1">#### YOUR TEXT HERE ####</span>
<span class="n">input_ids_instruct</span> <span class="o">=</span> <span class="n">tokenizer_instruct</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">instruction_text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">input_ids_lm</span> <span class="o">=</span> <span class="n">tokenizer_lm</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">instruction_text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">prediction_instruct</span> <span class="o">=</span> <span class="n">model_instruct</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">input_ids_instruct</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Instruction-tuned model&#39;s prediction: &quot;</span><span class="p">,</span> <span class="n">tokenizer_instruct</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">prediction_instruct</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>

<span class="n">prediction_lm</span> <span class="o">=</span> <span class="n">model_lm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">input_ids_lm</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;GPT-2&#39;s prediction: &quot;</span><span class="p">,</span> <span class="n">tokenizer_lm</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">prediction_lm</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>The distinctions above focused on distinctions in the <em>content</em> of the fine-tuning, i.e., the content of the input-output demonstrations in the datasets used for the supervised fine-tuning.</p>
<p>Additionally, the lecture introduced different <em>methods</em> of efficient supervised fine-tuning, which is especially important for large LMs that take a lot of resources to train.
The QA fine-tuning that we did in homework 1 was naive fine-tuning. That is, during the fine-tuning, all parameters were updated. However, as explained in the lecture, the more common state-of-the-art approach to fine-tuning is <em>parameter-efficient</em>, i.e., only a <em>selected</em> subset of the pretrained model parameters, or a <em>small set of new parameters</em> is updated.</p>
<p>The following code provides an simple example of vanilla selective fine-tuning where only the last transformer block and the last layer (i.e., LM head) of GPT-2 would be finetuned, and all other layers are frozen (❄️). Concretely, this means that we don’t want to compute gradients of parameters that are frozen, and we do not want to change their values. Usually parameters are (un)frozen by-layer / component.</p>
<p>Of course, the same approach can be used for (un)freezing any other subset of layers, in any other <code class="docutils literal notranslate"><span class="pre">transformers</span></code> model. For this, it is useful to know how to inspect and access different components of a pretrained model, as was briefly shown in <a class="reference external" href="https://cogsciprag.github.io/Understanding-LLMs-course/tutorials/03a-tokenization-transformers.html#transformers">sheet 3.1</a>.</p>
<blockquote>
<div><p>Optionally, you can reuse the code from the homework to fine-tune this partially frozen model on the QA task from the homework. Do your results change?</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gpt2_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>

<span class="c1"># first we inspect the model&#39;s configuration</span>
<span class="nb">print</span><span class="p">(</span><span class="n">gpt2_model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/anaconda3/envs/understanding_llms/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=768, out_features=50257, bias=False)
)
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/anaconda3/envs/understanding_llms/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># first, we can inspect the model&#39;s configuration and named parameters</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">gpt2_model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>transformer.wte.weight
transformer.wpe.weight
transformer.h.0.ln_1.weight
transformer.h.0.ln_1.bias
transformer.h.0.attn.c_attn.weight
transformer.h.0.attn.c_attn.bias
transformer.h.0.attn.c_proj.weight
transformer.h.0.attn.c_proj.bias
transformer.h.0.ln_2.weight
transformer.h.0.ln_2.bias
transformer.h.0.mlp.c_fc.weight
transformer.h.0.mlp.c_fc.bias
transformer.h.0.mlp.c_proj.weight
transformer.h.0.mlp.c_proj.bias
transformer.h.1.ln_1.weight
transformer.h.1.ln_1.bias
transformer.h.1.attn.c_attn.weight
transformer.h.1.attn.c_attn.bias
transformer.h.1.attn.c_proj.weight
transformer.h.1.attn.c_proj.bias
transformer.h.1.ln_2.weight
transformer.h.1.ln_2.bias
transformer.h.1.mlp.c_fc.weight
transformer.h.1.mlp.c_fc.bias
transformer.h.1.mlp.c_proj.weight
transformer.h.1.mlp.c_proj.bias
transformer.h.2.ln_1.weight
transformer.h.2.ln_1.bias
transformer.h.2.attn.c_attn.weight
transformer.h.2.attn.c_attn.bias
transformer.h.2.attn.c_proj.weight
transformer.h.2.attn.c_proj.bias
transformer.h.2.ln_2.weight
transformer.h.2.ln_2.bias
transformer.h.2.mlp.c_fc.weight
transformer.h.2.mlp.c_fc.bias
transformer.h.2.mlp.c_proj.weight
transformer.h.2.mlp.c_proj.bias
transformer.h.3.ln_1.weight
transformer.h.3.ln_1.bias
transformer.h.3.attn.c_attn.weight
transformer.h.3.attn.c_attn.bias
transformer.h.3.attn.c_proj.weight
transformer.h.3.attn.c_proj.bias
transformer.h.3.ln_2.weight
transformer.h.3.ln_2.bias
transformer.h.3.mlp.c_fc.weight
transformer.h.3.mlp.c_fc.bias
transformer.h.3.mlp.c_proj.weight
transformer.h.3.mlp.c_proj.bias
transformer.h.4.ln_1.weight
transformer.h.4.ln_1.bias
transformer.h.4.attn.c_attn.weight
transformer.h.4.attn.c_attn.bias
transformer.h.4.attn.c_proj.weight
transformer.h.4.attn.c_proj.bias
transformer.h.4.ln_2.weight
transformer.h.4.ln_2.bias
transformer.h.4.mlp.c_fc.weight
transformer.h.4.mlp.c_fc.bias
transformer.h.4.mlp.c_proj.weight
transformer.h.4.mlp.c_proj.bias
transformer.h.5.ln_1.weight
transformer.h.5.ln_1.bias
transformer.h.5.attn.c_attn.weight
transformer.h.5.attn.c_attn.bias
transformer.h.5.attn.c_proj.weight
transformer.h.5.attn.c_proj.bias
transformer.h.5.ln_2.weight
transformer.h.5.ln_2.bias
transformer.h.5.mlp.c_fc.weight
transformer.h.5.mlp.c_fc.bias
transformer.h.5.mlp.c_proj.weight
transformer.h.5.mlp.c_proj.bias
transformer.h.6.ln_1.weight
transformer.h.6.ln_1.bias
transformer.h.6.attn.c_attn.weight
transformer.h.6.attn.c_attn.bias
transformer.h.6.attn.c_proj.weight
transformer.h.6.attn.c_proj.bias
transformer.h.6.ln_2.weight
transformer.h.6.ln_2.bias
transformer.h.6.mlp.c_fc.weight
transformer.h.6.mlp.c_fc.bias
transformer.h.6.mlp.c_proj.weight
transformer.h.6.mlp.c_proj.bias
transformer.h.7.ln_1.weight
transformer.h.7.ln_1.bias
transformer.h.7.attn.c_attn.weight
transformer.h.7.attn.c_attn.bias
transformer.h.7.attn.c_proj.weight
transformer.h.7.attn.c_proj.bias
transformer.h.7.ln_2.weight
transformer.h.7.ln_2.bias
transformer.h.7.mlp.c_fc.weight
transformer.h.7.mlp.c_fc.bias
transformer.h.7.mlp.c_proj.weight
transformer.h.7.mlp.c_proj.bias
transformer.h.8.ln_1.weight
transformer.h.8.ln_1.bias
transformer.h.8.attn.c_attn.weight
transformer.h.8.attn.c_attn.bias
transformer.h.8.attn.c_proj.weight
transformer.h.8.attn.c_proj.bias
transformer.h.8.ln_2.weight
transformer.h.8.ln_2.bias
transformer.h.8.mlp.c_fc.weight
transformer.h.8.mlp.c_fc.bias
transformer.h.8.mlp.c_proj.weight
transformer.h.8.mlp.c_proj.bias
transformer.h.9.ln_1.weight
transformer.h.9.ln_1.bias
transformer.h.9.attn.c_attn.weight
transformer.h.9.attn.c_attn.bias
transformer.h.9.attn.c_proj.weight
transformer.h.9.attn.c_proj.bias
transformer.h.9.ln_2.weight
transformer.h.9.ln_2.bias
transformer.h.9.mlp.c_fc.weight
transformer.h.9.mlp.c_fc.bias
transformer.h.9.mlp.c_proj.weight
transformer.h.9.mlp.c_proj.bias
transformer.h.10.ln_1.weight
transformer.h.10.ln_1.bias
transformer.h.10.attn.c_attn.weight
transformer.h.10.attn.c_attn.bias
transformer.h.10.attn.c_proj.weight
transformer.h.10.attn.c_proj.bias
transformer.h.10.ln_2.weight
transformer.h.10.ln_2.bias
transformer.h.10.mlp.c_fc.weight
transformer.h.10.mlp.c_fc.bias
transformer.h.10.mlp.c_proj.weight
transformer.h.10.mlp.c_proj.bias
transformer.h.11.ln_1.weight
transformer.h.11.ln_1.bias
transformer.h.11.attn.c_attn.weight
transformer.h.11.attn.c_attn.bias
transformer.h.11.attn.c_proj.weight
transformer.h.11.attn.c_proj.bias
transformer.h.11.ln_2.weight
transformer.h.11.ln_2.bias
transformer.h.11.mlp.c_fc.weight
transformer.h.11.mlp.c_fc.bias
transformer.h.11.mlp.c_proj.weight
transformer.h.11.mlp.c_proj.bias
transformer.ln_f.weight
transformer.ln_f.bias
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># next, we define which layers NOT to freeze </span>
<span class="c1"># (of course, we can do this vice versa and define which layers to freeze)</span>

<span class="n">layers_to_unfreeze</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer.h.11&quot;</span><span class="p">,</span> <span class="s2">&quot;transformer.ln_f.weight&quot;</span><span class="p">,</span> <span class="s2">&quot;transformer.ln_f.bias&quot;</span><span class="p">]</span>

<span class="c1"># iterate over model&#39;s parameters</span>
<span class="c1"># note that, by default, in train mode, all parameters are set to require grad = True (i.e., unfrozen)</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">gpt2_model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
     <span class="c1"># check that these parameters are not in the layers_to_unfreeze list</span>
     <span class="k">if</span> <span class="nb">all</span><span class="p">([</span><span class="ow">not</span> <span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">layers_to_unfreeze</span><span class="p">]):</span> 
        <span class="c1"># if not, freeze these parameters</span>
        <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># now we check how many parameters are trainable</span>
<span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">gpt2_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;The model has </span><span class="si">{</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">params</span><span class="p">)</span><span class="si">:</span><span class="s1">,</span><span class="si">}</span><span class="s1"> trainable parameters&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The model has 7,089,408 trainable parameters
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><p><strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 4.1.2: PEFT</span></strong></p>
<ol class="arabic simple">
<li><p>Compare the number above with the number of trainable parameters in the original model. What changed? How do you expect this to affect fine-tuning results?</p></li>
<li><p>Suppose that we wanted to use rank <span class="math notranslate nohighlight">\(r=4\)</span> LoRA for fine-tuning the decoder self-attention block of GPT-2. How many parameters would the lower rank matrices A and B have (see slides 27-29 for reference)?</p></li>
</ol>
</div></blockquote>
<section id="outlook-peft-in-practice">
<h3>Outlook: PEFT in practice<a class="headerlink" href="#outlook-peft-in-practice" title="Permalink to this heading">#</a></h3>
<p>Below, more optional resources for the other types of fine-tuning introduced in the (LoRA, QLoRA) can be found.</p>
<p><strong>TODO</strong></p>
</section>
</section>
<section id="rl-fine-tuning">
<h2>RL fine-tuning<a class="headerlink" href="#rl-fine-tuning" title="Permalink to this heading">#</a></h2>
<p>Reinforcement learning is often introduced a separate type of machine learning, in addition to supervised and unsupervised learning. Reinforcement learning broadly defines the field of study and the methods for training agents to learn to take actions that (optimally) achieve a goal, based on experience in the environment. It can be seen as the computational formalization of trial-and-error learning.</p>
<p>The key difference to supervised learning is that the agent (the terms “model”, “LM” and “agent” will be used interchangeably in this section) learns which actions are useful for achieving the goal itself, rather than being shown the “ground truth” optimal actions as in supervised learning. The task of the developer is, therefore, to correctly specify the goal, and the agent will “discover” a way to achieve it.
In the formal framework which underpins RL (namely, MDPs), the goal is represented via the <em>reward function</em>. This function assigns high rewards to desired outcomes, and low rewards to undesired ones, therefore implicitly representing a goal.
It is important to note that, in general, the approach of RL allows to specify what the developer want the agent to learn to do (i.e., the goal), but <em>not necessarily how</em>, exactly. Correct specification of the goal is a far from trivial task and a lot of current research goes into understanding how to specify these goals in the field of <em>alignment</em> (more on this in future sessions).</p>
<p>Using RL for fine-tuning LLMs is one of the main methodological innovations that seems to have led to the impressive performance of SOTA LLMs. In particular, RL allows to fine-tune LLMs towards human preferences and commericial usability, because its mechanics lend itself to training a model based on a more abstract signal whether an output is good or bad (i.e., let it discover how to generate output that recevies a “good” reward!), rather than based on particular demonstrations. This is especially useful because the objectives of fine-tuning of SOTA LLMs often include aspects that are very difficult to specify via specific demonstrations, like being <em>helpful, honest, harmless</em> (<a class="reference external" href="https://arxiv.org/pdf/2204.05862">Bai et al., 2022</a>).
To this end, instead of using supervised learning, <em>human feedback</em> can be used as a reward signal to fine-tune the model.
This is why the fine-tuning technique in question  is called Reinforcement Learning from Human Feedback (RLHF).</p>
<p>Below, practical aspects of the core components of RLHF are discussed. These core components are (see <a class="reference external" href="https://cdn.openai.com/instruction-following/draft-20220126f/methods.svg">this</a> figure for an overview):</p>
<ul class="simple">
<li><p>the policy (i.e., the backbone LLM),</p></li>
<li><p>the supervised fine-tuning data (SFT) and training,</p></li>
<li><p>the reward modeling data and the resulting reward model,</p></li>
<li><p>and, finally the RL training objective (commonly, the PPO algorithm) and the dataset for fine-tuning.</p></li>
</ul>
<p>As with standard LM training, there are packages which implement some of the steps required for training for us. We will look at the <code class="docutils literal notranslate"><span class="pre">transformers</span></code> based package <a class="reference external" href="https://huggingface.co/docs/trl/en/index"><code class="docutils literal notranslate"><span class="pre">trl</span></code></a>.</p>
<section id="policy">
<h3>Policy<a class="headerlink" href="#policy" title="Permalink to this heading">#</a></h3>
<p>As the policy, a pretrained, sufficintly large LM is usually chosen. For instance, Llama offers a suite of models where both the initial LM, i.e., the <em>base</em> model and the resulting fine-tuned model is provided. For Llama-2 (<a class="reference external" href="https://arxiv.org/pdf/2307.09288">Touvron et al. (2022)</a>), the paper provides some information about the details of the training and the architecture of the models.</p>
<p>Under state-of-the-art models, usually LMs of at least &gt;=1B parameters are used as policies for further fine-tuning. The intuitive reason is that RLHF is mostly used for creating more general-purpose assistants rather than task-specific models, and therefore, the initial model should have rather large capacity and perform well on a wide range of tasks (which, as we know, tends to come with <a class="reference external" href="https://arxiv.org/pdf/2001.08361">scale</a>). In other words, the base model should be good – otherwise: “Garbage in, garbage out” (i.e., it might be very difficult if not impossible to fix a bad base model through fine-tuning).</p>
<p>Of course, it is absolutely possible to use RLHF for task-specific fine-tuning, e.g., early work fine-tuned a model for <a class="reference external" href="https://arxiv.org/pdf/2009.01325">summarization</a>.</p>
</section>
<section id="supervised-fine-tuning">
<h3>Supervised fine-tuning<a class="headerlink" href="#supervised-fine-tuning" title="Permalink to this heading">#</a></h3>
<p>This step is a “standard” supervised fine-tuning (SFT) step that is performed as we have discussed above. Some RL-tuned models are closed source, so it is unknown whether any of the PEFT techniques is used; others are open-source and might report their fine-tuning approach.</p>
<p>While the specific methodological details might vary, the conceptual point behind SFT is two-fold: (1) models are often instruction-tuned for turning into actually useful assistants and (2) (more importantly) the model is “nudged” towards outputting human-demonstrated (and therefore, human-preferred) texts for the ultimate goal of the fine-tuning (e.g., being helpful, harmless, honest). This makes subsequent RL-tuning more efficient. Intuitively, this is because the space of actions (i.e., any possible completion, given a prompt!) which the agent has to explore is quite giagantic, and the agent might make quite a lot of errors before “stumbling” upon high-reward actions. Through SFT, the agent is already biased towards the higher-reward space of actions.
Therefore, the SFT dataset usually consists of examples of target outputs written by human annotators.</p>
<blockquote>
<div><p><strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 4.1.3: Supervised fine-tuning for RL</span></strong></p>
<ol class="arabic simple">
<li><p>What would an SFT dataset look like for fine-tuning a model for summarization?</p></li>
<li><p>What apects do you think are important to keep in mind when performing SFT? (Think: what sorts of examples should human annotators see? What should they be instructed to do? Feel free to also use information from the OpenAI <a class="reference external" href="https://openai.com/index/chatgpt/">blogpost</a> for inspiration)</p></li>
<li><p>Below is an example of using the <code class="docutils literal notranslate"><span class="pre">trl</span></code> library for the <a class="reference external" href="https://huggingface.co/docs/trl/en/sft_trainer#quickstart">SFT step</a>. Please go through the code and make sure you understand it. Look at the docs for the training arguments class which is used by default by the SFTTrainer <a class="reference external" href="https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments">here</a>. By default, for how many epochs is the model trained?</p></li>
</ol>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># uncomment and run on Colab / install the package in your enironment if you haven&#39;t yet</span>
<span class="c1"># !pip install trl</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">from</span> <span class="nn">trl</span> <span class="kn">import</span> <span class="n">SFTTrainer</span>

<span class="c1"># load dataset</span>
<span class="c1"># you can inspect it to get a sense of its contents and formatting</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;CarperAI/openai_summarize_tldr&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">)</span>
<span class="c1"># load base model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;facebook/opt-350m&quot;</span><span class="p">)</span>

<span class="c1"># define a function that formats the prompts</span>
<span class="c1"># NOTE: the formatting of examples is extremely important for successful training and deployment for a given task</span>
<span class="c1"># for instance, the use of special tokens and prompt formatting should be consistent with the task, the model (if it already uses special tokens)</span>
<span class="c1"># and should be used in consistent ways in further training</span>

<span class="k">def</span> <span class="nf">formatting_prompts_func</span><span class="p">(</span><span class="n">example</span><span class="p">):</span>
    <span class="n">output_texts</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># iterate over batch</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">example</span><span class="p">[</span><span class="s1">&#39;prompt&#39;</span><span class="p">])):</span>
        <span class="c1"># retrieve both inputs and target labels</span>
        <span class="n">text</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">example</span><span class="p">[</span><span class="s1">&#39;prompt&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="si">}{</span><span class="n">example</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="n">output_texts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output_texts</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">SFTTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
    <span class="n">formatting_func</span><span class="o">=</span><span class="n">formatting_prompts_func</span><span class="p">,</span>
    <span class="n">max_seq_length</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
    <span class="n">dataset_batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

<span class="c1"># If one wants to pass custom training arguments, an example of doing so is here: https://github.com/huggingface/trl/blob/main/examples/scripts/sft.py</span>

<span class="c1"># NOTE: if you experience errors running this, setting the version of accelerate to pip install accelerate==0.27.2 might help</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">TypeError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">10</span><span class="p">],</span> <span class="n">line</span> <span class="mi">24</span>
<span class="g g-Whitespace">     </span><span class="mi">21</span>         <span class="n">output_texts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">22</span>     <span class="k">return</span> <span class="n">output_texts</span>
<span class="ne">---&gt; </span><span class="mi">24</span> <span class="n">trainer</span> <span class="o">=</span> <span class="n">SFTTrainer</span><span class="p">(</span>
<span class="g g-Whitespace">     </span><span class="mi">25</span>     <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">26</span>     <span class="n">train_dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">27</span>     <span class="n">formatting_func</span><span class="o">=</span><span class="n">formatting_prompts_func</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">28</span>     <span class="n">max_seq_length</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">29</span>     <span class="n">dataset_batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">30</span> <span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">31</span> <span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="g g-Whitespace">     </span><span class="mi">33</span> <span class="c1"># If one wants to pass custom training arguments, an example of doing so is here: https://github.com/huggingface/trl/blob/main/examples/scripts/sft.py</span>
<span class="g g-Whitespace">     </span><span class="mi">34</span> 
<span class="g g-Whitespace">     </span><span class="mi">35</span> <span class="c1"># NOTE: if you experience errors running this, setting the version of accelerate to pip install accelerate==0.27.2 might help</span>

<span class="nn">File /opt/anaconda3/envs/understanding_llms/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:323,</span> in <span class="ni">SFTTrainer.__init__</span><span class="nt">(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics, peft_config, dataset_text_field, packing, formatting_func, max_seq_length, infinite, num_of_sequences, chars_per_token, dataset_num_proc, dataset_batch_size, neftune_noise_alpha, model_init_kwargs, dataset_kwargs, eval_packing)</span>
<span class="g g-Whitespace">    </span><span class="mi">317</span> <span class="k">if</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">padding_side</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">padding_side</span> <span class="o">!=</span> <span class="s2">&quot;right&quot;</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">318</span>     <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">319</span>         <span class="s2">&quot;You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to &quot;</span>
<span class="g g-Whitespace">    </span><span class="mi">320</span>         <span class="s2">&quot;overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = &#39;right&#39;` to your code.&quot;</span>
<span class="g g-Whitespace">    </span><span class="mi">321</span>     <span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">323</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">324</span>     <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">325</span>     <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">326</span>     <span class="n">data_collator</span><span class="o">=</span><span class="n">data_collator</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">327</span>     <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">328</span>     <span class="n">eval_dataset</span><span class="o">=</span><span class="n">eval_dataset</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">329</span>     <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">330</span>     <span class="n">model_init</span><span class="o">=</span><span class="n">model_init</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">331</span>     <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">332</span>     <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">333</span>     <span class="n">optimizers</span><span class="o">=</span><span class="n">optimizers</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">334</span>     <span class="n">preprocess_logits_for_metrics</span><span class="o">=</span><span class="n">preprocess_logits_for_metrics</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">335</span> <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">337</span> <span class="c1"># Add tags for models that have been loaded with the correct transformers version</span>
<span class="g g-Whitespace">    </span><span class="mi">338</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;add_model_tags&quot;</span><span class="p">):</span>

<span class="nn">File /opt/anaconda3/envs/understanding_llms/lib/python3.10/site-packages/transformers/trainer.py:373,</span> in <span class="ni">Trainer.__init__</span><span class="nt">(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)</span>
<span class="g g-Whitespace">    </span><span class="mi">370</span> <span class="bp">self</span><span class="o">.</span><span class="n">deepspeed</span> <span class="o">=</span> <span class="kc">None</span>
<span class="g g-Whitespace">    </span><span class="mi">371</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_in_train</span> <span class="o">=</span> <span class="kc">False</span>
<span class="ne">--&gt; </span><span class="mi">373</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_accelerator_and_postprocess</span><span class="p">()</span>
<span class="g g-Whitespace">    </span><span class="mi">375</span> <span class="c1"># memory metrics - must set up as early as possible</span>
<span class="g g-Whitespace">    </span><span class="mi">376</span> <span class="bp">self</span><span class="o">.</span><span class="n">_memory_tracker</span> <span class="o">=</span> <span class="n">TrainerMemoryTracker</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">skip_memory_metrics</span><span class="p">)</span>

<span class="nn">File /opt/anaconda3/envs/understanding_llms/lib/python3.10/site-packages/transformers/trainer.py:4252,</span> in <span class="ni">Trainer.create_accelerator_and_postprocess</span><span class="nt">(self)</span>
<span class="g g-Whitespace">   </span><span class="mi">4249</span> <span class="n">gradient_accumulation_plugin</span> <span class="o">=</span> <span class="n">GradientAccumulationPlugin</span><span class="p">(</span><span class="o">**</span><span class="n">grad_acc_kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">4251</span> <span class="c1"># create accelerator object</span>
<span class="ne">-&gt; </span><span class="mi">4252</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span> <span class="o">=</span> <span class="n">Accelerator</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">4253</span>     <span class="n">deepspeed_plugin</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">deepspeed_plugin</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">4254</span>     <span class="n">gradient_accumulation_plugin</span><span class="o">=</span><span class="n">gradient_accumulation_plugin</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">4255</span>     <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">accelerator_config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(),</span>
<span class="g g-Whitespace">   </span><span class="mi">4256</span> <span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">4257</span> <span class="c1"># some Trainer classes need to use `gather` instead of `gather_for_metrics`, thus we store a flag</span>
<span class="g g-Whitespace">   </span><span class="mi">4258</span> <span class="bp">self</span><span class="o">.</span><span class="n">gather_function</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">gather_for_metrics</span>

<span class="ne">TypeError</span>: Accelerator.__init__() got an unexpected keyword argument &#39;use_seedable_sampler&#39;
</pre></div>
</div>
</div>
</div>
</section>
<section id="reward-modeling">
<h3>Reward modeling<a class="headerlink" href="#reward-modeling" title="Permalink to this heading">#</a></h3>
<p>A core component of RL fine-tuning is the reward model. There are various ways of obtaining a reward model. The perhaps most intuitive one, as discussed in the lecture, is obtaining human feedback as the reward signal. Since having humans give online feedback to an LM for thousands of samples is costly and cumbersome, instead, a reward model is trained on human preferences. For training such a reward model, human annotations are collected. As discussed in the lecture, these annotations can take different forms.</p>
<p>Below, we will look at an example based on simple binary comparisons, where human annotators were shown two outputs of the LM sampled for the same input <span class="math notranslate nohighlight">\(x\)</span> and had to indicate which of them they preferred (e.g., output <span class="math notranslate nohighlight">\(y_1\)</span> over <span class="math notranslate nohighlight">\(y_2\)</span>).
Specifically, the reward model is usually initialized from a pretrained LLM (maybe even the same one as the base LM of the policy), and fine-tuned with a specific <em>head</em> to output numerical scores. It is usually trained to maximize the difference in predicted scores scores <span class="math notranslate nohighlight">\(r\)</span> between the preferred and the rejected answer in a pair. Formally, the model is trained with the following loss:</p>
<div class="math notranslate nohighlight">
\[L(\theta) = -\frac{1}{N}\mathbb{E}_{(x, y_1, y_2) \sim D} [log \; (\sigma (r_{\theta} (x, y_1) - r_{\theta}(x, y_2)))] \]</div>
<p>This form of human annotations has proven especially useful for eliciting human intuitions for such difficult-to-capture concepts like “helpfulness”. It is much easier for humans to decide which of two options they prefer than, e.g., to consistently assign scalar 1-10 scores to outputs.</p>
<blockquote>
<div><p><strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 4.1.4: Reward modeling</span></strong></p>
<ol class="arabic simple">
<li><p>Consider <span class="xref myst">this</span> well-known dataset from Anthropic for training helpful and harmless assistants. Pick a specific sample. For this sample, which text corresponds to <span class="math notranslate nohighlight">\(x, y_1, y_2\)</span>?</p></li>
<li><p>An example of a model trained with the approach above can be found <a class="reference external" href="https://huggingface.co/reciprocate/openllama-13b_rm_oasst-hh">here</a>. Feel free to explore the repository, if you want. We will not test this model since it is quite large.</p></li>
</ol>
</div></blockquote>
<p>However, there are also alternative ways of providing rewards. For instance, some work has used other models to provide feedback, an approach that is called <a class="reference external" href="https://arxiv.org/pdf/2212.08073">RLAIF</a> (RL from AI feedback).
Alternatively, more task-specific reward models can be constructed. For instance, if one wants to use RL-tuning for training a summarization model, the score for evaluating summaries (ROUGE) can be used as the numerical reward. If one wants to train a model for generating positive-sentiment texts only, one can train a reward model on labelled sentiment classification data like, e.g., the IMDB dataset.</p>
<blockquote>
<div><p><strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 4.1.5: Task-specific reward modeling</span></strong></p>
<ol class="arabic simple">
<li><p>The code below provides an example loading a trained reward model. This model was trained on movie reviews, in particular to assign high scores to positive reviews and low scores to negative reviews, as described above. Please look at the code and make sure to understand it. Test it on a few of your own intuitive examples; do the scores (ordinally) correspond to your intuition?</p></li>
</ol>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">AutoTokenizer</span><span class="p">,</span> 
    <span class="n">AutoModelForSequenceClassification</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">import</span> <span class="nn">torch</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load reward model and its tokenizer</span>
<span class="n">reward_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;lvwerra/distilbert-imdb&quot;</span><span class="p">)</span>
<span class="n">reward_model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;lvwerra/distilbert-imdb&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run an example from the IMDB train split to see how the reward model works</span>
<span class="n">positive_sentence</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span> <span class="c1">#### YOUR EXAMPLE HERE ####</span>
<span class="n">negative_sentence</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span> <span class="c1">#### YOUR EXAMPLE HERE ####</span>

<span class="n">input_pos</span> <span class="o">=</span> <span class="n">reward_tokenizer</span><span class="p">(</span><span class="n">positive_sentence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)</span>
<span class="n">input_neg</span> <span class="o">=</span> <span class="n">reward_tokenizer</span><span class="p">(</span><span class="n">negative_sentence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)</span>

<span class="n">reward_pos</span> <span class="o">=</span> <span class="n">reward_model</span><span class="p">(</span><span class="o">**</span><span class="n">input_pos</span><span class="p">)</span>
<span class="n">reward_neg</span> <span class="o">=</span> <span class="n">reward_model</span><span class="p">(</span><span class="o">**</span><span class="n">input_neg</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Reward for positive sentence: &quot;</span><span class="p">,</span> <span class="n">reward_pos</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Reward for negative sentence: &quot;</span><span class="p">,</span> <span class="n">reward_neg</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="ppo-training">
<h3>PPO training<a class="headerlink" href="#ppo-training" title="Permalink to this heading">#</a></h3>
<p>Once the SFT model and the reward model are available, the final step is to fine-tune the SFT model with RL. For this, a dataset to fine-tune on is needed again. Depedning on the model, sometimes the same dataset as for SFT used, sometimes a similar dataset with other inputs is used. Note that now the dataset doesn’t need any labels, but only the inputs (i.e., initial “states”) based on which the model will generate predictions (i.e., actions) which, in turn, will be assigned rewards (by the reward model which represents the environment).</p>
<p>There are different algorithms for training the policy. One currently common choice is the Proximal Policy Optimization (<a class="reference external" href="https://arxiv.org/pdf/1707.06347">PPO</a>). Its components were developed in order to stabilize policy updates and speed up convergence. However, there is a core shared idea between PPO and other algorithms in the category of <em>policy-gradient</em> methods.
Specifically, the theorem behind this class of methods shows that the policy can be trained with a loss based on “trials and errors” (i.e., based on sampling; this objective has been shown to update the policy in such a way that, by following it, the agent is expected to receive higher rewards). Specifically, for a training step, we can sample actions, retrieve their log probability under the current policy, get rewards for these actions, and calculate weight updates based on the product of log probability and the reward.</p>
<blockquote>
<div><p><strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 4.1.6: RL training</span></strong></p>
<ol class="arabic simple">
<li><p>Suppose you train an LM for summarization and have a suitable reward model and dataset of input texts. Consider the last sentence of the explanation above. Please provide a specific details of such a training step with this summarization example (in words).</p></li>
<li><p>An example of using the <code class="docutils literal notranslate"><span class="pre">trl</span></code> library for training a model to generate positive model reviews (using the reward model above!) with PPO can be found <a class="reference external" href="https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb">here</a>. Please look at the code and try to understand all of it! Please ask questions or research if anything is unclear, this approach might be relevant for your own future exercise ;)</p></li>
</ol>
</div></blockquote>
</section>
<section id="optional-outlook">
<h3>Optional outlook<a class="headerlink" href="#optional-outlook" title="Permalink to this heading">#</a></h3>
<p>RL fine-tuning is an active area of research, so there are many developments and new methods. Below are some optional resources if you want to know more.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/pdf/2305.18290">Rafailov et al. (2023) Direct Preference Optimization: Your Language Model is Secretly a Reward Model</a></p></li>
</ul>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "CogSciPrag/Understanding-LLMs-course",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./tutorials"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#flavours-of-fine-tuning">Flavours of fine-tuning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#outlook-peft-in-practice">Outlook: PEFT in practice</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rl-fine-tuning">RL fine-tuning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#policy">Policy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#supervised-fine-tuning">Supervised fine-tuning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reward-modeling">Reward modeling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ppo-training">PPO training</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optional-outlook">Optional outlook</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Michael Franke, Carsten Eickhoff, Polina Tsvilodub
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>