

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Sheet 4.1 Supervised fine-tuning and RL fine-tuning &#8212; Understanding LLMs</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'tutorials/04a-finetuning-RL';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="LLM systems &amp; agents" href="../lectures/06-agents.html" />
    <link rel="prev" title="Fine-tuning and RLHF" href="../lectures/05-finetuning-RLHF.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo-ULM-2024.png" class="logo__image only-light" alt="Understanding LLMs - Home"/>
    <script>document.write(`<img src="../_static/logo-ULM-2024.png" class="logo__image only-dark" alt="Understanding LLMs - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Course overview: Understanding LLMs
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">01 Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../lectures/01-introduction.html">Background</a></li>
<li class="toctree-l1"><a class="reference internal" href="01-introduction.html">Sheet 1.1: Practical set-up &amp; Training data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">02 ANNs &amp; RNNs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../lectures/02-torch-ANNs-RNNs.html">PyTorch, ANNs &amp; LMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="02a-pytorch-intro.html">Sheet 2.1: PyTorch essentials</a></li>
<li class="toctree-l1"><a class="reference internal" href="02b-MLE.html">Sheet 2.2: ML-estimation</a></li>
<li class="toctree-l1"><a class="reference internal" href="02c-MLP-pytorch.html">Sheet 2.3: Non-linear regression (MLP w/ PyTorch modules)</a></li>
<li class="toctree-l1"><a class="reference internal" href="02d-char-level-RNN.html">Sheet 2.4: Character-level sequence modeling w/ RNNs</a></li>
<li class="toctree-l1"><a class="reference internal" href="02e-intro-to-hf.html">Sheet 2.5: Introduction to HuggingFace &amp; LMs</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">03 LSTMs &amp; transformers</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../lectures/03-LSTMs-Transformers.html">LSTMs &amp; Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="03a-tokenization-transformers.html">Sheet 3.1: Tokenization &amp; Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="03b-transformers-heads-training.html">Sheet 3.2: Transformer configurations &amp; Training utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lectures/04-LLMs-Prompting.html">Prompting &amp; Current LMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="03c-decoding-prompting.html">Sheet 3.3: Prompting &amp; Decoding</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">04 Fine-tuning &amp; RLHF</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../lectures/05-finetuning-RLHF.html">Fine-tuning and RLHF</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Sheet 4.1 Supervised fine-tuning and RL fine-tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lectures/06-agents.html">LLM systems &amp; agents</a></li>
<li class="toctree-l1"><a class="reference internal" href="05a-agents.html">Sheet 5.1 LLM agents</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lectures/07-attribution.html">Attribution methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="06a-attribution.html">Sheet 6.1 LLM probing &amp; attribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lectures/08-evaluation.html">Evaluation &amp; behavioral assessment</a></li>
<li class="toctree-l1"><a class="reference internal" href="07a-behavioral-assessment.html">Sheet 7.1: Behavioral assessment &amp; Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="07b-biases-assessment.html">Sheet 7.2: Advanced evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="08a-mechanistic-interpretability.html">Sheet 8.1: Mechanistic interpretability</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Homework</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../homework/01-language-modeling.html">Homework 1: Language models (50 points)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../homework/02-prompting.html">Homework 2: Prompting &amp; Generation with LMs (50 points)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../homework/03-agents-RL.html">Homework 3: LLM agents &amp; RL fine-tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../homework/04-evaluation.html">Homework 4: LLM evaluation</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/CogSciPrag/Understanding-LLMs-course/main?urlpath=tree/understanding-llms/tutorials/04a-finetuning-RL.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onBinder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li><a href="https://colab.research.google.com/github/CogSciPrag/Understanding-LLMs-course/blob/main/understanding-llms/tutorials/04a-finetuning-RL.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="initThebeSBT()"
  class="btn btn-sm btn-launch-thebe dropdown-item"
  title="Launch Thebe"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="btn__text-container">Live Code</span>
</button>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/CogSciPrag/Understanding-LLMs-course" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/CogSciPrag/Understanding-LLMs-course/issues/new?title=Issue%20on%20page%20%2Ftutorials/04a-finetuning-RL.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/tutorials/04a-finetuning-RL.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Sheet 4.1 Supervised fine-tuning and RL fine-tuning</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#flavours-of-fine-tuning">Flavours of fine-tuning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#outlook-peft-in-practice">Outlook: PEFT in practice</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rl-fine-tuning">RL fine-tuning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#policy">Policy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#supervised-fine-tuning">Supervised fine-tuning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reward-modeling">Reward modeling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ppo-training">PPO training</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optional-outlook">Optional outlook</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="sheet-4-1-supervised-fine-tuning-and-rl-fine-tuning">
<h1>Sheet 4.1 Supervised fine-tuning and RL fine-tuning<a class="headerlink" href="#sheet-4-1-supervised-fine-tuning-and-rl-fine-tuning" title="Permalink to this heading">#</a></h1>
<p><strong>Author:</strong> Polina Tsvilodub</p>
<p>This sheet provides an overview of different flavours of fine-tuning of LLMs and their respective use cases. Particular focus is provided for <em>RL based</em> fine-tuning, and specifically, <em>RLHF</em> (reinforcement learning from human feedback).</p>
<p>The key learning goals for this sheet are:</p>
<ul class="simple">
<li><p>be able to identify the type of fine-tuning used for a particular model and explain it conceptually</p></li>
<li><p>gain a practical understanding of RLHF components, in particular:</p>
<ul>
<li><p>creation and use of a reward model</p></li>
<li><p>training steps for the policy.</p></li>
</ul>
</li>
</ul>
<section id="flavours-of-fine-tuning">
<h2>Flavours of fine-tuning<a class="headerlink" href="#flavours-of-fine-tuning" title="Permalink to this heading">#</a></h2>
<p>In <a class="reference external" href="https://cogsciprag.github.io/Understanding-LLMs-course/tutorials/02e-intro-to-hf.html">sheet 2.5</a>, we already brushed over the distinction between pretrained and fine-tuned models. In particular, fine-tuning was introduced as the training LMs on <em>task-specific</em> datasets, e.g., for movie review generation or classification. For classification, LMs are usually fine-tuned with a classification head (see <a class="reference external" href="https://cogsciprag.github.io/Understanding-LLMs-course/tutorials/03b-transformers-heads-training.html">sheet 3.2</a> for a recap). However, from here on, we will focus on fine-tuning of model for <em>generative</em> tasks, i.e., simply fine-tuning LMs for next-word prediction on a specific task.</p>
<p>We have seen fine-tuning of a generative LM for question answering in homework 1. Here, the specific task the model is supposed to learn is constituted by the specific dataset and the formatting of the questions and the answers. The model was trained on examples of questions with correct answers; i.e., this is <em>supervised fine-tuning</em> where the model was shown what the desired behavior is (i.e., which next tokens it is supposed to predict).</p>
<p>For state-of-the-art LLMs, it has been identified that there is a particular kind of task that <em>general-purpose LLMs</em>, and in particular <em>assistants</em>, should be able to complete: namely, <strong>instruction-following</strong>. That is, rather than creating LMs that can only do QA on a specific dataset with a particular formatting, the community started to build <em>instruction-tuned</em> LLMs, which generate sensible outputs given user instructions ranging from “Provide ten recipes with tofu in a bullet list format” to “Summarize the following scientific paper”.
This has also been achieved with supervised fine-tuning, where the example input and output pairs in the dataset consist of example instructions, and their respective completions.</p>
<blockquote>
<div><p><strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 4.1.1: Supervised-finetuning</span></strong></p>
<ol class="arabic simple">
<li><p>Take a look at <a class="reference external" href="https://huggingface.co/datasets/HuggingFaceH4/instruction-dataset?row=23">this</a> dataset. For which kind of fine-tuning is it intended? What kinds of examples are there?</p></li>
<li><p>Consider the following use cases for which you want to build an LM: (a) an assistant tutor model for students for different subjects, (b) a model for answering highly specific questions about a medical knowledge base, (c) a model intended for writing abstracts of scientific papers. What kind of fine-tuning set up would you consider (i.e., what kind of fine-tuning dataset would you ideally choose)?</p></li>
<li><p>Please come up with a prompt for testing whether an LM can follow instructions. Use the following code to test the instruction-following performance of an instruction-tuned model (Phi-3) and a simple small LM (GPT-2). Feel free to play around with the decoding parameters! (<strong>WARNING</strong>: the instruction-tuned model is a large model, so it can take a moment to load on Colab. Please also be aware of it if you execute the notebook locally!)</p></li>
</ol>
</div></blockquote>
<p>Click below to see the solution</p>
<div class="toggle docutils container">
<blockquote>
<div><ol class="arabic simple">
<li><p>The dataset contains generative fine-tuning examples on a broad range of tasks, from calculations to writing stories to cooking recipes.</p></li>
<li><p>The assistant tutor model has to be finetuned such that it can have natural-sounding conversations with students on a range of subjects, i.e., we need dialogues from a teaching context as data for training. For the medical knowledge base, it is probably necessary to implement an integrated retrieval system that allows the language model to access the database, rather than just generating everything from next-token predictions. For writing abstracts of scientific papers, we have to optimise the model on summarisation, possibly already using scientific papers in the training.</p></li>
<li><p>You can use something like “Please write a sentence where every word begins with “a”. Answer: “. In this case, the output was “Alice admired” for the instruction-tuned model (which fulfills the instruction), but “”A” is a noun” for GPT-2, which does not.</p></li>
</ol>
</div></blockquote>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># import packages</span>

<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
<span class="kn">import</span> <span class="nn">torch</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer_instruct</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;microsoft/Phi-3-mini-4k-instruct&quot;</span><span class="p">)</span>
<span class="n">model_instruct</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;microsoft/Phi-3-mini-4k-instruct&quot;</span><span class="p">,</span>
    <span class="n">load_in_4bit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
    <span class="n">bnb_4bit_use_double_quant</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">bnb_4bit_quant_type</span><span class="o">=</span><span class="s2">&quot;nf4&quot;</span><span class="p">,</span>
    <span class="n">bnb_4bit_compute_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">tokenizer_lm</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2-small&quot;</span><span class="p">)</span>
<span class="n">model_lm</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2-small&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">instruction_text</span> <span class="o">=</span> <span class="c1">#### YOUR TEXT HERE ####</span>
<span class="n">input_ids_instruct</span> <span class="o">=</span> <span class="n">tokenizer_instruct</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">instruction_text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">input_ids_lm</span> <span class="o">=</span> <span class="n">tokenizer_lm</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">instruction_text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">prediction_instruct</span> <span class="o">=</span> <span class="n">model_instruct</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">input_ids_instruct</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Instruction-tuned model&#39;s prediction: &quot;</span><span class="p">,</span> <span class="n">tokenizer_instruct</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">prediction_instruct</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>

<span class="n">prediction_lm</span> <span class="o">=</span> <span class="n">model_lm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">input_ids_lm</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;GPT-2&#39;s prediction: &quot;</span><span class="p">,</span> <span class="n">tokenizer_lm</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">prediction_lm</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>The distinctions above focused on distinctions in the <em>content</em> of the fine-tuning, i.e., the content of the input-output demonstrations in the datasets used for the supervised fine-tuning.</p>
<p>Additionally, the lecture introduced different <em>methods</em> of efficient supervised fine-tuning, which is especially important for large LMs that take a lot of resources to train.
The QA fine-tuning that we did in homework 1 was naive fine-tuning. That is, during the fine-tuning, all parameters were updated. However, as explained in the lecture, the more common state-of-the-art approach to fine-tuning is <em>parameter-efficient</em>, i.e., only a <em>selected</em> subset of the pretrained model parameters, or a <em>small set of new parameters</em> is updated.</p>
<p>The following code provides an simple example of vanilla selective fine-tuning where only the last transformer block and the last layer (i.e., LM head) of GPT-2 would be finetuned, and all other layers are frozen (❄️). Concretely, this means that we don’t want to compute gradients of parameters that are frozen, and we do not want to change their values. Usually parameters are (un)frozen by-layer / component.</p>
<p>Of course, the same approach can be used for (un)freezing any other subset of layers, in any other <code class="docutils literal notranslate"><span class="pre">transformers</span></code> model. For this, it is useful to know how to inspect and access different components of a pretrained model, as was briefly shown in <a class="reference external" href="https://cogsciprag.github.io/Understanding-LLMs-course/tutorials/03a-tokenization-transformers.html#transformers">sheet 3.1</a>.</p>
<blockquote>
<div><p>Optionally, you can reuse the code from the homework to fine-tune this partially frozen model on the QA task from the homework. Do your results change?</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gpt2_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>

<span class="c1"># first we inspect the model&#39;s configuration</span>
<span class="nb">print</span><span class="p">(</span><span class="n">gpt2_model</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># first, we can inspect the model&#39;s configuration and named parameters</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">gpt2_model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># next, we define which layers NOT to freeze </span>
<span class="c1"># (of course, we can do this vice versa and define which layers to freeze)</span>

<span class="n">layers_to_unfreeze</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer.h.11&quot;</span><span class="p">,</span> <span class="s2">&quot;transformer.ln_f.weight&quot;</span><span class="p">,</span> <span class="s2">&quot;transformer.ln_f.bias&quot;</span><span class="p">]</span>

<span class="c1"># iterate over model&#39;s parameters</span>
<span class="c1"># note that, by default, in train mode, all parameters are set to require grad = True (i.e., unfrozen)</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">gpt2_model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
     <span class="c1"># check that these parameters are not in the layers_to_unfreeze list</span>
     <span class="k">if</span> <span class="nb">all</span><span class="p">([</span><span class="ow">not</span> <span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">layers_to_unfreeze</span><span class="p">]):</span> 
        <span class="c1"># if not, freeze these parameters</span>
        <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># now we check how many parameters are trainable</span>
<span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">gpt2_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;The model has </span><span class="si">{</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">params</span><span class="p">)</span><span class="si">:</span><span class="s1">,</span><span class="si">}</span><span class="s1"> trainable parameters&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><p><strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 4.1.2: PEFT</span></strong></p>
<ol class="arabic simple">
<li><p>Compare the number above with the number of trainable parameters in the original model. What changed? How do you expect this to affect fine-tuning results?</p></li>
<li><p>Suppose that we wanted to use rank <span class="math notranslate nohighlight">\(r=4\)</span> LoRA for fine-tuning the decoder self-attention block of GPT-2. How many parameters would the lower rank matrices A and B have (see slides 27-29 for reference)?</p></li>
</ol>
</div></blockquote>
<p>Click below to see the solution</p>
<div class="toggle docutils container">
<blockquote>
<div><ol class="arabic simple">
<li><p>Number of trainable parameters before freezing: 124 million, after freezing: 7 million. Although intuitively, one might think that the performance will be worse when fewer parameters are fine-tuned, PEFT has turned out to be efficient and even have further advantages over fine-tuning all parameters, e.g. with regard to catastrophic forgetting.</p></li>
<li><p>The attention weight matrix has a size of 768×2304, so the lower rank matrices have the dimensions 768×4 and 4×2304.</p></li>
</ol>
</div></blockquote>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
<section id="outlook-peft-in-practice">
<h3>Outlook: PEFT in practice<a class="headerlink" href="#outlook-peft-in-practice" title="Permalink to this heading">#</a></h3>
<p>Below, more optional resources for the other types of fine-tuning introduced in the (LoRA, QLoRA) can be found.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/blog/4bit-transformers-bitsandbytes">Blog post on QLoRA with <code class="docutils literal notranslate"><span class="pre">transformers</span></code></a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/blog/peft">Overview blogpost on PEFT with <code class="docutils literal notranslate"><span class="pre">transformers</span></code></a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/docs/peft/en/index">PEFT package based on transformers</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/2305.14314">QLoRA paper</a></p></li>
</ul>
</section>
</section>
<section id="rl-fine-tuning">
<h2>RL fine-tuning<a class="headerlink" href="#rl-fine-tuning" title="Permalink to this heading">#</a></h2>
<p>Reinforcement learning is often introduced a separate type of machine learning, in addition to supervised and unsupervised learning. Reinforcement learning broadly defines the field of study and the methods for training agents to learn to take actions that (optimally) achieve a goal, based on experience in the environment. It can be seen as the computational formalization of trial-and-error learning.</p>
<p>The key difference to supervised learning is that the agent (the terms “model”, “LM” and “agent” will be used interchangeably in this section) learns which actions are useful for achieving the goal itself, rather than being shown the “ground truth” optimal actions as in supervised learning. The task of the developer is, therefore, to correctly specify the goal, and the agent will “discover” a way to achieve it.
In the formal framework which underpins RL (namely, MDPs), the goal is represented via the <em>reward function</em>. This function assigns high rewards to desired outcomes, and low rewards to undesired ones, therefore implicitly representing a goal.
It is important to note that, in general, the approach of RL allows to specify what the developer want the agent to learn to do (i.e., the goal), but <em>not necessarily how</em>, exactly. Correct specification of the goal is a far from trivial task and a lot of current research goes into understanding how to specify these goals in the field of <em>alignment</em> (more on this in future sessions).</p>
<p>Using RL for fine-tuning LLMs is one of the main methodological innovations that seems to have led to the impressive performance of SOTA LLMs. In particular, RL allows to fine-tune LLMs towards human preferences and commericial usability, because its mechanics lend itself to training a model based on a more abstract signal whether an output is good or bad (i.e., let it discover how to generate output that recevies a “good” reward!), rather than based on particular demonstrations. This is especially useful because the objectives of fine-tuning of SOTA LLMs often include aspects that are very difficult to specify via specific demonstrations, like being <em>helpful, honest, harmless</em> (<a class="reference external" href="https://arxiv.org/pdf/2204.05862">Bai et al., 2022</a>).
To this end, instead of using supervised learning, <em>human feedback</em> can be used as a reward signal to fine-tune the model.
This is why the fine-tuning technique in question  is called Reinforcement Learning from Human Feedback (RLHF).</p>
<p>Below, practical aspects of the core components of RLHF are discussed. These core components are (see <a class="reference external" href="https://cdn.openai.com/instruction-following/draft-20220126f/methods.svg">this</a> figure for an overview):</p>
<ul class="simple">
<li><p>the policy (i.e., the backbone LLM),</p></li>
<li><p>the supervised fine-tuning data (SFT) and training,</p></li>
<li><p>the reward modeling data and the resulting reward model,</p></li>
<li><p>and, finally the RL training objective (commonly, the PPO algorithm) and the dataset for fine-tuning.</p></li>
</ul>
<p>As with standard LM training, there are packages which implement some of the steps required for training for us. We will look at the <code class="docutils literal notranslate"><span class="pre">transformers</span></code> based package <a class="reference external" href="https://huggingface.co/docs/trl/en/index"><code class="docutils literal notranslate"><span class="pre">trl</span></code></a>.</p>
<section id="policy">
<h3>Policy<a class="headerlink" href="#policy" title="Permalink to this heading">#</a></h3>
<p>As the policy, a pretrained, sufficintly large LM is usually chosen. For instance, Llama offers a suite of models where both the initial LM, i.e., the <em>base</em> model and the resulting fine-tuned model is provided. For Llama-2 (<a class="reference external" href="https://arxiv.org/pdf/2307.09288">Touvron et al. (2022)</a>), the paper provides some information about the details of the training and the architecture of the models.</p>
<p>Under state-of-the-art models, usually LMs of at least &gt;=1B parameters are used as policies for further fine-tuning. The intuitive reason is that RLHF is mostly used for creating more general-purpose assistants rather than task-specific models, and therefore, the initial model should have rather large capacity and perform well on a wide range of tasks (which, as we know, tends to come with <a class="reference external" href="https://arxiv.org/pdf/2001.08361">scale</a>). In other words, the base model should be good – otherwise: “Garbage in, garbage out” (i.e., it might be very difficult if not impossible to fix a bad base model through fine-tuning).</p>
<p>Of course, it is absolutely possible to use RLHF for task-specific fine-tuning, e.g., early work fine-tuned a model for <a class="reference external" href="https://arxiv.org/pdf/2009.01325">summarization</a>.</p>
</section>
<section id="supervised-fine-tuning">
<h3>Supervised fine-tuning<a class="headerlink" href="#supervised-fine-tuning" title="Permalink to this heading">#</a></h3>
<p>This step is a “standard” supervised fine-tuning (SFT) step that is performed as we have discussed above. Some RL-tuned models are closed source, so it is unknown whether any of the PEFT techniques is used; others are open-source and might report their fine-tuning approach.</p>
<p>While the specific methodological details might vary, the conceptual point behind SFT is two-fold: (1) models are often instruction-tuned for turning into actually useful assistants and (2) (more importantly) the model is “nudged” towards outputting human-demonstrated (and therefore, human-preferred) texts for the ultimate goal of the fine-tuning (e.g., being helpful, harmless, honest). This makes subsequent RL-tuning more efficient. Intuitively, this is because the space of actions (i.e., any possible completion, given a prompt!) which the agent has to explore is quite giagantic, and the agent might make quite a lot of errors before “stumbling” upon high-reward actions. Through SFT, the agent is already biased towards the higher-reward space of actions.
Therefore, the SFT dataset usually consists of examples of target outputs written by human annotators. This step is also often called <em>behavioral cloning</em>.</p>
<blockquote>
<div><p><strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 4.1.3: Supervised fine-tuning for RL</span></strong></p>
<ol class="arabic simple">
<li><p>What would an SFT dataset look like for fine-tuning a model for summarization?</p></li>
<li><p>What apects do you think are important to keep in mind when performing SFT? (Think: what sorts of examples should human annotators see? What should they be instructed to do? Feel free to also use information from the OpenAI <a class="reference external" href="https://openai.com/index/chatgpt/">blogpost</a> for inspiration)</p></li>
<li><p>Below is an example of using the <code class="docutils literal notranslate"><span class="pre">trl</span></code> library for the <a class="reference external" href="https://huggingface.co/docs/trl/en/sft_trainer#quickstart">SFT step</a>. Please go through the code and make sure you understand it. Look at the docs for the training arguments class which is used by default by the SFTTrainer <a class="reference external" href="https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments">here</a>. By default, for how many epochs is the model trained?</p></li>
</ol>
</div></blockquote>
<p>Click below to see the solution</p>
<div class="toggle docutils container">
<blockquote>
<div><ol class="arabic simple">
<li><p>For summarization, a dataset for supervised fine-tuning would have to include text and good summaries of them.</p></li>
<li><p>Aspects that are to be kept in mind are, for example, truthfulness of answers, length of answers, potentially harmful content (although filtering this out is one of the task that RL will help with), and answers should generally make sense (since nonsensical answers will alienate annotators).</p></li>
<li><p>By default, the model is trained for 3 epochs.</p></li>
</ol>
</div></blockquote>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># uncomment and run on Colab / install the package in your enironment if you haven&#39;t yet</span>
<span class="c1"># !pip install trl</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">from</span> <span class="nn">trl</span> <span class="kn">import</span> <span class="n">SFTTrainer</span>

<span class="c1"># load dataset</span>
<span class="c1"># you can inspect it to get a sense of its contents and formatting</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;CarperAI/openai_summarize_tldr&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">)</span>
<span class="c1"># load base model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;facebook/opt-350m&quot;</span><span class="p">)</span>

<span class="c1"># define a function that formats the prompts</span>
<span class="c1"># NOTE: the formatting of examples is extremely important for successful training and deployment for a given task</span>
<span class="c1"># for instance, the use of special tokens and prompt formatting should be consistent with the task, the model (if it already uses special tokens)</span>
<span class="c1"># and should be used in consistent ways in further training</span>

<span class="k">def</span> <span class="nf">formatting_prompts_func</span><span class="p">(</span><span class="n">example</span><span class="p">):</span>
    <span class="n">output_texts</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># iterate over batch</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">example</span><span class="p">[</span><span class="s1">&#39;prompt&#39;</span><span class="p">])):</span>
        <span class="c1"># retrieve both inputs and target labels</span>
        <span class="n">text</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">example</span><span class="p">[</span><span class="s1">&#39;prompt&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="si">}{</span><span class="n">example</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="n">output_texts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output_texts</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">SFTTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
    <span class="n">formatting_func</span><span class="o">=</span><span class="n">formatting_prompts_func</span><span class="p">,</span>
    <span class="n">max_seq_length</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
    <span class="n">dataset_batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

<span class="c1"># If one wants to pass custom training arguments, an example of doing so is here: https://github.com/huggingface/trl/blob/main/examples/scripts/sft.py</span>

<span class="c1"># NOTE: if you experience errors running this, setting the version of accelerate to pip install accelerate==0.27.2 might help</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="reward-modeling">
<h3>Reward modeling<a class="headerlink" href="#reward-modeling" title="Permalink to this heading">#</a></h3>
<p>A core component of RL fine-tuning is the reward model. There are various ways of obtaining a reward model. The perhaps most intuitive one, as discussed in the lecture, is obtaining human feedback as the reward signal. Since having humans give online feedback to an LM for thousands of samples is costly and cumbersome, instead, a reward model is trained on human preferences. For training such a reward model, human annotations are collected. As discussed in the lecture, these annotations can take different forms.</p>
<p>Below, we will look at an example based on simple binary comparisons, where human annotators were shown two outputs of the LM sampled for the same input <span class="math notranslate nohighlight">\(x\)</span> and had to indicate which of them they preferred (e.g., output <span class="math notranslate nohighlight">\(y_1\)</span> over <span class="math notranslate nohighlight">\(y_2\)</span>).
Specifically, the reward model is usually initialized from a pretrained LLM (maybe even the same one as the base LM of the policy), and fine-tuned with a specific <em>head</em> to output numerical scores. It is usually trained to maximize the difference in predicted scores scores <span class="math notranslate nohighlight">\(r\)</span> between the preferred and the rejected answer in a pair. Formally, the model is trained with the following loss:</p>
<div class="math notranslate nohighlight">
\[L(\theta) = -\frac{1}{N}\mathbb{E}_{(x, y_1, y_2) \sim D} [log \; (\sigma (r_{\theta} (x, y_1) - r_{\theta}(x, y_2)))] \]</div>
<p>This form of human annotations has proven especially useful for eliciting human intuitions for such difficult-to-capture concepts like “helpfulness”. It is much easier for humans to decide which of two options they prefer than, e.g., to consistently assign scalar 1-10 scores to outputs.</p>
<blockquote>
<div><p><strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 4.1.4: Reward modeling</span></strong></p>
<ol class="arabic simple">
<li><p>Consider <span class="xref myst">this</span> well-known dataset from Anthropic for training helpful and harmless assistants. Pick a specific sample. For this sample, which text corresponds to <span class="math notranslate nohighlight">\(x, y_1, y_2\)</span>?</p></li>
<li><p>An example of a model trained with the approach above can be found <a class="reference external" href="https://huggingface.co/reciprocate/openllama-13b_rm_oasst-hh">here</a>. Feel free to explore the repository, if you want. We will not test this model since it is quite large.</p></li>
</ol>
</div></blockquote>
<p>Click below to see the solution</p>
<div class="toggle docutils container">
<blockquote>
<div><ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(x\)</span> is the input and <span class="math notranslate nohighlight">\(y_1, y_2\)</span> are the outputs (whose nature depends on the chosen sample).</p></li>
</ol>
</div></blockquote>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">However</span><span class="p">,</span> <span class="n">there</span> <span class="n">are</span> <span class="n">also</span> <span class="n">alternative</span> <span class="n">ways</span> <span class="n">of</span> <span class="n">providing</span> <span class="n">rewards</span><span class="o">.</span> <span class="n">For</span> <span class="n">instance</span><span class="p">,</span> <span class="n">some</span> <span class="n">work</span> <span class="n">has</span> <span class="n">used</span> <span class="n">other</span> <span class="n">models</span> <span class="n">to</span> <span class="n">provide</span> <span class="n">feedback</span><span class="p">,</span> <span class="n">an</span> <span class="n">approach</span> <span class="n">that</span> <span class="ow">is</span> <span class="n">called</span> <span class="p">[</span><span class="n">RLAIF</span><span class="p">](</span><span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">arxiv</span><span class="o">.</span><span class="n">org</span><span class="o">/</span><span class="n">pdf</span><span class="o">/</span><span class="mf">2212.08073</span><span class="p">)</span> <span class="p">(</span><span class="n">RL</span> <span class="kn">from</span> <span class="nn">AI</span> <span class="n">feedback</span><span class="p">)</span><span class="o">.</span>
<span class="n">Alternatively</span><span class="p">,</span> <span class="n">more</span> <span class="n">task</span><span class="o">-</span><span class="n">specific</span> <span class="n">reward</span> <span class="n">models</span> <span class="n">can</span> <span class="n">be</span> <span class="n">constructed</span><span class="o">.</span> <span class="n">For</span> <span class="n">instance</span><span class="p">,</span> <span class="k">if</span> <span class="n">one</span> <span class="n">wants</span> <span class="n">to</span> <span class="n">use</span> <span class="n">RL</span><span class="o">-</span><span class="n">tuning</span> <span class="k">for</span> <span class="n">training</span> <span class="n">a</span> <span class="n">summarization</span> <span class="n">model</span><span class="p">,</span> <span class="n">the</span> <span class="n">score</span> <span class="k">for</span> <span class="n">evaluating</span> <span class="n">summaries</span> <span class="p">(</span><span class="n">ROUGE</span><span class="p">)</span> <span class="n">can</span> <span class="n">be</span> <span class="n">used</span> <span class="k">as</span> <span class="n">the</span> <span class="n">numerical</span> <span class="n">reward</span><span class="o">.</span> <span class="n">If</span> <span class="n">one</span> <span class="n">wants</span> <span class="n">to</span> <span class="n">train</span> <span class="n">a</span> <span class="n">model</span> <span class="k">for</span> <span class="n">generating</span> <span class="n">positive</span><span class="o">-</span><span class="n">sentiment</span> <span class="n">texts</span> <span class="n">only</span><span class="p">,</span> <span class="n">one</span> <span class="n">can</span> <span class="n">train</span> <span class="n">a</span> <span class="n">reward</span> <span class="n">model</span> <span class="n">on</span> <span class="n">labelled</span> <span class="n">sentiment</span> <span class="n">classification</span> <span class="n">data</span> <span class="n">like</span><span class="p">,</span> <span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="p">,</span> <span class="n">the</span> <span class="n">IMDB</span> <span class="n">dataset</span><span class="o">.</span>

<span class="o">&gt;</span> <span class="o">&lt;</span><span class="n">strong</span><span class="o">&gt;&lt;</span><span class="n">span</span> <span class="n">style</span><span class="o">=&amp;</span><span class="n">ldquo</span><span class="p">;</span><span class="n">color</span><span class="p">:</span><span class="c1">#D83D2B;&amp;rdquo;&gt;Exercise 4.1.5: Task-specific reward modeling&lt;/span&gt;&lt;/strong&gt;</span>
<span class="o">&gt;</span> <span class="mf">1.</span> <span class="n">The</span> <span class="n">code</span> <span class="n">below</span> <span class="n">provides</span> <span class="n">an</span> <span class="n">example</span> <span class="n">loading</span> <span class="n">a</span> <span class="n">trained</span> <span class="n">reward</span> <span class="n">model</span><span class="o">.</span> <span class="n">This</span> <span class="n">model</span> <span class="n">was</span> <span class="n">trained</span> <span class="n">on</span> <span class="n">movie</span> <span class="n">reviews</span><span class="p">,</span> <span class="ow">in</span> <span class="n">particular</span> <span class="n">to</span> <span class="n">assign</span> <span class="n">high</span> <span class="n">scores</span> <span class="n">to</span> <span class="n">positive</span> <span class="n">reviews</span> <span class="ow">and</span> <span class="n">low</span> <span class="n">scores</span> <span class="n">to</span> <span class="n">negative</span> <span class="n">reviews</span><span class="p">,</span> <span class="k">as</span> <span class="n">described</span> <span class="n">above</span><span class="o">.</span> <span class="n">Please</span> <span class="n">look</span> <span class="n">at</span> <span class="n">the</span> <span class="n">code</span> <span class="ow">and</span> <span class="n">make</span> <span class="n">sure</span> <span class="n">to</span> <span class="n">understand</span> <span class="n">it</span><span class="o">.</span> <span class="n">Test</span> <span class="n">it</span> <span class="n">on</span> <span class="n">a</span> <span class="n">few</span> <span class="n">of</span> <span class="n">your</span> <span class="n">own</span> <span class="n">intuitive</span> <span class="n">examples</span><span class="p">;</span> <span class="n">do</span> <span class="n">the</span> <span class="n">scores</span> <span class="p">(</span><span class="n">ordinally</span><span class="p">)</span> <span class="n">correspond</span> <span class="n">to</span> <span class="n">your</span> intuition<span class="o">?</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">AutoTokenizer</span><span class="p">,</span> 
    <span class="n">AutoModelForSequenceClassification</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">import</span> <span class="nn">torch</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load reward model and its tokenizer</span>
<span class="n">reward_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;lvwerra/distilbert-imdb&quot;</span><span class="p">)</span>
<span class="n">reward_model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;lvwerra/distilbert-imdb&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run an example from the IMDB train split to see how the reward model works</span>
<span class="n">positive_sentence</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span> <span class="c1">#### YOUR EXAMPLE HERE ####</span>
<span class="n">negative_sentence</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span> <span class="c1">#### YOUR EXAMPLE HERE ####</span>

<span class="n">input_pos</span> <span class="o">=</span> <span class="n">reward_tokenizer</span><span class="p">(</span><span class="n">positive_sentence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)</span>
<span class="n">input_neg</span> <span class="o">=</span> <span class="n">reward_tokenizer</span><span class="p">(</span><span class="n">negative_sentence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)</span>

<span class="n">reward_pos</span> <span class="o">=</span> <span class="n">reward_model</span><span class="p">(</span><span class="o">**</span><span class="n">input_pos</span><span class="p">)</span>
<span class="n">reward_neg</span> <span class="o">=</span> <span class="n">reward_model</span><span class="p">(</span><span class="o">**</span><span class="n">input_neg</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Reward for positive sentence: &quot;</span><span class="p">,</span> <span class="n">reward_pos</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Reward for negative sentence: &quot;</span><span class="p">,</span> <span class="n">reward_neg</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="ppo-training">
<h3>PPO training<a class="headerlink" href="#ppo-training" title="Permalink to this heading">#</a></h3>
<p>Once the SFT model and the reward model are available, the final step is to fine-tune the SFT model with RL. For this, a dataset to fine-tune on is needed again. Depedning on the model, sometimes the same dataset as for SFT used, sometimes a similar dataset with other inputs is used. Note that now the dataset doesn’t need any labels, but only the inputs (i.e., initial “states”) based on which the model will generate predictions (i.e., actions) which, in turn, will be assigned rewards (by the reward model which represents the environment).</p>
<p>There are different algorithms for training the policy. One currently common choice is the Proximal Policy Optimization (<a class="reference external" href="https://arxiv.org/pdf/1707.06347">PPO</a>). Its components were developed in order to stabilize policy updates and speed up convergence. However, there is a core shared idea between PPO and other algorithms in the category of <em>policy-gradient</em> methods.
Specifically, the theorem behind this class of methods shows that the policy can be trained with a loss based on “trials and errors” (i.e., based on sampling; this objective has been shown to update the policy in such a way that, by following it, the agent is expected to receive higher rewards). Specifically, for a training step, we can sample actions, retrieve their log probability under the current policy, get rewards for these actions, and calculate weight updates based on the product of log probability and the reward.</p>
<p>Note that choosing <em>hyperparameters</em> for successful RL fine-tuning is very important. While there are no proven results, the community best practice seems to indicate that the batch size should be relatively large, and the training should be quite short (e.g., only one epoch) to avoid undesired effects. Some of these details can be found, e.g., in the Llama-2 report.</p>
<blockquote>
<div><p><strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 4.1.6: RL training</span></strong></p>
<ol class="arabic simple">
<li><p>Suppose you train an LM for summarization and have a suitable reward model and dataset of input texts. Consider the last sentence of the explanation above. Please provide a specific details of such a training step with this summarization example (in words).</p></li>
<li><p>An example of using the <code class="docutils literal notranslate"><span class="pre">trl</span></code> library for training a model to generate positive model reviews (using the reward model above!) with PPO can be found <a class="reference external" href="https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb">here</a>. Please look at the code and try to understand all of it! Please ask questions or research if anything is unclear, this approach might be relevant for your own future exercise ;)</p></li>
</ol>
</div></blockquote>
<p>Click below to see the solution</p>
<div class="toggle docutils container">
<blockquote>
<div><ol class="arabic simple">
<li><p>For reference about typical hyperparameter settings: This is what the Llama report tells about RLHF training details: “Training Details. We train for one epoch over the training data. In earlier experiments, we found that training longer can lead to over-fitting. We use the same optimizer parameters as for the base model. The maximum learning rate is 5 × 10−6 for the 70B parameter Llama 2-Chat and 1 × 10−5 for the rest. The learning rate is decreased on a cosine learning rate schedule, down to 10% of the maximum learning rate. We use a warm-up of 3% of the total number of steps, with a minimum of 5. The effective batch size is kept fixed at 512 pairs, or 1024 rows per batch.“</p></li>
</ol>
</div></blockquote>
</div>
</section>
<section id="optional-outlook">
<h3>Optional outlook<a class="headerlink" href="#optional-outlook" title="Permalink to this heading">#</a></h3>
<p>RL fine-tuning is an active area of research, so there are many developments and new methods. Below are some optional resources if you want to know more.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/pdf/2305.18290">Rafailov et al. (2023) Direct Preference Optimization: Your Language Model is Secretly a Reward Model</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/2210.10760">Gao et al. (2022) Scaling Laws for Reward Model Overoptimization</a></p></li>
</ul>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "CogSciPrag/Understanding-LLMs-course",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./tutorials"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../lectures/05-finetuning-RLHF.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Fine-tuning and RLHF</p>
      </div>
    </a>
    <a class="right-next"
       href="../lectures/06-agents.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">LLM systems &amp; agents</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#flavours-of-fine-tuning">Flavours of fine-tuning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#outlook-peft-in-practice">Outlook: PEFT in practice</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rl-fine-tuning">RL fine-tuning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#policy">Policy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#supervised-fine-tuning">Supervised fine-tuning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reward-modeling">Reward modeling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ppo-training">PPO training</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optional-outlook">Optional outlook</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Michael Franke, Carsten Eickhoff, Polina Tsvilodub
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>