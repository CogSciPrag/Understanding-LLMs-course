
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Sheet 7.1: Behavioral assessment &amp; Evaluation &#8212; Understanding LMs</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'tutorials/07a-behavioral-assessment';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Sheet 7.2: Advanced evaluation" href="07b-biases-assessment.html" />
    <link rel="prev" title="Evaluation &amp; behavioral assessment" href="../lectures/08-evaluation.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo-ULM-2024.png" class="logo__image only-light" alt="Understanding LMs - Home"/>
    <script>document.write(`<img src="../_static/logo-ULM-2024.png" class="logo__image only-dark" alt="Understanding LMs - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Course overview: Understanding LMs
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">01 Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../lectures/01-introduction.html">Background</a></li>
<li class="toctree-l1"><a class="reference internal" href="01-introduction.html">Sheet 1.1: Practical set-up &amp; Training data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">02 ANNs &amp; RNNs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../lectures/02-torch-ANNs-RNNs.html">PyTorch, ANNs &amp; LMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="02a-pytorch-intro.html">Sheet 2.1: PyTorch essentials</a></li>
<li class="toctree-l1"><a class="reference internal" href="02b-MLE.html">Sheet 2.2: ML-estimation</a></li>
<li class="toctree-l1"><a class="reference internal" href="02c-MLP-pytorch.html">Sheet 2.3: Non-linear regression (MLP w/ PyTorch modules)</a></li>
<li class="toctree-l1"><a class="reference internal" href="02d-char-level-RNN.html">Sheet 2.4: Character-level sequence modeling w/ RNNs</a></li>
<li class="toctree-l1"><a class="reference internal" href="02e-intro-to-hf.html">Sheet 2.5: Introduction to HuggingFace &amp; LMs</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">03 LSTMs &amp; transformers</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../lectures/03-LSTMs-Transformers.html">LSTMs &amp; Transformers</a></li>

<li class="toctree-l1"><a class="reference internal" href="03a-tokenization-transformers.html">Sheet 3.1: Tokenization &amp; Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="03b-transformers-heads-training.html">Sheet 3.2: Transformer configurations &amp; Training utilities</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">04 Prompting</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../lectures/04-LLMs-Prompting.html">Prompting &amp; Current LMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="03c-decoding-prompting.html">Sheet 3.3: Prompting &amp; Decoding</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">05 Fine-tuning &amp; RLHF</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../lectures/05-finetuning-RLHF.html">Fine-tuning and RLHF</a></li>
<li class="toctree-l1"><a class="reference internal" href="04a-finetuning-RL.html">Sheet 4.1 Supervised fine-tuning and RL fine-tuning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">06 Agents &amp; applications</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../lectures/06-agents.html">LLM systems &amp; agents</a></li>
<li class="toctree-l1"><a class="reference internal" href="05a-agents.html">Sheet 5.1 LLM agents</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">07 Attribution</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../lectures/07-attribution.html">Attribution methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="06a-attribution.html">Sheet 6.1 LLM probing &amp; attribution</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">08 Behavioral evaluation</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../lectures/08-evaluation.html">Evaluation &amp; behavioral assessment</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Sheet 7.1: Behavioral assessment &amp; Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="07b-biases-assessment.html">Sheet 7.2: Advanced evaluation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">09 Mechanistic interpretation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../lectures/10-mechanistic-interpretability.html">Mechanistic Interpretability</a></li>
<li class="toctree-l1"><a class="reference internal" href="08a-mechanistic-interpretability.html">Sheet 8.1: Mechanistic interpretability</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">10 Understanding LLMs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../lectures/09-philosophy.html">Implications, Understanding &amp; Philosophy</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Homework</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../homework/01-language-modeling.html">Homework 1: Language models (58 points)</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/CogSciPrag/Understanding-LLMs-course/main?urlpath=tree/understanding-llms/tutorials/07a-behavioral-assessment.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Binder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Binder logo" src="../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li><a href="https://colab.research.google.com/github/CogSciPrag/Understanding-LLMs-course/blob/main/understanding-llms/tutorials/07a-behavioral-assessment.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="initThebeSBT()"
  class="btn btn-sm btn-launch-thebe dropdown-item"
  title="Launch Thebe"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="btn__text-container">Live Code</span>
</button>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/CogSciPrag/Understanding-LLMs-course" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/CogSciPrag/Understanding-LLMs-course/issues/new?title=Issue%20on%20page%20%2Ftutorials/07a-behavioral-assessment.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/tutorials/07a-behavioral-assessment.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Sheet 7.1: Behavioral assessment & Evaluation</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmark-testing">Benchmark testing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#metrics">Metrics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#outlook">Outlook</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-psychology">Machine psychology</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="sheet-7-1-behavioral-assessment-evaluation">
<h1>Sheet 7.1: Behavioral assessment &amp; Evaluation<a class="headerlink" href="#sheet-7-1-behavioral-assessment-evaluation" title="Link to this heading">#</a></h1>
<p><strong>Author</strong>: Polina Tsvilodub</p>
<p>This sheet focuses on evaluating the input-output (I/O) behavior of LLMs. Inspired by experimental paradigms / the terminology in cognitive science and psychology which investigate a blackbox (the human mind) via looking at the behavior across different interesting conditions (inputs), such assessment of LLMs (also blackboxes) can be called “behavioral assessment”. This approach can be seen as one piece that should work in combination with attribution methods discussed in the previous sheet in order to provide fuller understanding of what LLMs can or cannot do (I/O testing) and how they do it (attributions).
Following the structure of the lecture, we will first look at practical aspects of benchmark testing, and then look at “machine psychology”, which often draws on the same methods but addresses somewhat different research questions.</p>
<p>Therefore, the learning goals of this sheet are:</p>
<ul class="simple">
<li><p>look at examples of a few different benchmarks and how they are usually constructed</p></li>
<li><p>become familiar with standard evaluation metrics and methods used for evaluating LLMs on benchmarks (these include PPL, log probability based scores, accuracy, F1, free generation etc)</p></li>
<li><p>look at examples of machine psychology and how, in practice, LLM performance can be easily compared to human data.</p></li>
</ul>
<section id="benchmark-testing">
<h2>Benchmark testing<a class="headerlink" href="#benchmark-testing" title="Link to this heading">#</a></h2>
<p>Such I/O evaluations are the most common approach to LLM evaluation. Taking a more technical / engineering-oriented perspective which aims at building LLMs for specific application, it is very common to make use of large benchmark datasets which are designed to test models’ performance on a variety of tasks in an automated way. This is often done by checking the models’ outputs against ground truth answers or by computing standard scores for certain datasets. Therefore, quality of LLMs is measured by their scores on these benchmarks.</p>
<p>Initially, these benchmarks were designed to test LLMs’ linguistic performance since the goal of building the model is a system that predict grammatical and fluent natural language. Therefore, some first benchmarks (or, commonly used textual datasets) are, for instance, Wikipedia texts, the Penn Treebank, and the GLUE benchmark. Wikipedia texts are often used for measuring the perplexity of the model on this standard text (see below for details). The Penn Treebank was often used for fine-tuning or evaluating models, e.g., on part-of-speech tagging as an approximation of syntactic performance, while the GLUE benchmark contains tasks which are supposed to approximate (semantic) natural language understanding in the form of paraphrase tasks, sentiment classification, natural language inference tasks etc.</p>
<p>Now recent LLMs have shown perhaps unexpectedly impressive generalization to tasks which seem to require more than linguistic fluency, like solving math and reasoning problems. Therefore, more recent benchmarks incorporate tests of various tasks going beyond linguistic capability. Two of the most widely used benchmarks include the <a class="reference external" href="https://arxiv.org/abs/2009.03300">MMLU</a> and the <a class="reference external" href="https://arxiv.org/abs/2206.04615">BIG-Bench</a> datasets.
Given that SOTA LLMs are also often designed as assisstants and embedded in user-facing applications, it also became crucial to evaluate potenital social impacts that LLMs might exhibit with their outputs, like assessing biases and toxicity of the generations. To this end, specialized benchmarks like <a class="reference external" href="https://arxiv.org/abs/2009.11462">RealToxicityPrompts</a> or <a class="reference external" href="https://arxiv.org/abs/1804.09301">WinoGender</a> were created.</p>
<p>One crucial assumption behind benchmark evaluation is that benchmarks are representative of tasks and covers a wide variety of data that the model should perform well on in order to count as a good model for its target deployment. Although benchmarks arguably provide a wide coverage (they commonly contain thousands of inputs and answers), they often test only an approximation of what the model does in deployment (i.e., free text generation).
Furthermore, with newer models trained on newer crawls of the internet, there  are increasing worries of so-called <em>contamination</em>, i.e., actually including the test datasets in the training data of the models, thereby potentially inflating the models’ true generalization scores. For instance, Wikipedia is included in the training data of most of the modern models.</p>
<p>Scalably evaluating longer generated texts is quite a difficult task. This is because, intuitively, there is no single “ground truth answer” when it comes to writing; there are many equally good ways of writing summary of a text, or even potentially multiple ways of translating a sentence. This makes text evaluation difficult to evaluate automatically. This is still a largely unsolved issue (!), so that human or machine evaluation is often used. The available methods for automated text scoring are rooted in work on summarization and machine translation, and require (human-written) gold-standard texts.</p>
<p>Note that when mentioning a <em>model</em> in the explanations, we refer to trained models which are evaluated with respect to their performance, i.e., in <em>inference mode</em>. If one wanted to track the performance on certain benchmarks during training, one could also run evaluations on intermediate model checkpoints during training, too. Just note that the model is “frozen” and runs in inference mode during all of the testing described in this sheet.</p>
<p>In sum, the reasons why benchmarks are so widely used are a few core advantages:</p>
<ul class="simple">
<li><p>the availability if a few well-known datasets leads to (somewhat of a) standardization of the evaluation procedure across different work.</p></li>
<li><p>their large scale often provides high coverage, more reliable results (although coverage might not always mean consistent quality or variability expected, e.g., by linguists).</p></li>
<li><p><strong>crucially</strong>: they are design to be evaluated with easy to compute <em>automatic evaluation metrics</em>. You have heard about them in the lecture; we will recap these below and then work with them in practice.</p></li>
</ul>
<section id="metrics">
<h3>Metrics<a class="headerlink" href="#metrics" title="Link to this heading">#</a></h3>
<p><strong>Perplexity</strong>: It is computed as:
$<span class="math notranslate nohighlight">\(PPL_{LM}(x_0 ... x_n) = \exp(\frac{1}{n}\sum_{i=0}^n - \log P_{LM}(x_i \mid x_{&lt;i})) \)</span>$</p>
<p>Note that this is only applicable to causal language models. This is the metric commonly used, e.g., on the Wikipedia texts. For instance, the PPL of GPT-2 on the Penn Treebank dataset is 35.76, while the perplexity of GPT-3 on the same dataset is 20.50.
The idea is that an ideal model should have a perplexity as close to 0 as possible for a naturally occurring text that it has learned, thereby approximating good fit to the “ground truth distribution of natural language”.</p>
<p>Below is some code for computing the perplexity of different sizes of GPT-2 for an exerpt from Wkipedia.</p>
<blockquote>
<div><p><strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 7.1.1: Calculating perplexity</span></strong></p>
<ol class="arabic simple">
<li><p>Please complete the code below. (Hint: only one simple transformation is required in order to calculate the perplexity from the NLL loss)</p></li>
<li><p>Compare the results for the models of different sizes. Does their comparison (ordering) match your intuition?</p></li>
</ol>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
<span class="k">elif</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;mps&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># perplexity evaluation</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
<span class="n">test</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;wikitext&quot;</span><span class="p">,</span> <span class="s1">&#39;wikitext-2-raw-v1&#39;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;test&quot;</span><span class="p">)</span>

<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
    <span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">test</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">][:</span><span class="mi">10</span><span class="p">]),</span> 
    <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span>
<span class="p">)</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># select a part of the text</span>
<span class="c1"># input_tokens = encodings[:,10:50]</span>

<span class="c1"># load models of different sizes</span>
<span class="n">model_s</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">model_xl</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2-xl&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">output_s</span> <span class="o">=</span> <span class="n">model_s</span><span class="p">(</span><span class="n">input_tokens</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">input_tokens</span><span class="p">)</span>
<span class="n">output_xl</span> <span class="o">=</span> <span class="n">model_xl</span><span class="p">(</span><span class="n">input_tokens</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">input_tokens</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Average NLL for wikipedia chunk under small model &quot;</span><span class="p">,</span> <span class="n">output_s</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Average NLL for wikipedia chunk under xl model &quot;</span><span class="p">,</span> <span class="n">output_xl</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

<span class="c1">### your code for computing the perplexity goes here ###</span>
<span class="n">perplexity_s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span> <span class="n">output_s</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
<span class="n">perplexity_xl</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span> <span class="n">output_xl</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;PPL of smaller model: </span><span class="si">{</span><span class="n">perplexity_s</span><span class="si">}</span><span class="s2">, PPL of larger model: </span><span class="si">{</span><span class="n">perplexity_xl</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://huggingface.co/docs/transformers/en/perplexity">This</a> blogpost provides an interesting outlook to dealing with the issue of fixed length of the context window of LMs when trying to compute the perplexity of longer texts (e.g., Wikipedia).</p>
<p><strong>Accuracy</strong>: this is a standard metric widely used in many domains, not only NLP. It computes the proportion of correct responses in a set of tasks. Presupposes that there is a single correct answer for a given input. We have seen in the lecture that one way to compute accuracy is to score each answer option, given the input, under the LLM, and retreive the predicted options via <span class="math notranslate nohighlight">\(argmax\)</span>; i.e., take the option for which the model assigned the highest (log) probability to be the chosen option. If this option is the ground truth option, the model’s prediction is correct for this test item (i.e., correctness = 1); otherwise, correctness = 0. Accuracy is then the average correctness across all the test items in the benchmark. The lecture pointed out limitations of the argmax approach. Just as a recap, the underlying assumption is that a model that can perform a task correctly will predict:
$<span class="math notranslate nohighlight">\(\log P_{LM}(\text{correct label} \mid \text{context}) &gt;  \log P_{LM}(\text{incorrect label} \mid \text{context})\)</span>$</p>
<p>The advantage of this approach is that it makes sure to score only the available answer options under the model, which is an especially important constraint for weaker models. However, SOTA more powerful LLMs, especially if they are instruction-tuned are often also tested via <em>text generation</em>. I.e., the input is given with an appropriate instruction, and the model’s generated text is evaluated via string matching (e.g., regex of simple matching). If the correct answer option was generated, the model’s correctness is 1 for this trial, and 0 otherwise.</p>
<p>Below is some code exemplifying evaluating a model on a question answering benchmark CommonsenseQA which we have already used in the homework, via scoring answers under the model. This now provides an automatic implementation of the last task of HW1 / task 2 in HW2. For retrieving conditional log probabilities of different options, given a context, we will be using the package <a class="reference external" href="https://github.com/kanishkamisra/minicons"><code class="docutils literal notranslate"><span class="pre">minicons</span></code></a>.</p>
<p>Note that here we are interested in scoring the different response options, given the questions, under the model, rather prompting the model with a list of possible options and letting it generate the option label. Therefore, the wrangling of the dataset is slightly different than in the homework.</p>
<blockquote>
<div><p><strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 7.1.2: Calculating accuracy</span></strong></p>
<ol class="arabic simple">
<li><p>Please complete the code below.</p></li>
<li><p>Compare the results to your results from the homework. Which are better? Do you think the log probability based evaluation is better than the strategy we used in the homework? Why (not)?</p></li>
<li><p>What is the expected chance accuracy on this dataset? Why is it important to consider chance accuracy when interpreting the results of a system?</p></li>
<li><p>The lecture mentioned effects of various bias corrections that can be applied to the raw scores. In the code below, by default, a length correction is applied (i.e., average log probabilities are used). use the docs / examples of the minicons package <a class="reference external" href="https://github.com/kanishkamisra/minicons/blob/master/examples/surprisals.md">here</a> to retrieve “raw” log probabilities of the completions (i.e., sums over the token probabilities) and use those to calculate the accuracy. Do the results change?</p></li>
</ol>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># load dataset </span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;tau/commonsense_qa&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">massage_input_text</span><span class="p">(</span><span class="n">example</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Helper for converting labels, answer options</span>
<span class="sd">    into a single string.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    example: dict</span>
<span class="sd">        Sample input from the dataset which contains the </span>
<span class="sd">        question, answer labels (e.g. A, B, C, D),</span>
<span class="sd">        the answer options for the question, and which </span>
<span class="sd">        of the answers is correct.</span>
<span class="sd">    </span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    answer_options: list[str]</span>
<span class="sd">        Formatted list of answer options (e.g., &#39;A. &lt;option 1&gt; B. &lt;option 2&gt;&#39; etc)</span>
<span class="sd">        and the ground truth answer.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># combine each label with its corresponding text</span>
    <span class="n">answer_options_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span>
        <span class="n">example</span><span class="p">[</span><span class="s2">&quot;choices&quot;</span><span class="p">][</span><span class="s2">&quot;label&quot;</span><span class="p">],</span>
        <span class="n">example</span><span class="p">[</span><span class="s2">&quot;choices&quot;</span><span class="p">][</span><span class="s2">&quot;text&quot;</span><span class="p">]</span>
    <span class="p">))</span>
    <span class="c1"># join each label and text with . and space</span>
    <span class="n">answer_options</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">label</span><span class="si">}</span><span class="s2">. </span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">answer_options_list</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">answer_options</span>

<span class="c1"># process input texts of validation dataset</span>
<span class="n">massaged_dataset_val</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;validation&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="n">example</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="n">example</span><span class="p">[</span><span class="s2">&quot;question&quot;</span><span class="p">],</span>
        <span class="s2">&quot;answers&quot;</span><span class="p">:</span> <span class="n">massage_input_text</span><span class="p">(</span><span class="n">example</span><span class="p">),</span>
        <span class="c1"># get the index of the correct answer</span>
        <span class="s2">&quot;label&quot;</span><span class="p">:</span> <span class="n">example</span><span class="p">[</span><span class="s2">&quot;choices&quot;</span><span class="p">][</span><span class="s2">&quot;label&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">example</span><span class="p">[</span><span class="s2">&quot;answerKey&quot;</span><span class="p">])</span>
    <span class="p">}</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">massaged_dataset_val</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># iterate over part of the validation set an compute accuracy </span>
<span class="c1"># (the test set doesn&#39;t have ground truth labels)</span>

<span class="c1"># set up a scorer </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">minicons</span><span class="w"> </span><span class="kn">import</span> <span class="n">scorer</span> 

<span class="n">lm_scorer</span> <span class="o">=</span> <span class="n">scorer</span><span class="o">.</span><span class="n">IncrementalLMScorer</span><span class="p">(</span>
    <span class="s1">&#39;gpt2&#39;</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># initialize list for storing the correctness of the model predictions</span>
<span class="n">correctness</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="c1"># get the ith example from the validation set</span>
    <span class="n">example</span> <span class="o">=</span> <span class="n">massaged_dataset_val</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="c1"># get the text of the question</span>
    <span class="n">question</span> <span class="o">=</span> <span class="n">example</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span>
    <span class="c1"># get the list of answer options</span>
    <span class="n">answer_options</span> <span class="o">=</span> <span class="n">example</span><span class="p">[</span><span class="s1">&#39;answers&#39;</span><span class="p">]</span>
    <span class="c1"># get the ground truth label</span>
    <span class="n">label</span> <span class="o">=</span> <span class="n">example</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span>
    
    <span class="c1"># pass a list of contexts and a list of continuations to be scored</span>
    <span class="n">answer_scores</span> <span class="o">=</span> <span class="n">lm_scorer</span><span class="o">.</span><span class="n">conditional_score</span><span class="p">(</span>
        <span class="c1"># format the question into a list of same length as the number of answer options</span>
        <span class="p">[</span><span class="n">question</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">answer_options</span><span class="p">),</span> 
        <span class="n">answer_options</span><span class="p">,</span>
    <span class="p">)</span> 
    <span class="c1"># get the predicted answer (Hint: check above how we determine what the model predicts is the correct answer)</span>
    <span class="n">predicted_label</span> <span class="o">=</span> <span class="c1">### YOUR CODE HERE ###</span>
    <span class="c1"># check if the prediction is correct</span>
    <span class="n">is_correct</span> <span class="o">=</span> <span class="n">predicted_label</span> <span class="o">==</span> <span class="n">label</span>
    <span class="n">correctness</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">is_correct</span><span class="p">)</span>

<span class="c1"># compute the accuracy</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy: &quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">correctness</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p><strong>F1-score</strong>:</p>
<p>This is a score that is commonly used on <em>binary</em> tasks (i.e., tasks with only two possible answer options) instead of accuracy. It is calculated from the <em>precision</em> and <em>recall</em> of the test results.  The precision is the number of true positive results divided by the number of all samples predicted to be positive, including those not identified correctly. The recall is the number of true positive results divided by the number of all samples that should have been identified as positive. Here, positive and negative results refer to predictions in each of the two answer categories, respectively.</p>
<p>The F1 score is the harmonic mean of the precision and recall. It thus symmetrically represents both precision and recall in one metric:
$<span class="math notranslate nohighlight">\(F1 = 2 \times \frac{\text{precision} \cdot \text{recall}}{\text{precision} + \text{recall}}\)</span><span class="math notranslate nohighlight">\(
The more generic \)</span>F_{\beta}$ score applies additional weights, valuing one of precision or recall more than the other.
The highest possible value of an F-score is 1.0, indicating perfect precision and recall, and the lowest possible value is 0, if precision and recall are zero.</p>
<p>We will use the BoolQ dataset from the SuperGLUE benchmark and evaluate GPT-2’s performance in terms of F1 scores on it. This is a task wherein the model has to predict an answer (true/false) to a question, given context. Therefore, the positive prediction here will be “true”, and the negative “false”.</p>
<p>You can find the test dataset <a class="reference external" href="https://github.com/CogSciPrag/Understanding-LLMs-course/tree/main/understanding-llms/tutorials/files/super_glue_boolq.csv">here</a>.
We will retrieve the model’s predictions similarly to the accuracy evaluation above. Specifically, we will retrieve the probabilities of “true” and “false”, given the context and the question.</p>
<blockquote>
<div><p><strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 7.1.3: Calculating F1 scores</span></strong></p>
<ol class="arabic simple">
<li><p>Please complete the code below.</p></li>
<li><p>Calculate the results. Does GPT-2 do well in this task?</p></li>
<li><p>Evaluate the performance of the model using accuracy. What is the conceptual difference between the two results? Which one might be more reliable and why?</p></li>
<li><p>Find out how to compute the F1 score with the <code class="docutils literal notranslate"><span class="pre">sklearn.metrics</span></code> package.</p></li>
</ol>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="n">df_boolq</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;files/super_glue_boolq.csv&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># inspect the dataset to understand its structure</span>
<span class="c1"># if is_true = 1, it means that the answer to the question is &quot;True&quot;</span>
<span class="n">df_boolq</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predicted_answer</span><span class="o">=</span> <span class="p">[]</span>
<span class="n">true_answers</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">df_boolq</span><span class="p">[:</span><span class="mi">200</span><span class="p">]</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
    <span class="c1"># get the context for the question</span>
    <span class="n">context</span> <span class="o">=</span> <span class="n">r</span><span class="p">[</span><span class="s1">&#39;sentence1&#39;</span><span class="p">]</span>
    <span class="c1"># get the text of the question</span>
    <span class="n">question</span> <span class="o">=</span> <span class="n">r</span><span class="p">[</span><span class="s1">&#39;sentence2&#39;</span><span class="p">]</span>
    <span class="c1"># construct the list of answer options</span>
    <span class="n">answer_options</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;False&quot;</span><span class="p">,</span> <span class="s2">&quot;True&quot;</span><span class="p">]</span>
    <span class="c1"># get the ground truth label</span>
    <span class="n">true_answer</span> <span class="o">=</span> <span class="n">r</span><span class="p">[</span><span class="s2">&quot;is_true&quot;</span><span class="p">]</span>
    
    <span class="c1"># pass a list of contexts and a list of continuations to be scored</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">answer_scores</span> <span class="o">=</span> <span class="n">lm_scorer</span><span class="o">.</span><span class="n">conditional_score</span><span class="p">(</span>
            <span class="c1"># format the context + question into a list of same length as the number of answer options</span>
            <span class="p">[</span><span class="n">context</span> <span class="o">+</span> <span class="s2">&quot; &quot;</span> <span class="o">+</span> <span class="n">question</span> <span class="o">+</span> <span class="s2">&quot;?&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">answer_options</span><span class="p">),</span> 
            <span class="n">answer_options</span><span class="p">,</span>
        <span class="p">)</span> 
    <span class="k">except</span><span class="p">:</span>
        <span class="k">continue</span>
    <span class="c1"># get the predicted answer (Hint: check above how we determine what the model predicts is the correct answer)</span>
    <span class="n">predicted_label</span> <span class="o">=</span> <span class="c1">### YOUR CODE HERE ###</span>
    <span class="c1"># record the predicted answer</span>
    <span class="n">predicted_answer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">predicted_label</span><span class="p">)</span>
    <span class="n">true_answers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">true_answer</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># compute the F1 score</span>
<span class="n">true_positive</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([(</span><span class="n">i</span> <span class="o">==</span> <span class="n">j</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">i</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">predicted_answer</span><span class="p">,</span> <span class="n">true_answers</span><span class="p">)])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;True positive: &quot;</span><span class="p">,</span> <span class="n">true_positive</span><span class="p">)</span>
<span class="n">false_positive</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([(</span><span class="n">i</span> <span class="o">!=</span> <span class="n">j</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">i</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">predicted_answer</span><span class="p">,</span> <span class="n">true_answers</span><span class="p">)])</span> 
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;False positive: &quot;</span><span class="p">,</span> <span class="n">false_positive</span><span class="p">)</span>
<span class="n">false_negative</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([(</span><span class="n">i</span> <span class="o">!=</span> <span class="n">j</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">predicted_answer</span><span class="p">,</span> <span class="n">true_answers</span><span class="p">)])</span>
<span class="n">f1_score</span> <span class="o">=</span> <span class="c1"># YOUR CODE HERE</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;F1 score: &quot;</span><span class="p">,</span> <span class="n">f1_score</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>NLG metrics</strong>: The lecture discussed the common metrics for generation evaluation: BLEU, ROUGE and METEOR. We already used ROUGE in task 2 of HW 3. These metrics all check whether the predicted text overlaps with ground truth texts. Often different overlap measures are used; for instance, overlaps of unigrams, bigrams or trigrams can be computed.
These metrics originate from summarization and machine translation, where corpora of reference human summaries or translations. These are also applied to any other generation tasks, too, as long as reference texts are available.</p>
<p>Below is space for trying out the BLEU score, in order to evaluate the translation predicted by FLAN-T5 small.</p>
<blockquote>
<div><p><strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 7.1.3: Calculating NLG scores</span></strong></p>
<ol class="arabic simple">
<li><p>Please complete the code below by referring to the docs <a class="reference external" href="https://huggingface.co/spaces/evaluate-metric/bleu">here</a>.</p></li>
<li><p>Calculate the results. What happens if you change the values of the <code class="docutils literal notranslate"><span class="pre">max_order</span></code> parameter, for this example and in general?</p></li>
<li><p>If possible, try this out with a different language pair / a different sentence pair.</p></li>
</ol>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># import the implementation of the bleu score computation</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchtext.data.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">bleu_score</span>
<span class="c1"># load model and tokenizer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">T5Tokenizer</span><span class="p">,</span> <span class="n">T5ForConditionalGeneration</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="n">tokenizer_t5</span> <span class="o">=</span> <span class="n">T5Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;google/flan-t5-small&quot;</span><span class="p">)</span>
<span class="n">model_t5</span> <span class="o">=</span> <span class="n">T5ForConditionalGeneration</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;google/flan-t5-small&quot;</span><span class="p">)</span>

<span class="c1"># define example sentences for translating from English to German</span>
<span class="n">text_en</span> <span class="o">=</span> <span class="s2">&quot;All of the others were of a different opinion.&quot;</span>
<span class="n">text_de</span> <span class="o">=</span> <span class="s2">&quot;Alle anderen waren anderer Meinung.&quot;</span>
<span class="c1"># define task </span>
<span class="n">prefix</span> <span class="o">=</span> <span class="s2">&quot;Translate to German: &quot;</span>

<span class="c1"># encode the source and the target sentences</span>
<span class="n">encoding_en</span> <span class="o">=</span> <span class="n">tokenizer_t5</span><span class="p">(</span>
    <span class="p">[</span><span class="n">prefix</span> <span class="o">+</span> <span class="n">text_en</span><span class="p">],</span>
    <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span>
<span class="p">)</span><span class="o">.</span><span class="n">input_ids</span>
<span class="c1"># we don&#39;t need the task prefix before the target</span>
<span class="n">encoding_de</span> <span class="o">=</span> <span class="n">tokenizer_t5</span><span class="p">(</span>
    <span class="p">[</span><span class="n">text_de</span><span class="p">],</span>
    <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span>
<span class="p">)</span><span class="o">.</span><span class="n">input_ids</span>

<span class="c1"># predict with model</span>
<span class="n">predicted_de</span> <span class="o">=</span> <span class="n">model_t5</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">encoding_en</span><span class="p">)</span>



<span class="c1"># decode the prediction</span>
<span class="n">predicted_decoded_de</span> <span class="o">=</span> <span class="n">tokenizer_t5</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span>
    <span class="n">predicted_de</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Predicted translation: &quot;</span><span class="p">,</span> <span class="n">predicted_decoded_de</span><span class="p">)</span>

<span class="c1"># compute BLEU for the prediction</span>
<span class="c1">### YOUR CODE CALLING THE HELPER ABOVE GOES HERE ###</span>
<span class="n">bleu</span> <span class="o">=</span> 
</pre></div>
</div>
</div>
</div>
</section>
<section id="outlook">
<h3>Outlook<a class="headerlink" href="#outlook" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>The log probability based scoring methods are generally only well-defined for causal LMs. However, the work by <a class="reference external" href="https://arxiv.org/abs/1910.14659">Salazar et al. (2019)</a> introduces a pseudo log likelihood scoring for masked LMs.</p></li>
<li><p>The lecture and the sheet have pointed out the diversity in available evaluation methods of LMs, which might raise the natural question for you which method to choose and which one might work best. While this is an open research question, this great paper by <a class="reference external" href="https://aclanthology.org/2023.emnlp-main.306.pdf">Hu &amp; Levy (2032)</a> provides some insights regarding prompting vs. log probability based methods.</p></li>
<li><p>The lecture discussed the topic of <em>calibration</em>. There is a whole suite of work addressing calibration from a slightly more performance oriented perspective: the correlation of predicted probabilities of the correct response in multiple choice tasks is compared to the accuracy of the LM on those tasks, which is often put in context of LM’s knowledge and confidence about factual information. One influential paper is by [Kadavath et al. (2022) (https://arxiv.org/abs/2207.05221)].</p></li>
</ul>
</section>
</section>
<section id="machine-psychology">
<h2>Machine psychology<a class="headerlink" href="#machine-psychology" title="Link to this heading">#</a></h2>
<p>As discussed in the lecture, there is another important perspective on evaluting LLMs that can be called <em>machine psychology</em>, which can provide better and more robust evaluation results of LLMs in tandem with benchmark testing.<br />
This approach targets better understanding of different (e.g., emergent) capabilities of LLMs and is often informed by methods from psuchology, linguistics and cognitive science.
There are several critical points that this perspective addresses:</p>
<ul class="simple">
<li><p>The datasets and tests used here are often much more curated and, motivated by best practices of human research, cover diverse conditions related to the same phenomenon and better isolate that phenomenon (in contrast to more generic latge benchmarks).</p></li>
<li><p>Studies in this domain may aim to evaluate to what extent LLMs’ I/O behavior is <em>human-like</em>. This may be relevant, e.g., in user-facing scenarios where the systems are employed.</p></li>
<li><p>Finally, studies in this domain might shed light onto long-standing theoretical debates. For instance, recent models have been taken to provide evidence regarding the learnability of grammar from data only (without innate biases). <a class="reference external" href="https://lingbuzz.net/lingbuzz/007180">This opinion paper</a> provides details on this debate.</p></li>
</ul>
<p>Importantly, the LLM prediction retrieval methods for investigating machine psychology are often similar or based on the benchmark evaluation methods. The difference often lies in the careful layout of the datasets, the hypotheses, and the overall methods for testing these hypotheses (e.g., supplemented with careful comaprison to human data).</p>
<p>The sections below provide some examples of research questions within machine psychology that were mentioned in the lecture, and practical implementations for addressing them.</p>
<p>First, we will look at targeted syntactic evaluation of LLMs and address the question of whether GPT-2 is capable of distinguishing grammatical and ungrammatical sentences.</p>
<blockquote>
<div><p><strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 7.1.4: Machine psychology</span></strong></p>
<ol class="arabic simple">
<li><p>Please complete the code below. (The docs <a class="reference external" href="https://github.com/kanishkamisra/minicons/blob/master/examples/surprisals.md">here</a> might help)</p></li>
<li><p>Compute the results. How would you answer the research question above, based on these results?</p></li>
<li><p>What are alternative scores which could be used to test this question? Might any of them be better than the implementation below? Why?</p></li>
</ol>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">grammaticality_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;files/grammaticality_tests.csv&quot;</span><span class="p">)</span>
<span class="n">grammaticality_df</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># iterate over the pairs of sentences and compare the grammatical and ungrammatical sentences</span>
<span class="n">grammaticality_predictions</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">grammaticality_df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
    <span class="c1"># get the grammatical sentence</span>
    <span class="n">grammatical_sentence</span> <span class="o">=</span> <span class="n">r</span><span class="p">[</span><span class="s2">&quot;grammatical_sentence&quot;</span><span class="p">]</span>
    <span class="c1"># get the ungrammatical sentence</span>
    <span class="n">ungrammatical_sentence</span> <span class="o">=</span> <span class="n">r</span><span class="p">[</span><span class="s2">&quot;ungrammatical_sentence&quot;</span><span class="p">]</span>
    <span class="c1"># compute sentence log probabilities</span>
    <span class="n">grammatical_log_prob</span> <span class="o">=</span> <span class="n">lm_scorer</span><span class="o">.</span><span class="n">sequence_score</span><span class="p">(</span>
        <span class="c1">### YOUR CODE HERE ###</span>
    <span class="p">)</span>
    <span class="n">ungrammatical_log_prob</span> <span class="o">=</span> <span class="n">lm_scorer</span><span class="o">.</span><span class="n">sequence_score</span><span class="p">(</span>
        <span class="c1">### YOUR CODE HERE ###</span>
    <span class="p">)</span>
    <span class="c1"># compare the log probabilities</span>
    <span class="n">is_grammatical</span> <span class="o">=</span> <span class="c1">### YOUR CODE HERE ###</span>
    <span class="n">grammaticality_predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">is_grammatical</span><span class="p">)</span>
    
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy: &quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">grammaticality_predictions</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Now, we address a reserach question at the intersection linguistic theory and methodological best practices. Specifically, following <a class="reference external" href="https://arxiv.org/abs/2403.00998">this</a> paper, we want to understand whether:</p>
<ul class="simple">
<li><p>LLMs can perform pragmatic language understanding tasks</p></li>
<li><p>whether they do so in a human-like way (in terms of mathcing human accuracy)</p></li>
<li><p>and whether different ways of retrieving LLM predictions lead to different fits to human data.</p></li>
</ul>
<p>Specifically, we will focus on the interpretation of <em>metaphors</em>. The data from one LLM, namely GPT-3.5-turbo-instruct, and from humans, can be found <a class="reference external" href="https://github.com/CogSciPrag/Understanding-LLMs-course/tree/main/understanding-llms/tutorials/files">here</a>.
The human data used in the paper and provided here is taken from the paper by <a class="reference external" href="https://aclanthology.org/2023.acl-long.230/">Hu et al. (2022)</a>.
An item in this dataset is a multiple choice task, and looks like this:</p>
<blockquote>
<div><p>Context: Mary was asked about the town that she has just moved to. Mary responded: “This town is a chimney.” What does Mary mean?
Answer options:</p>
<ul class="simple">
<li><p>The town is not one of the cleanest one. (target nonliteral interpretation)</p></li>
<li><p>The people living in this town are very welcoming. (incorrect nonliteral interpretation)</p></li>
<li><p>All houses in this town have chimneys. (incorrect nonliteral interpretation)</p></li>
<li><p>The town is a chimney. (incorrect literal interpretation)</p></li>
<li><p>Mary found a job at a company installing chimneys. (incorrect distractor)</p></li>
</ul>
</div></blockquote>
<blockquote>
<div><p><strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 7.1.5: Machine psychology 2</span></strong></p>
<ol class="arabic simple">
<li><p>Please look at the papers and complete the code below. What do the results tell us with respect to our research questions above?</p></li>
</ol>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">metaphor_results_gpt</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;files/gpt_metaphor_results.csv&quot;</span><span class="p">)</span>
<span class="n">metaphor_results_human</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;files/Human_Metaphor.csv&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">metaphor_results_gpt</span>
<span class="n">metaphor_results_human</span>
</pre></div>
</div>
</div>
</div>
<p>Specifically, the GPT results were computed with different scoring methods which is recorded in the <code class="docutils literal notranslate"><span class="pre">score</span></code> column. First, we are interested in the question which score resulted in the highest accuracy for GPT (whether the prediction for a given item is correct is recorded in the column <code class="docutils literal notranslate"><span class="pre">target</span></code>):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### YOUR CODE HERE ###</span>
</pre></div>
</div>
</div>
</div>
<p>Next, we are interested in comparing human and GPT results. Human results contain information whether the participant answered the item correcty in the column <code class="docutils literal notranslate"><span class="pre">Correct</span></code>.
There are various ways of comparing the predictions. Following Hu et al (2022), we could compute the correlations of by-item accuracies of GPT and human data. The item IDs can be found in <code class="docutils literal notranslate"><span class="pre">itemNum</span></code> and <code class="docutils literal notranslate"><span class="pre">item_id</span></code>, respectively. One way to compute correlations in Python is documented e.g., <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.corrcoef.html">here</a>. Furthermore, might want to investigate the correlation separately for the different LLM scoring methods.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#### YOUR CODE HERE ####</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "CogSciPrag/Understanding-LLMs-course",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./tutorials"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../lectures/08-evaluation.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Evaluation &amp; behavioral assessment</p>
      </div>
    </a>
    <a class="right-next"
       href="07b-biases-assessment.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Sheet 7.2: Advanced evaluation</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmark-testing">Benchmark testing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#metrics">Metrics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#outlook">Outlook</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-psychology">Machine psychology</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Michael Franke, Carsten Eickhoff, Polina Tsvilodub
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>