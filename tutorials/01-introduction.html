

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Sheet 1.1: Practical set-up &amp; Training data &#8212; Understanding LLMs</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'tutorials/01-introduction';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="PyTorch, ANNs &amp; LMs" href="../lectures/02-torch-ANNs-RNNs.html" />
    <link rel="prev" title="Background" href="../lectures/01-introduction.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo-ULM-2024.png" class="logo__image only-light" alt="Understanding LLMs - Home"/>
    <script>document.write(`<img src="../_static/logo-ULM-2024.png" class="logo__image only-dark" alt="Understanding LLMs - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Course overview: Understanding LLMs
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">01 Introduction</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../lectures/01-introduction.html">Background</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Sheet 1.1: Practical set-up &amp; Training data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">02 ANNs, RNNs &amp; Transformers</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../lectures/02-torch-ANNs-RNNs.html">PyTorch, ANNs &amp; LMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="02a-pytorch-intro.html">Sheet 2.1: PyTorch essentials</a></li>
<li class="toctree-l1"><a class="reference internal" href="02b-MLE.html">Sheet 2.2: ML-estimation</a></li>
<li class="toctree-l1"><a class="reference internal" href="02c-MLP-pytorch.html">Sheet 2.3: Non-linear regression (MLP w/ PyTorch modules)</a></li>
<li class="toctree-l1"><a class="reference internal" href="02d-char-level-RNN.html">Sheet 2.4: Character-level sequence modeling w/ RNNs</a></li>
<li class="toctree-l1"><a class="reference internal" href="02e-intro-to-hf.html">Sheet 2.5: Introduction to HuggingFace &amp; LMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lectures/03-LSTMs-Transformers.html">LSTMs &amp; Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="03a-tokenization-transformers.html">Sheet 3.1: Tokenization &amp; Transformers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Homework</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../homework/01-language-modeling.html">Homework 1: Language models (50 points)</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/CogSciPrag/Understanding-LLMs-course/main?urlpath=tree/understanding-llms/tutorials/01-introduction.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onBinder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li><a href="https://colab.research.google.com/github/CogSciPrag/Understanding-LLMs-course/blob/main/understanding-llms/tutorials/01-introduction.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="initThebeSBT()"
  class="btn btn-sm btn-launch-thebe dropdown-item"
  title="Launch Thebe"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="btn__text-container">Live Code</span>
</button>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/CogSciPrag/Understanding-LLMs-course" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/CogSciPrag/Understanding-LLMs-course/issues/new?title=Issue%20on%20page%20%2Ftutorials/01-introduction.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/tutorials/01-introduction.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Sheet 1.1: Practical set-up & Training data</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#installing-requirements">Installing requirements</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#colab">Colab</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#local-installation">Local installation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#verifying-requirement-installation">Verifying requirement installation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#best-practices-for-writing-code">Best practices for writing code</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-training-data">Understanding training data</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#core-concepts">Core concepts</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#main-training-data-processing-steps">Main training data processing steps</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-documentation">Dataset documentation</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="sheet-1-1-practical-set-up-training-data">
<h1>Sheet 1.1: Practical set-up &amp; Training data<a class="headerlink" href="#sheet-1-1-practical-set-up-training-data" title="Permalink to this heading">#</a></h1>
<p><strong>Author</strong>: Polina Tsvilodub</p>
<p>This page contains materials for the first tutorial session (April 19th).</p>
<p>The learning goals for the first tutorial are:</p>
<ul class="simple">
<li><p>preparing the Python requirements for practical exercises in the upcoming tutorials,</p></li>
<li><p>test-running a few lines of code,</p></li>
<li><p>familiarization with a few coding best practices,</p></li>
<li><p>understanding key processing steps and terms of the first building block for training any language model – the training data.</p></li>
</ul>
<p><strong>Please try to complete the first block of this tutorial sheet (i.e., installation of requirements) AHEAD of the tutorial session</strong>, ideally, while you have a stable internet connection. This way we can try to solve any problems that might have come up with the installation during the tutorial on Friday.</p>
<section id="installing-requirements">
<h2>Installing requirements<a class="headerlink" href="#installing-requirements" title="Permalink to this heading">#</a></h2>
<p>Throughout the semester, we will use Python, PyTorch and various packages for practical work. Both the in-tutorial exercise sheets and homework will require you to execute Python code yourself.
Please follow the steps below to set up the requirements (i.e., most packages required for completing exercises) that we will use in the course. We will most likely install more packages as we go during the semester, though.</p>
<p>You can do so either on your own machine, or by using <a class="reference external" href="https://colab.research.google.com/">Google Colab</a>. You can easily access the latter option by pressing the Colab icon at the top of the webook’s page. Depending on your choice, please follow the respective requirement installation steps below.</p>
<section id="colab">
<h3>Colab<a class="headerlink" href="#colab" title="Permalink to this heading">#</a></h3>
<p>The advantage of using Colab is that you don’t need to install software on your own machine; i.e., it is a safer option if you are not very comfortable with using Python on your own machine. Colab is a  platform provided by Google for free, and it also provides limited access to GPU computation (which will be useful for working with language models). Using it only requires a Google account.</p>
<p>For using a GPU on Colab, before executing your code, navigate to Runtime &gt; Change runtime type &gt; GPU &gt; Save. Please note that the provided Colab computational resources are free, so please be mindful when using them. Further, Colab monitors GPU usage, so if it is used a lot very frequently, the user might not be able to access GPU run times for a while.</p>
<p>Colab already provides Python as well as a number of basic packages. If you choose to use it, you will only need to install the more specific packages. Note that you will have to so <em>every time</em> you open a new Colab runtime. To test that you can access requirements for the class, please open this notebook in Colab (see above), uncomment and run the following line:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># !pip install datasets langchain torchrl llama-index bertviz wikipedia</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="local-installation">
<h3>Local installation<a class="headerlink" href="#local-installation" title="Permalink to this heading">#</a></h3>
<p>Using your computer for local execution of all practical exercises might be a more advanced option. If you do so, we strongly encourage you to create an environment (e.g., with Conda) before installing any packages. Furthermore, ideally, check if you have a GPU suitable for deep learning because using a GPU will significantly speed up the work with language models. You can do so by checking your computer specs and finding out whether your GPU works with CUDA, MPS or ROCm. If you don’t have a suitable GPU, you can use Colab for tasks that require GPU access. Finally, please note that we will download some pretrained models and some datasets which will occupy some of your local storage.</p>
<p>If you choose to use your own machine, please do the following steps:</p>
<ul class="simple">
<li><p>install Python &gt;= 3.9</p></li>
<li><p>create an environment (optional but recommended)</p></li>
<li><p>download the requirements file <a class="reference external" href="https://github.com/CogSciPrag/Understanding-LLMs-course/tree/main/understanding-llms/tutorials/files/requirements.txt">here</a></p></li>
<li><p>if you have a deep learning supporting GPU:</p>
<ul>
<li><p>please check <a class="reference external" href="https://pytorch.org/get-started/locally/">here</a> which PyTorch version you need in order to use the GPU</p></li>
<li><p>please modify the first line of the requirements file to reflect the PyTorch version suitable for your machine (if needed)</p></li>
<li><p>please install the requirements from the requirements file (e.g., run: <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">-r</span> <span class="pre">requirements.txt</span></code> once pip is available in your environment; adjust path to file if needed)</p></li>
</ul>
</li>
<li><p>if you do NOT have a deep learning supporting GPU:</p>
<ul>
<li><p>please install the requirements from the requirements file (e.g., run: <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">-r</span> <span class="pre">requirements.txt</span></code> once pip is available in your environment; adjust path to file if needed)</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="verifying-requirement-installation">
<h2>Verifying requirement installation<a class="headerlink" href="#verifying-requirement-installation" title="Permalink to this heading">#</a></h2>
<p>Please run the following code cells to make sure that the key requirements were installed successfully. If errors occur and you cannot solve them ahead of the tutorial, please don’t be shy and let us know in the first tutorial!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># import packages</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>
<span class="kn">from</span> <span class="nn">langchain.tools</span> <span class="kn">import</span> <span class="n">WikipediaQueryRun</span>
<span class="kn">from</span> <span class="nn">langchain_community.utilities</span> <span class="kn">import</span> <span class="n">WikipediaAPIWrapper</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># check available computation device</span>
<span class="c1"># if you have a local GPU or if you are using a GPU on Colab, the following code should return &quot;CUDA&quot;</span>
<span class="c1"># if you are on Mac and have an &gt; M1 chip, the following code should return &quot;MPS&quot;</span>
<span class="c1"># otherwise, it should return &quot;CPU&quot;</span>

<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Device: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">elif</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;mps&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Device: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Device: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># test PyTorch</span>

<span class="c1"># randomly initialize a tensor of shape (5, 3)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shape of tensor x:&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Device of tensor x:&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># initialize a tensor of shape (5, 3) with ones</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># multiply x and y in a pointwise manner</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span>
<span class="nb">print</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

<span class="c1"># note that to do a matrix multiplication between x and y, we need to transpose</span>
<span class="c1"># either matrix x or y because the inner dimensions must match</span>
<span class="n">z1</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">y</span><span class="o">.</span><span class="n">T</span>
<span class="nb">print</span><span class="p">(</span><span class="n">z1</span><span class="p">)</span>

<span class="c1"># or:</span>
<span class="n">z2</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span>
<span class="nb">print</span><span class="p">(</span><span class="n">z2</span><span class="p">)</span>
<span class="c1"># and note that the result of z1 and z2 are not the same and have different shapes</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># testing LangChain</span>

<span class="c1"># run a Wikipedia query, searching for the article &quot;Attention is all you need&quot;</span>
<span class="c1"># NB: requires an internet connection</span>
<span class="n">wikipedia</span> <span class="o">=</span> <span class="n">WikipediaQueryRun</span><span class="p">(</span><span class="n">api_wrapper</span><span class="o">=</span><span class="n">WikipediaAPIWrapper</span><span class="p">())</span>
<span class="n">wikipedia</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="s2">&quot;Attention is all you need&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># testing the package transformers which provides pre-trained language models</span>
<span class="c1"># and excellent infrastructure around them</span>

<span class="c1"># download (if not available yet) and load GPT-2 tokenizer</span>
<span class="n">tokenizer_gpt2</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
<span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;Attention is all you need&quot;</span>
<span class="c1"># tokenize the text (i.e., convert the string into a tensor of token IDs)</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer_gpt2</span><span class="p">(</span>
    <span class="n">text</span><span class="p">,</span>
    <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span>
<span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Input IDs:&quot;</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="best-practices-for-writing-code">
<h2>Best practices for writing code<a class="headerlink" href="#best-practices-for-writing-code" title="Permalink to this heading">#</a></h2>
<p>There is a lot of debate around best practices for writing, documenting and formatting Python code and their actual implementation in daily practice, and many people have different personal preferences. We are not committing to a particular side in this debate, but we do care about a few general aspects:</p>
<ul class="simple">
<li><p>working with clean code</p></li>
<li><p>working with understandable code (i.e., commented, with understandable variable names etc)</p></li>
<li><p>producing well-documented projects (e.g., supplied with relevant READMEs etc). Think: your work should be structured such that you could look at it in a year and be able to immediately know what you did, how and why.</p></li>
</ul>
<p>There are a few de facto standard <em>formatting</em> practices that help to keep Python code crisp and clean. Please take a look at these and adhere to these as much as you can (as so will we):</p>
<ul class="simple">
<li><p><a class="reference external" href="https://pep8.org/">PEP8</a>: style guide for Python code defining e.g., variable naming conventions, how many spaces to use for indentation, how long single lines should be etc.</p>
<ul>
<li><p>Here is an overview <a class="reference external" href="https://www.youtube.com/watch?v=D4_s3q038I0">video</a> of some of the PEP8 conventions</p></li>
<li><p>There is handy software that reformats your code for you according to some of these conventions. Such software is often seamlessly integrated in IDEs. This includes for instance <em>Black</em> or <em>Ruff</em> Python formatters. They can be installed as extensions in, e.g., Visual Studio Code.</p></li>
</ul>
</li>
<li><p><em>docstrings</em> are comments (strings) that document a specific code object and always directly follow the definition of the object (e.g., directly after <code class="docutils literal notranslate"><span class="pre">def</span> <span class="pre">fct(...)</span></code>). They specify the functionality, inputs, outputs and their types. Again, there are slightly different formatting styles for docstrings; please try to be consistent about your formatting.</p>
<ul>
<li><p>One example style of docstrings is <a class="reference external" href="https://numpydoc.readthedocs.io/en/latest/format.html#docstring-standard"><em>numpydoc</em></a>; you might see that the provided code might often use such docstrings.</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># example: bad formatting</span>
<span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span><span class="o">+</span><span class="n">b</span>

<span class="c1"># example: better formatting</span>
<span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>

<span class="c1"># example: bad docstring</span>

<span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;a+b&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>

<span class="c1"># example: better docstring</span>
<span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Add two numbers.</span>

<span class="sd">    Args</span>
<span class="sd">    ----</span>
<span class="sd">    a: int</span>
<span class="sd">        First number.</span>
<span class="sd">    b: int</span>
<span class="sd">        Second number.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    int: Sum of a and b.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
</pre></div>
</div>
</div>
</div>
<p>There are also some hints regarding structuring larger projects and e.g. GitHub repositories (just fyi):</p>
<ul class="simple">
<li><p><a class="reference external" href="https://djnavarro.net/slides-project-structure/#1">project structure</a></p></li>
<li><p><a class="reference external" href="https://www.freecodecamp.org/news/how-to-write-a-good-readme-file/">writing good READMEs</a></p></li>
<li><p><a class="reference external" href="https://vimeo.com/412835411">tidy collaboration and git</a></p></li>
</ul>
<p>These best practices will be useful to you beyond this class and possibly even beyond your studies when collaborating on other coding projects within teams or even by yourself. We do our best to stick to these guidelines ourselves and kindly urge you to do the same when submitting assignments and possibly projects.</p>
</section>
<section id="understanding-training-data">
<h2>Understanding training data<a class="headerlink" href="#understanding-training-data" title="Permalink to this heading">#</a></h2>
<p>One of the critical building blocks of any language model (be it an n-gram model or GPT-4) is the <strong>training data</strong>. The contents of the training data determine, for instance, which tokens (e.g., words) the model “sees” during training, how often each of them occurs, which language the model learns, but also which potential <em>biases</em> the model might inherit (more on this in lecture 9).</p>
<p>The goals of this part of the sheet are:</p>
<ul class="simple">
<li><p>introduce core terms and concepts that we might be using throughout the class, and that are often used in NLP papers</p></li>
<li><p>understand core data processing steps used before training LMs</p></li>
<li><p>try hands-on loading a dataset and performing basic preprocessing steps</p></li>
</ul>
<p>Tasks:</p>
<ul class="simple">
<li><p>read the sections below, try to understand each concept and ask yourself whether you have already heard it, and if so, in which context</p></li>
<li><p>complete the exercises</p></li>
<li><p>complete the coding exercises where you can load and process a dataset yourself.</p></li>
</ul>
<section id="core-concepts">
<h3>Core concepts<a class="headerlink" href="#core-concepts" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>(training) data / dataset</strong> (in the context of LMs): a collection of text data which is used as input to the LM in order to optimize its parameters, so that, ideally, the model learns to perform well on its target task; that is, to predict fluent text. Anything from a single sentence to a book can be considered data; but since learning statistics of natural language is very difficult, usually very large collections of texts (i.e., very large datasets) are used to train LMs. Generalization to other machine learning models: the type of input data might be different (e.g., images and labels for image classification models) but the purpose is the same. Data and dataset are mostly used interchangeably.</p>
<ul>
<li><p><strong>corpus</strong> [ling.]: “A corpus is a collection of pieces of language text in electronic form, selected according to external criteria to represent, as far as possible, a language or language variety as a source of data for linguistic research.” <a class="reference external" href="https://user.phil.hhu.de/~bontcheva/SS10CTCL/CTCL-IntroNotes.pdf">source</a> For the purposes of NLP, the term corpus is often used interchangeably with the term dataset, especially when referring to collections of literary texts (e.g., the Books corpus) or when sourced from corpora created in linguistics.</p>
<ul>
<li><p>well-known linguistic corpora are, e.g.: the <a class="reference external" href="http://icame.uib.no/brown/bcm.html">Brown corpus</a>, the British National Corpus <a class="reference external" href="http://www.natcorp.ox.ac.uk/">BNC</a>.</p></li>
</ul>
</li>
<li><p><strong>test / validation data</strong> (general ML concept): the full dataset is sually split into the <em>training data</em> (used to optimize the model), and the held-out <em>validation data</em> and <em>test data</em> (called dataset splits). Validation data is often used to optimize aspects of the model architecture (so-called hyperparameters like optimizer, drop out rate etc). This split is sometimes ommited if no hyperparameter tuning is done. Test data is then used to assess the model’s performance on <em>unseen</em> data. That is, it is used to approximately answer the question: How well will my trained model perform on completely new inputs? In the context of LMs, all dataset splits are texts.</p></li>
</ul>
</li>
<li><p><strong>cleaning &amp; preprocessing</strong>: this is the step when “raw” data (e.g., from the web) is processed so as to massage the data into a format that is optimal for the NLP task which we want to accomplish. This can include, for instance, removing markup tags, lower-casing data, splitting it into single sentences etc.</p></li>
<li><p><strong>annotation</strong>: this step refers to enriching “raw” data with additional information like judgements about the quality of data, “gold standard” demonstrations of a task (e.g., gold standard answer to a question) etc, usually provided by humans. This is done generate high-quality training datasets which cannot be obtained otherwise.</p>
<ul>
<li><p>most prominently, human annotation is often used in the process of fine-tuning LLMs with RLHF (more on this in lecture 5).</p></li>
</ul>
</li>
<li><p><strong>token</strong>: minimal unit of text which is mapped onto a numerical representation to be readable for the LM. Different types of tokens have been used: single words, single characters, and recently mostly sub-word parts (and most most recently some experiments with tokens that are large multi-word chunks). Note that unique minimal units are assigned different tokens; whenever such a unit occurs in a particular context, the same numerical representation (i.e., token ID) is assigned to that unit. Therefore, the notion of a token in NLP is not completely equivalent to the notion in lingusitics (and there are no types in NLP as opposed to linguistics).</p>
<ul>
<li><p>tokenization is the process of converting a string to a list or tensor of tokens.</p></li>
<li><p>part of tokenization for training transformers is also creating <em>attention masks</em> which “mask” certain tokens for the model (i.e., hide it from the model during training). This is done to train models to predict next words based only on preceding context.</p></li>
<li><p>tokenization will be discussed in more detail in the session of week 3.</p></li>
</ul>
</li>
<li><p><strong>vocabulary</strong>: the set of unique tokens used by a particular LM-tookenizer pair. For example, in case of the Llama-2 model, the vocabulary consists of ~32 000 tokens.</p></li>
<li><p><strong>embedding</strong>: a vector representation of a single token (e.g., word2vec). These vector representations are learned in a way optimizing the next token prediction task and, intuitively, can be understood as approximating (some aspects of) the meaning of a word.</p></li>
<li><p><strong>batch</strong>: a set of input samples (i.e., texts) that is passed through the LM during training simultaneously, in parallel, during one training step, before updating the internal model parameters. The <strong>batch size</strong> refers to the number of input samples in the set. The batch size is a common hyperparameter of the LM architectures and might have a significant effect on set-up requirements (a large batch size requires a lot of memory) and the model performance (because model parameters are updated based on the training signal from the entire batch).</p></li>
<li><p><strong>epoch</strong>: an interation over the entire training dataset. Often a model is trained for several epochs, i.e., training iterates over the training set several times.</p></li>
</ul>
<p>We will likely extend this list and learn about more important aspects as we go on with the class, but this should already equip you with a good foundation for understanding the parts of the LM literature related to data.</p>
</section>
<section id="main-training-data-processing-steps">
<h3>Main training data processing steps<a class="headerlink" href="#main-training-data-processing-steps" title="Permalink to this heading">#</a></h3>
<p>Before beginning to train the LM, the following steps are commonly completed:</p>
<ol class="arabic simple">
<li><p>acquiring the training data: this step involves downloading or collecting the data of your choice onto the machine which will be used for training. Nowadays many datasets for various tasks can be downloaded from <a class="reference external" href="https://huggingface.co/datasets">HuggingFace</a> or are made available in GitHub repositories.</p></li>
<li><p>exploring and understanding the dataset: it is important to understand what kinds of texts, from which sources, on which topics, with what sentence length, … the dataset contains. This is crucial because the model will pick up on features of the dataset in a way that might be difficult to fully anticipate (which is good if the features are, e.g., gramamticality of sentences, but bad if it is toxic language).</p></li>
<li><p>creating the desired combination: nowadays training datasets might consist of a mix of different smaller datasets. See the exercise below for more details.</p></li>
<li><p>cleaning: this step involves filtering out or converting non-machine readable or undesired characters, often lower-casing, removal of punctuation or digits or so-called stop-words (very common words like “a”, “and”). However, the last three steps are not very common any more for state-of-the-art LLM training.</p>
<ul class="simple">
<li><p>Specifically, the last cleaning steps have been introduced for tasks like, e.g., sentiment classification. Arguably, stopwords and punctuation etc probably don’t contribute much task-relevant information, but might rather introduce encoding difficulties. On the other hand, for natural language generation, punctuation, all words as well as other symbols like emojis carry valuable information. Therefore, such preprocessing is not common for LM training.</p></li>
</ul>
</li>
<li><p>splitting the dataset into train, validation, test splits</p></li>
<li><p>prepairing the training split: training texts are often shuffled and sometimes split into shorter texts. Specifically, splitting is required if the length of a text exceeds the maximal <em>context window size</em> of the transformer model (i.e., the maximal number of tokens a model can process). In this case, texts are often split into shorter slightly overlapping chunks.</p></li>
<li><p>tokenizing: converting the single texts into lists of tokens, i.e., into lists of numerical IDs. More on tokenization in the session of week 3.</p></li>
<li><p>batching: to speed up training, the model is often fed multiple texts at the same time (i.e., at each training step). To create these batches, often additional steps are needed to ensure that several tokenized texts (i.e., several lists with token IDs) can be represented as one input tensor. These steps are either restricting texts to a maximal common length (and cutting off the rest) or <em>padding</em> all the texts to the same length. More on this in the tokenization session.</p></li>
</ol>
<p><a class="reference external" href="https://www.geeksforgeeks.org/natural-language-processing-nlp-pipeline/">This article</a> provides a great and more detailed overview of the steps 1-4 and provides insights into traditional approaches (e.g., feature engineering) which are more common for task-specific models than for foundation language models.</p>
<blockquote>
<div><p><strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 1.1.: Massaging a Twitter dataset</span></strong></p>
<p>Below are a few code blocks for implementing some data processing steps on an example dataset of <a class="reference external" href="https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment">tweets about financial news</a>, downloaded from HuggingFace. We will use the <code class="docutils literal notranslate"><span class="pre">datasets</span></code> <a class="reference external" href="https://huggingface.co/docs/datasets/en/tutorial">package</a> to work with the dataset. Originally, the dataset is intended for sentiment classification, but we will just use the tweets from the column “text”.</p>
<ol class="arabic simple">
<li><p>Please go through the code and complete it in slots which say “#### YOUR CODE HERE”. Refer to the comments and hints for instructions about what the code is supposed to do. Make sure to try to understand every line!</p></li>
<li><p>What is prominent about the dataset? Are the processing steps adequate if you wanted to train a Twitter bot which could write tweets on this data? Specifically, do you think the resulting cleaned sentences are sensible, good training examples to learn to generate text, and, more specifically, tweets?</p></li>
</ol>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># 1. load dataset, only training split</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span>
    <span class="s2">&quot;zeroshot/twitter-financial-news-sentiment&quot;</span><span class="p">,</span>
    <span class="n">split</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># 2. understand dataset</span>
<span class="c1"># print first 5 examples</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>

<span class="c1"># print the columns of the dataset</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">column_names</span><span class="p">)</span>

<span class="c1"># get the number of examples in the dataset</span>
<span class="n">dataset_size</span> <span class="o">=</span> <span class="c1">### YOUR CODE HERE ###</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Dataset size: </span><span class="si">{</span><span class="n">dataset_size</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># compute the tweet lengths (in words, i.e., split by whitespace)</span>
<span class="c1"># plot them and compute the average tweet length</span>
<span class="n">tweets</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span>
<span class="n">tweet_lengths</span> <span class="o">=</span> <span class="c1">### YOUR CODE HERE ###</span>
<span class="n">average_tweet_length</span> <span class="o">=</span> <span class="c1">### YOUR CODE HERE ###</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Average tweet length: </span><span class="si">{</span><span class="n">average_tweet_length</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># plot a histogram of the tweet lengths</span>
<span class="c1">### YOUR CODE HERE ###</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Tweet length&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Click below to see the solution.</p>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># 1. load dataset, only training split</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span>
    <span class="s2">&quot;zeroshot/twitter-financial-news-sentiment&quot;</span><span class="p">,</span>
    <span class="n">split</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># 2. understand dataset</span>
<span class="c1"># print first 5 examples</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>

<span class="c1"># print the columns of the dataset</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">column_names</span><span class="p">)</span>

<span class="c1"># get the number of examples in the dataset</span>
<span class="n">dataset_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span> <span class="c1">### YOUR CODE HERE ###</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Dataset size: </span><span class="si">{</span><span class="n">dataset_size</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># compute the tweet lengths (in words, i.e., split by whitespace)</span>
<span class="c1"># plot them and compute the average tweet length</span>
<span class="n">tweets</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span>
<span class="n">tweet_lengths</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">tweet</span><span class="o">.</span><span class="n">split</span><span class="p">())</span> <span class="k">for</span> <span class="n">tweet</span> <span class="ow">in</span> <span class="n">tweets</span><span class="p">]</span> <span class="c1">### YOUR CODE HERE ###</span>
<span class="n">average_tweet_length</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">tweet_lengths</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">tweet_lengths</span><span class="p">)</span> <span class="c1">### YOUR CODE HERE ###</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Average tweet length: </span><span class="si">{</span><span class="n">average_tweet_length</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># plot the tweet lengths</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">tweet_lengths</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span> <span class="c1">### YOUR CODE HERE ###</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Tweet length&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/anaconda3/envs/understanding_llms/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;text&#39;: [&#39;$BYND - JPMorgan reels in expectations on Beyond Meat https://t.co/bd0xbFGjkT&#39;, &#39;$CCL $RCL - Nomura points to bookings weakness at Carnival and Royal Caribbean https://t.co/yGjpT2ReD3&#39;, &#39;$CX - Cemex cut at Credit Suisse, J.P. Morgan on weak building outlook https://t.co/KN1g4AWFIb&#39;, &#39;$ESS: BTIG Research cuts to Neutral https://t.co/MCyfTsXc2N&#39;, &#39;$FNKO - Funko slides after Piper Jaffray PT cut https://t.co/z37IJmCQzB&#39;], &#39;label&#39;: [0, 0, 0, 0, 0]}
[&#39;text&#39;, &#39;label&#39;]
Dataset size: 9543
Average tweet length: 12.17835062349366
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 0, &#39;Tweet length&#39;)
</pre></div>
</div>
<img alt="../_images/54bebe0bc64b73c0e351b077dc07b2284faab5835d922c79b8cfbb0f975b06d0.png" src="../_images/54bebe0bc64b73c0e351b077dc07b2284faab5835d922c79b8cfbb0f975b06d0.png" />
</div>
</details>
</div>
<div class="toggle docutils container">
<blockquote>
<div><p><strong><span style=&ldquo;color:#D83D2B;&rdquo;>Answers Exercise 1.1.: Massaging a Twitter dataset</span></strong></p>
<ol class="arabic simple" start="2">
<li><p>The Tweet lenght peaks at 10 tokens, followed by between 10 and 15 tokens. The tweet lenght is almost never larger then 25 tokens. The Average tweet length is 12.17835062349366. This is quite small for tweets.
So if you would train your Twitter bot with this dataset you would receive a bot that answers in very short sentences. If that is what you are looking for, the dataset would be appropiate.</p></li>
</ol>
</div></blockquote>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 4. clean tweets: remove non-alphabetic or non-space characters</span>
<span class="c1"># Hint: you can easily google how to remove non-alphabetic characters in Python</span>


<span class="k">def</span> <span class="nf">clean_tweet</span><span class="p">(</span><span class="n">tweet</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Remove non-alphabetic or non-space characters from a tweet.</span>

<span class="sd">    Args</span>
<span class="sd">    ----</span>
<span class="sd">    tweet: str</span>
<span class="sd">        Tweet to clean.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    cleaned_tweet: str</span>
<span class="sd">        Cleaned tweet without non-alphabetic symbols.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">tweet</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
        <span class="c1">### YOUR CODE HERE ###</span>
    <span class="p">)</span>  
    <span class="k">return</span> <span class="n">tweet</span>

<span class="c1"># apply the preprocessing function to all tweets</span>
<span class="n">cleaned_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="n">example</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="n">clean_tweet</span><span class="p">(</span><span class="n">example</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">])</span>
    <span class="p">}</span>
<span class="p">)</span>

<span class="c1"># look at a few examples of clean tweets</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cleaned_dataset</span><span class="p">[:</span><span class="mi">2</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Click below to see the solution.</p>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 4. clean tweets: remove non-alphabetic characters</span>
<span class="c1"># Hint: you can easily google how to remove non-alphabetic characters in Python</span>


<span class="k">def</span> <span class="nf">clean_tweet</span><span class="p">(</span><span class="n">tweet</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Remove non-alphabetic or non-space characters from a tweet.</span>

<span class="sd">    Args</span>
<span class="sd">    ----</span>
<span class="sd">    tweet: str</span>
<span class="sd">        Tweet to clean.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    cleaned_tweet: str</span>
<span class="sd">        Cleaned tweet without non-alphabetic symbols.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">tweet</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
        <span class="p">[</span><span class="n">char</span> <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="n">tweet</span> <span class="k">if</span> <span class="n">char</span><span class="o">.</span><span class="n">isalpha</span><span class="p">()</span> <span class="ow">or</span> <span class="n">char</span><span class="o">.</span><span class="n">isspace</span><span class="p">()]</span>
    <span class="p">)</span>  <span class="c1">### YOUR CODE HERE ###</span>
    <span class="k">return</span> <span class="n">tweet</span>

<span class="c1"># apply the preprocessing function to all tweets</span>
<span class="n">cleaned_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="n">example</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="n">clean_tweet</span><span class="p">(</span><span class="n">example</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">])</span>
    <span class="p">}</span>
<span class="p">)</span>

<span class="c1"># look at a few examples of clean tweets</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cleaned_dataset</span><span class="p">[:</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 5. split dataset into training and testing set</span>

<span class="c1"># select the proportion of the dataset that should be used for training</span>
<span class="c1"># and the proportion that should be used for testing</span>
<span class="c1"># commonly train : test is around 80:20</span>
<span class="n">train_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.8</span> <span class="o">*</span> <span class="n">dataset_size</span><span class="p">)</span>  <span class="c1">### YOUR CODE HERE ###</span>
<span class="n">test_size</span> <span class="o">=</span> <span class="c1">### YOUR CODE HERE ###</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Train size: </span><span class="si">{</span><span class="n">train_size</span><span class="si">}</span><span class="s2">, Test size: </span><span class="si">{</span><span class="n">test_size</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># split the dataset into training and testing set</span>
<span class="c1"># this will create two new sub-datasets with the keys &quot;train&quot; and &quot;test&quot;</span>
<span class="n">cleaned_dataset_split</span> <span class="o">=</span> <span class="n">cleaned_dataset</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">test_size</span><span class="o">=</span><span class="n">test_size</span><span class="p">,</span>  
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Train split examples: &quot;</span><span class="p">,</span> <span class="n">cleaned_dataset_split</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">][:</span><span class="mi">3</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test split examples: &quot;</span><span class="p">,</span> <span class="n">cleaned_dataset_split</span><span class="p">[</span><span class="s2">&quot;test&quot;</span><span class="p">][:</span><span class="mi">3</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Click below to see the solution.</p>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 5. split dataset into training and testing set</span>

<span class="c1"># select the proportion of the dataset that should be used for training</span>
<span class="c1"># and the proportion that should be used for testing</span>
<span class="c1"># commonly train : test is around 80:20</span>
<span class="n">train_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.8</span> <span class="o">*</span> <span class="n">dataset_size</span><span class="p">)</span>  <span class="c1">### YOUR CODE HERE ###</span>
<span class="n">test_size</span> <span class="o">=</span> <span class="n">dataset_size</span> <span class="o">-</span> <span class="n">train_size</span>  <span class="c1">### YOUR CODE HERE ###</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Train size: </span><span class="si">{</span><span class="n">train_size</span><span class="si">}</span><span class="s2">, Test size: </span><span class="si">{</span><span class="n">test_size</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># split the dataset into training and testing set</span>
<span class="c1"># this will create two new sub-datasets with the keys &quot;train&quot; and &quot;test&quot;</span>
<span class="n">cleaned_dataset_split</span> <span class="o">=</span> <span class="n">cleaned_dataset</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">test_size</span><span class="o">=</span><span class="n">test_size</span><span class="p">,</span>  
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Train split examples: &quot;</span><span class="p">,</span> <span class="n">cleaned_dataset_split</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">][:</span><span class="mi">3</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test split examples: &quot;</span><span class="p">,</span> <span class="n">cleaned_dataset_split</span><span class="p">[</span><span class="s2">&quot;test&quot;</span><span class="p">][:</span><span class="mi">3</span><span class="p">])</span>
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 7-8. Tokenize and batch the dataset with wrappers provided by the datasets package</span>
<span class="c1"># for tokeinization, we use the GPT-2 tokenizer (more details for what is going on</span>
<span class="c1"># under the hood of these wrappers is to come in the next sessions)</span>

<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">tokenization</span><span class="p">(</span><span class="n">example</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Wrapper around the tokenizer to tokenize the text of an example.</span>

<span class="sd">    Args</span>
<span class="sd">    ----</span>
<span class="sd">    example: dict</span>
<span class="sd">        Example tweet from the dataset. Key &quot;text&quot; contains the tweet.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict</span>
<span class="sd">        Tokenized tweet with token IDs and an attention mask.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="c1">### YOUR CODE HERE ###</span>
        <span class="p">)</span>

<span class="c1"># apply the tokenization function to the train dataset</span>
<span class="n">preprocessed_train_dataset</span> <span class="o">=</span> <span class="n">cleaned_dataset_split</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenization</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># datasets provides a handy method to format the dataset for training models with PyTorch</span>
<span class="c1"># specifically, it makes sure that dataset samples that are loaded from the</span>
<span class="c1"># dataset are PyTorch tensors. It also selects columns to be used.</span>
<span class="n">preprocessed_train_dataset</span><span class="o">.</span><span class="n">set_format</span><span class="p">(</span>
    <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;torch&quot;</span><span class="p">,</span> 
    <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">,</span> <span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="s2">&quot;label&quot;</span><span class="p">]</span>
<span class="p">)</span>

<span class="n">preprocessed_train_dataset</span><span class="o">.</span><span class="n">format</span><span class="p">[</span><span class="s1">&#39;type&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Click below to see the solution.</p>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 7-8. Tokenize and batch the dataset with wrappers provided by the datasets package</span>
<span class="c1"># for tokeinization, we use the GPT-2 tokenizer (more details for what is going on</span>
<span class="c1"># under the hood of these wrappers is to come in the next sessions)</span>

<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">tokenization</span><span class="p">(</span><span class="n">example</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Wrapper around the tokenizer to tokenize the text of an example.</span>

<span class="sd">    Args</span>
<span class="sd">    ----</span>
<span class="sd">    example: dict</span>
<span class="sd">        Example tweet from the dataset.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict</span>
<span class="sd">        Tokenized tweet with token IDs and an attention mask.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">example</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">])</span>

<span class="c1"># apply the tokenization function to the train dataset</span>
<span class="n">preprocessed_train_dataset</span> <span class="o">=</span> <span class="n">cleaned_dataset_split</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenization</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># </span>
<span class="n">preprocessed_train_dataset</span><span class="o">.</span><span class="n">set_format</span><span class="p">(</span>
    <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;torch&quot;</span><span class="p">,</span> 
    <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">,</span> <span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="s2">&quot;label&quot;</span><span class="p">]</span>
<span class="p">)</span>
<span class="n">preprocessed_train_dataset</span><span class="o">.</span><span class="n">format</span><span class="p">[</span><span class="s1">&#39;type&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># finally, to see what the preprocessed dataset looks like </span>
<span class="c1"># we iterate over the dataset for a few steps, as we would do in training</span>
<span class="c1"># note: usually a DataLoader would be used to iterate over the dataset for training</span>
<span class="c1"># we will cover this in the next sessions</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">preprocessed_train_dataset</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p><strong>NOTE</strong>: if you are building your own dataset instead of e.g. loading it via <code class="docutils literal notranslate"><span class="pre">datasets</span></code>, PyTorch provides a class <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> which is easily customizable and essentially allows to explicitly implement functionality that is tucked away in the <code class="docutils literal notranslate"><span class="pre">datasets</span></code> package. Working with it is covered in sheet 2.3 (for next week!).</p>
<blockquote>
<div><p><strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 1.2.: Understanding The Pile</span></strong></p>
<p>To make things more specific, consider <a class="reference external" href="https://arxiv.org/pdf/2101.00027.pdf">The Pile dataset (Gao et al., 2020)</a>. Read trough the abstract and section 1 (Introduction), look at Table 1 (if needed, glimpse at other sections describing what the single names stand for), read section 5.2.
The following exercises are meant to foster understanding and critical thinking about training datasets. Please try to answer the following questions to yourself:</p>
<ol class="arabic simple">
<li><p>Which language(s) does The Pile mainly consist of? If an LM is trained on The Pile as it is, how would you expect the LM will perform when completing a text in, e.g., Hungarian?</p></li>
<li><p>What is the difference between The Pile and Common Crawl? Why was The Pile introduced?</p></li>
<li><p>What does the “epochs” column in Table 1 refer to? What is the idea behind it?</p></li>
<li><p>What kind of data is missing from the mix reported in Table 1? What do you think the effect of adding such data would be on an LM trained with the data?</p></li>
</ol>
</div></blockquote>
<p>Click below to see the solution.</p>
<div class="toggle docutils container">
<p><strong><span style=&ldquo;color:#D83D2B;&rdquo;>Answers Exercise 1.2.: Understanding The Pile</span></strong></p>
<blockquote>
<div><ol class="arabic simple">
<li><p>The majority of the data is english. Additionally is consists in 14 languages. Nonetheless, the main criteria for multilanguage data was the existence of english data. Therefore Hungarian text completion is probably not possible.</p></li>
<li><p>Common crawl consists in a large amount of raw data, which can result in a poor quality. Because it is favourable to uses a mixtures of many diverse datasets of high quality and from different fields, the Pile was introduced. Furthermore common crawl is part if The Pile changed into PIle-CC while using jusText on Web Archive files for extraction to receive a higher quality.</p></li>
<li><p>Epochs are the number of passes over each constituent dataset during a full epoch over the Pile. Therefore smaller datasets can pass the dataset more often the large datasets resulting in a better diversion.</p></li>
<li><p>The standard diviation of the document could give inforation whether the dataset has strong outliers. It can give an impression whether the dataset is biased in some way or not.</p></li>
</ol>
</div></blockquote>
</div>
</section>
<section id="dataset-documentation">
<h3>Dataset documentation<a class="headerlink" href="#dataset-documentation" title="Permalink to this heading">#</a></h3>
<p>Although datasets are a crucial part of the NLP pipeline, unfortunately, there are very few or no established practices for <em>documenting</em> shared datasets or <em>reporting</em> the datasets which are used to traing published models. This results in issues of reproducibility of the training because details about the data are unknown, biases of models due to under- or misrepresentation in the data and other issues. This paper (a completely optional read) provides an overview as well as suggestions for improving the situation in the area of machine learning:</p>
<p><a class="reference external" href="https://dl.acm.org/doi/pdf/10.1145/3351095.3372829">Jo &amp; Gebru (2020). Lessons from Archives: Strategies for Collecting Sociocultural Data in Machine Learning</a></p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "CogSciPrag/Understanding-LLMs-course",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./tutorials"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../lectures/01-introduction.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Background</p>
      </div>
    </a>
    <a class="right-next"
       href="../lectures/02-torch-ANNs-RNNs.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">PyTorch, ANNs &amp; LMs</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#installing-requirements">Installing requirements</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#colab">Colab</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#local-installation">Local installation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#verifying-requirement-installation">Verifying requirement installation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#best-practices-for-writing-code">Best practices for writing code</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-training-data">Understanding training data</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#core-concepts">Core concepts</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#main-training-data-processing-steps">Main training data processing steps</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-documentation">Dataset documentation</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Michael Franke, Carsten Eickhoff, Polina Tsvilodub
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>