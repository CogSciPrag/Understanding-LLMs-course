

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Sheet 3.1: Tokenization &amp; Transformers &#8212; Understanding LLMs</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'tutorials/03a-tokenization-transformers';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Homework 1: Language models (50 points)" href="../homework/01-language-modeling.html" />
    <link rel="prev" title="LSTMs &amp; Transformers" href="../lectures/03-LSTMs-Transformers.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo-ULM-2024.png" class="logo__image only-light" alt="Understanding LLMs - Home"/>
    <script>document.write(`<img src="../_static/logo-ULM-2024.png" class="logo__image only-dark" alt="Understanding LLMs - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Course overview: Understanding LLMs
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">01 Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../lectures/01-introduction.html">Background</a></li>
<li class="toctree-l1"><a class="reference internal" href="01-introduction.html">Sheet 1.1: Practical set-up &amp; Training data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">02 ANNs, RNNs &amp; Transformers</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../lectures/02-torch-ANNs-RNNs.html">PyTorch, ANNs &amp; LMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="02a-pytorch-intro.html">Sheet 2.1: PyTorch essentials</a></li>
<li class="toctree-l1"><a class="reference internal" href="02b-MLE.html">Sheet 2.2: ML-estimation</a></li>
<li class="toctree-l1"><a class="reference internal" href="02c-MLP-pytorch.html">Sheet 2.3: Non-linear regression (MLP w/ PyTorch modules)</a></li>
<li class="toctree-l1"><a class="reference internal" href="02d-char-level-RNN.html">Sheet 2.4: Character-level sequence modeling w/ RNNs</a></li>
<li class="toctree-l1"><a class="reference internal" href="02e-intro-to-hf.html">Sheet 2.5: Introduction to HuggingFace &amp; LMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lectures/03-LSTMs-Transformers.html">LSTMs &amp; Transformers</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Sheet 3.1: Tokenization &amp; Transformers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Homework</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../homework/01-language-modeling.html">Homework 1: Language models (50 points)</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/CogSciPrag/Understanding-LLMs-course/main?urlpath=tree/understanding-llms/tutorials/03a-tokenization-transformers.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onBinder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li><a href="https://colab.research.google.com/github/CogSciPrag/Understanding-LLMs-course/blob/main/understanding-llms/tutorials/03a-tokenization-transformers.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="initThebeSBT()"
  class="btn btn-sm btn-launch-thebe dropdown-item"
  title="Launch Thebe"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="btn__text-container">Live Code</span>
</button>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/CogSciPrag/Understanding-LLMs-course" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/CogSciPrag/Understanding-LLMs-course/issues/new?title=Issue%20on%20page%20%2Ftutorials/03a-tokenization-transformers.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/tutorials/03a-tokenization-transformers.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Sheet 3.1: Tokenization & Transformers</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenization">Tokenization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bpe-tokenization">BPE tokenization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#special-tokens">Special tokens</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#outlook-optional-eos-tokens">Outlook (optional): EOS tokens</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pretrained-tokenizers">Pretrained tokenizers</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-masks">Attention masks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transformers">Transformers</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch">PyTorch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pretrained-transformers">Pretrained transformers</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#outlook-optional">Outlook (optional)</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="sheet-3-1-tokenization-transformers">
<h1>Sheet 3.1: Tokenization &amp; Transformers<a class="headerlink" href="#sheet-3-1-tokenization-transformers" title="Permalink to this heading">#</a></h1>
<p><strong>Author:</strong> Polina Tsvilodub</p>
<p>In this sheet, we will focus on two topics: <em>tokenization</em>, the process of converting raw text into single tokens which are mapped onto neural-network friendly numerical representations; and <em>transformers</em>, the architecture behind state-of-the-art language models. The learning goals for this sheet are:</p>
<ul class="simple">
<li><p>understand core steps of tokenization</p></li>
<li><p>learn to use state-of-the-art tokenization correctly</p></li>
<li><p>learn to build transformers in PyTorch</p></li>
<li><p>understand pretrained transformers like GPT-2 which can be loaded from HuggingFace.</p></li>
</ul>
<section id="tokenization">
<h2>Tokenization<a class="headerlink" href="#tokenization" title="Permalink to this heading">#</a></h2>
<p>Under the hood, neural networks are complicated functions; therefore, they cannot deal with raw text represented as strings, but uses numerical representations of the text. This conversion is done by a <em>tokenizer</em>, and the process is called <em>tokenization</em>. Tokenization commonly consists of the following steps:</p>
<ol class="arabic simple">
<li><p>splitting the input text into words, subwords, or symbols (like punctuation) that are called <em>tokens</em></p></li>
<li><p>mapping each token to an integer (or, index)</p></li>
<li><p>adding additional inputs (or, <em>special tokens</em>) that may be useful to the model</p></li>
</ol>
<p>The set of unique tokens of a given tokenizer is often called the <em>vocabulary</em>.</p>
<blockquote>
<div><p><strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 3.1.1: Simple tokenization </span></strong></p>
<p>We have seen the simplest version explicit version of tokenization in <a class="reference external" href="https://cogsciprag.github.io/Understanding-LLMs-course/tutorials/02d-char-level-RNN.html">sheet 2.4</a>. Here, the “tokenizer” is just a simple mapping.</p>
<ol class="arabic simple">
<li><p>What are the minimal units, i.e., tokens in sheet 2.4?</p></li>
<li><p>What is the range of indices representing tokens in sheet 2.4? I.e., how many different tokens are there?</p></li>
</ol>
</div></blockquote>
<p>However, this simple approach has several limitations. Specifically, the vocabulary is very limited and is manually defined a priori. While such an approach may work for simple tasks or very specific domains like predicting names, it is not very useful for dealing with more general texts which may include numbers, emojis, or different languages with different alphabets. Under this simple approach, we wouldn’t be able to represent any of these things.</p>
<p>To allow for more flexibility but avoid having to manually specify all possible tokens, special tokenization approaches have been developed. The most prominent tokenization algorithm used, e.g., for the GPT models, is the so-called <em>byte-pair-encoding</em> (BPE) tokenization. BPE tokenizers are trained, i.e., the can be adjusted on specific texts so as to optimally represent the data.</p>
<section id="bpe-tokenization">
<h3>BPE tokenization<a class="headerlink" href="#bpe-tokenization" title="Permalink to this heading">#</a></h3>
<p>Here are the core steps behind BPE tokenization:</p>
<ul class="simple">
<li><p>Text from the training corpus is normalized. That is, text undergoes general preprocessing like removing unnecessary whitespaces, lower-casing, possibly doing unicode normalization.</p></li>
<li><p>Text in the corpus is pre-tokenized. Here, the text is commonly split, e.g., into single words (i.e., by whitespace) and punctuation.</p></li>
<li><p>Next, all characters used in the preprocessed text are identified. Special tokens are added (more on these below).</p></li>
<li><p>Next, frequencies of pairs of characters in the training corpus are identified. The most frequent pair is then <em>merged</em> into a single token, and the frequencies are identified again, now using the merged representation of the identified pair. This is repeated, until a desired vocabulary size is reached.</p></li>
<li><p>After training, to tokenize a new text, the learned merge rules are applied to it and the respective tokens are assigned to the text.</p></li>
</ul>
<p>The important take-aways are:</p>
<ul class="simple">
<li><p>Tokenizers are training data-dependent. This means, what exactly is represented by single tokens depends on the freuqencies of different words and their contexts in the corpus. The size of the vocabulary is determined by the developers.</p></li>
<li><p>Same characters or word-pieces can be mapped onto different tokens, depending on their context! For instance, the same word at the beginning or in the middle of a sentence can be represented with different tokens.</p></li>
</ul>
<p>If you want to dive deeper into the algorithm, take a look at <a class="reference external" href="https://huggingface.co/learn/nlp-course/en/chapter6/5#byte-pair-encoding-tokenization">this</a> tutorial. Another common approach is WordPiece tokenization; you can learn more <a class="reference external" href="https://huggingface.co/learn/nlp-course/en/chapter6/6">here</a>.</p>
</section>
<section id="special-tokens">
<h3>Special tokens<a class="headerlink" href="#special-tokens" title="Permalink to this heading">#</a></h3>
<p>An important aspect of tokenizers are special tokens. They are called “special” because they carry special meaning rather than simply representing parts of a text. Common SOTA tokenizers have the following special tokens:</p>
<ul class="simple">
<li><p><strong>beginning-of-sequence (BOS)</strong> (or, start-of-sequence, SOS) token: it is prepended to the start of every training sequence, and at inference time, it is used to signal to the LM that text should be predicted. In a sense, it represents a “signal to act”.</p>
<ul>
<li><p>Different tokenizers use different strings as BOS. For instance, these could be “&lt;s&gt;”, “|startofsequence|” or anything thelike.</p></li>
</ul>
</li>
<li><p><strong>end-of-sequence (EOS)</strong> token: it is appended to the end of every training sequence so as to signal to the LM that a text is “finished” at a certain point. Thereby the LM learns to predict finite sequences. At inference time, once the LM samples the EOS, it stops generating further tokens.</p>
<ul>
<li><p>Different tokenizers use different strings as EOS. For instance, these could be “&lt;\s&gt;”, “|endofsequence|” or anything thelike.</p></li>
</ul>
</li>
<li><p><strong>pad</strong> token: it is used to make a sequence longer and have a certain number of tokens. This is used for training LMs on batches of sequences. Since each batch of <span class="math notranslate nohighlight">\(n\)</span> sequences is represented by a matrix with the dimensions <span class="math notranslate nohighlight">\(n \times m\)</span>, each sequence has to have the length of <span class="math notranslate nohighlight">\(m\)</span> tokens. If, in fact, it only has less than <span class="math notranslate nohighlight">\(m\)</span> tokens, we append pad tokens so that the sequence has <span class="math notranslate nohighlight">\(m\)</span> tokens.</p>
<ul>
<li><p>The pad token could be, e.g., “[PAD]”. Sometimes, however, pad tokens are not provided by a pretrained tokenizer. In this case, usually the EOS token is set as the pad token.</p></li>
<li><p>It is important to note that pad tokens are an “engineering” necessity rather than tokens carrying meaning. Therefore, these are often <em>masked</em> during training, while other special tokens are not. More on  masking below.</p></li>
<li><p>We can set the padding side for a tokenizer. It represents on which side of the sequence the pad tokens should be added. For auto-regressive models (i.e., common LMs), the padding side should be on the left. Concretely, a padded sequence should, e.g., look like this: “[PAD] [PAD] [PAD] Hi there!”</p></li>
</ul>
</li>
<li><p><strong>unknown (UNK)</strong> token: it is used to represent a character or a part of a sequence which cannot be mapped to known tokens. Under some tokenization approaches (e.g., byte level pair encoding), UNK tokens aren’t possible in principle, and therefore, such tokenizers don’t have UNK tokens.</p></li>
<li><p><strong>system</strong> tokens: these tokens have been introduced more recently with the introduction of chat-optimized and assistant LMs, and are used to delineate to an LM different types of contents. These are, for instance, special tokens which are introduced to delineate system prompts, user inputs, previous model responses, etc (more on prompting and assistants in the next lectures).</p></li>
</ul>
<p>These special tokens are added to the vocabulary of a tokenizer, i.e., represented by their own token indices.</p>
<p>The important take-away is that, in order to get optimal performance of a trained LM, one must use tokenization in the same way as what was used when the model was trained!</p>
<section id="outlook-optional-eos-tokens">
<h4>Outlook (optional): EOS tokens<a class="headerlink" href="#outlook-optional-eos-tokens" title="Permalink to this heading">#</a></h4>
<p>Next to the intuitive role of signaling to the LM that a sequence ended, EOS tokens play an important conceptual role. Specifically, it is necessary to define a set of finite strings so as to ensure that the LM represents a valid probability measure over the vocabulary (or, alphabet) (<a class="reference external" href="https://arxiv.org/pdf/2212.10502">Du et al., 2022</a>). See <a class="reference external" href="https://drive.google.com/file/d/1IYgjs0Vf8TPmVW6w4S125j3G5Asatn4f/view">these</a> lecture notes for more information.</p>
</section>
</section>
<section id="pretrained-tokenizers">
<h3>Pretrained tokenizers<a class="headerlink" href="#pretrained-tokenizers" title="Permalink to this heading">#</a></h3>
<p>In practice, as with language models, we don’t have to create tokenizers ourselves – we can download pretrained tokenizers that were created for specific LMs and are shipped with them on HF. We have already used a pretrained tokenizer in the previous sheets under the hood. Below, we take a closer look at the pretrained GPT-2 tokenizer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># load tokenizer</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>First, we inspect the configurations of the tokenizer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of tokens of GPT2 tokenizer&quot;</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;GPT2 vocabulary &quot;</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">items</span><span class="p">())[:</span><span class="mi">10</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>The core functionality of the tokenizer is to map strings to token IDs (i.e., encode), and vice versa - map lists of token IDs back to human-readable texts (i.e., decode). By calling the tokenizer, we automatically call the encoding function. Note in the example below that the same word (“the”) can be mapped onto different tokens, depending on its context.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">text1</span> <span class="o">=</span> <span class="s2">&quot;the quick brown fox jumped over the lazy dog&quot;</span>
<span class="n">enc1</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Encoded text 1: &quot;</span><span class="p">,</span> <span class="n">enc1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># we look at the single tokens and chek what strings they represent</span>
<span class="k">for</span> <span class="n">tok</span> <span class="ow">in</span> <span class="n">enc1</span><span class="o">.</span><span class="n">input_ids</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Token: &quot;</span><span class="p">,</span> <span class="n">tok</span><span class="p">,</span> <span class="s2">&quot;String: &quot;</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tok</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># note that often a single token corresponds to only a part of a word</span>
<span class="c1"># and that the same word in different languages might be represented by different tokens</span>
<span class="n">text2</span> <span class="o">=</span> <span class="s2">&quot;attention is all you need&quot;</span>
<span class="n">enc2</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Encoded text 2: &quot;</span><span class="p">,</span> <span class="n">enc2</span><span class="p">)</span>

<span class="k">for</span> <span class="n">tok</span> <span class="ow">in</span> <span class="n">enc2</span><span class="o">.</span><span class="n">input_ids</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Token: &quot;</span><span class="p">,</span> <span class="n">tok</span><span class="p">,</span> <span class="s2">&quot;String: &quot;</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tok</span><span class="p">))</span>

<span class="c1"># German text</span>
<span class="n">text3</span> <span class="o">=</span> <span class="s2">&quot;der schnelle braune Fuchs sprang über den faul Hund&quot;</span>
<span class="n">enc3</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Encoded text 3: &quot;</span><span class="p">,</span> <span class="n">enc3</span><span class="p">)</span>

<span class="k">for</span> <span class="n">tok</span> <span class="ow">in</span> <span class="n">enc3</span><span class="o">.</span><span class="n">input_ids</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Token: &quot;</span><span class="p">,</span> <span class="n">tok</span><span class="p">,</span> <span class="s2">&quot;String: &quot;</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tok</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><p><strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 3.1.2: Pretrained tokenizers</span></strong></p>
<ol class="arabic simple">
<li><p>Inspect the three tokenization related files that are loaded together with the pretrained GPT-2 <a class="reference external" href="https://huggingface.co/openai-community/gpt2/tree/main">here</a>. What do the single files contain?</p></li>
<li><p>What kind of special tokens does the GPT-2 tokenizer have? (Hint: use the HF documentation of the respective tokenizer)</p></li>
<li><p>Find out what kind of special Llama-2 chat tokens there are (look for <code class="docutils literal notranslate"><span class="pre">meta-llama/Llama-2-7b-chat-hf</span></code> on HF). How should they be used to represent the system prompt and the user input? (Hint: use the HF documentation. You can also find out more about LLama-chat prompting <a class="reference external" href="https://huggingface.co/blog/llama2#how-to-prompt-llama-2">here</a>)</p></li>
</ol>
</div></blockquote>
<p>After tokenization, each token is mapped onto an <em>embedding</em>. As discussed in the lecture, there are different types of embeddings: type-based and contextualized embeddings. In both cases, the token IDs are mapped to vector representations as the first processing step in an LM. For type-based representations, these vectors are already pretrained static embeddings. For contextualized cases, these vectores are passed through the net and only the processed vectors are considered embeddings.</p>
<p>The initial mapping is done by creating <em>one-hot vector</em> as a representation of each index. These are then multipled with the embedding matrix, which results in embedding representations of each token. These are then further processed.</p>
<section id="attention-masks">
<h4>Attention masks<a class="headerlink" href="#attention-masks" title="Permalink to this heading">#</a></h4>
<p>Note that, next to input IDs (i.e., token IDs), the tokenizer encoding returns an attention mask <code class="docutils literal notranslate"><span class="pre">attention_mask</span></code>. The attention mask has the same shape as the input IDs. It is used for training in order to allow the transformer to base its predictions only on certain parts of the input text. Specifically, for causal LMs, attention masks are needed because we can nicely parallelise the training of transformers and process an entire sequence at once (in contrast to RNN training); but we still want to base the predictions only on the preceding part of a sentence.</p>
<p>In the widespread HF implementation, the attention mask has a 1 to indicate that the transformer should <em>attend</em> to tokens at the particular positions, while a 0 indicates that the transformer should not attend to the given token. The attention mask provided by the tokenizer (or created manually by the user) is further transformed and added to the attention computations. This operation results in “zero-ing out” contributions of tokens we don’t want to attend to when computing a given token representation, essentially as if they weren’t there.</p>
<p><strong>Causal mask</strong>:
The attention mask is used to enable <em>causal</em> language modeling, i.e., to hide tokens that come <em>after</em> a token at a given position that is currently used for training. For training in a transformer in parallel over the entire sentence, the mask is created as a matrix, where each row <span class="math notranslate nohighlight">\(i\)</span> represents the correct mask for the <span class="math notranslate nohighlight">\(i\)</span>-th token. The matrix on the right in the picture shows an example causal attention mask.</p>
<p><img alt="img" src="../_images/03a-masking.png" /></p>
<p>For more details on how the forward step through a transformer, including attention mask application and positional embedding works, please carefully look at <a class="reference external" href="https://medium.com/&#64;jinoo/a-simple-example-of-attention-masking-in-transformer-decoder-a6c66757bc7d">this</a> tutorial.</p>
<p>If you are curious to see the HF implementation of masking, you can (optionally!) find them for GPT-2 <a class="reference external" href="https://github.com/huggingface/transformers/blob/ae54e3c3b18bac0832ad62ea9b896dfd52a09850/src/transformers/models/gpt2/modeling_gpt2.py#L792">here</a>.</p>
<blockquote>
<div><p><strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 3.1.3: Embeddings &amp; Attention</span></strong></p>
<ol class="arabic simple">
<li><p>Consider the sentence “The quick fox jumped.”. Write down the causal attention mask that we would use for i=2, i.e,. when training the model to predict the word that comes after “fox”. Assume word / punctuation-sign level tokenization.</p></li>
<li><p>Consider a vocabulary that consists of five tokens. Consider the following embedding matrix with 3-dimensional embeddings:
$$
E =</p></li>
</ol>
<div class="amsmath math notranslate nohighlight" id="equation-bebe4dde-5af5-454f-b27e-bdc79813ca7f">
<span class="eqno">(1)<a class="headerlink" href="#equation-bebe4dde-5af5-454f-b27e-bdc79813ca7f" title="Permalink to this equation">#</a></span>\[\begin{bmatrix}
&gt;0.11 &amp; 0.02 &amp; 0.37\\
&gt;0.56 &amp; 0.89 &amp; 0.41 \\
&gt;0.60 &amp; 0.89 &amp; 0 \\
&gt; 0.20 &amp; 1 &amp; 0.62 \\
&gt; 0.12 &amp; 0.03 &amp; 0.29 \\ 
&gt;\end{bmatrix}\]</div>
<p>$$
Extract the embedding for the token with the ID 2 (assume 0-indexing). Write down the respective calculation step.
3. What is a very well known masked LM architecture?</p>
</div></blockquote>
<p>The contents of the first part of the sheet were inspired by <a class="reference external" href="https://huggingface.co/learn/nlp-course/chapter2/2">this</a> tutorial.</p>
</section>
</section>
</section>
<section id="transformers">
<h2>Transformers<a class="headerlink" href="#transformers" title="Permalink to this heading">#</a></h2>
<p>In the lecture we have now considered what the transformer architecture actually is under the hood. In this part of the sheet, the formal concepts of the architecture are mapped on their implementation, as provided by the PyTorch library and by HuggingFace within the pre-build and pre-trained models.</p>
<section id="pytorch">
<h3>PyTorch<a class="headerlink" href="#pytorch" title="Permalink to this heading">#</a></h3>
<p>PyTorch provides a pre-build <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html">transformer</a> architecture which allows to specify your desired model via different parameters.</p>
<blockquote>
<div><p><strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 3.1.4: Simple transformers in PyTorch</span></strong></p>
<ol class="arabic simple">
<li><p>Write a docstring for the following model class definition. Make sure you can map the parameters of the model onto concepts that were discussed in the lecture.</p></li>
</ol>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># imports</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">math</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Inject some information about the relative or absolute position of the tokens in the sequence.</span>
<span class="sd">    The positional encodings have the same dimension as the embeddings, so that the two can be summed.</span>
<span class="sd">    Here, we use sine and cosine functions of different frequencies.</span>
<span class="sd">    .. math:</span>
<span class="sd">        PosEncoder(pos, 2i) = sin(pos/10000^(2i/d_model))</span>
<span class="sd">        PosEncoder(pos, 2i+1) = cos(pos/10000^(2i/d_model))</span>
<span class="sd">        where pos is the word position and i is the embed idx</span>
<span class="sd">    Args</span>
<span class="sd">    ----</span>
<span class="sd">        d_model: the embed dim (required).</span>
<span class="sd">        dropout: the dropout value (default=0.1).</span>
<span class="sd">        max_len: the max. length of the incoming sequence (default=5000).</span>
<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; pos_encoder = PositionalEncoding(d_model)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">5000</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PositionalEncoding</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>

        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">))</span>
        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
        <span class="n">pe</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;pe&#39;</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">TransformerModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Transformer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Container module with an encoder-decoder transformer model.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ntoken</span><span class="p">,</span> <span class="n">ninp</span><span class="p">,</span> <span class="n">nhead</span><span class="p">,</span> <span class="n">nhid</span><span class="p">,</span> <span class="n">nlayers</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        #### YOUR DOCSTRING HERE #####</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TransformerModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">d_model</span><span class="o">=</span><span class="n">ninp</span><span class="p">,</span> 
            <span class="n">nhead</span><span class="o">=</span><span class="n">nhead</span><span class="p">,</span> 
            <span class="n">dim_feedforward</span><span class="o">=</span><span class="n">nhid</span><span class="p">,</span> 
            <span class="n">num_encoder_layers</span><span class="o">=</span><span class="n">nlayers</span><span class="p">,</span>
            <span class="n">num_decoder_layers</span><span class="o">=</span><span class="n">nlayers</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model_type</span> <span class="o">=</span> <span class="s2">&quot;Transformer&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">src_mask</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoder</span> <span class="o">=</span> <span class="n">PositionalEncoding</span><span class="p">(</span><span class="n">ninp</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">input_emb</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">ntoken</span><span class="p">,</span> <span class="n">ninp</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ninp</span> <span class="o">=</span> <span class="n">ninp</span>
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><p><strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 3.1.5: Simple transformers in PyTorch</span></strong></p>
<ol class="arabic simple">
<li><p>Consider the following transformer model. First, think about the dimensions of the different matrices that are part of the transformer, based on the parameters. Then, check the dimensions of the model instance (you can find a hint for accessing the different parameters of the model below). Does your prediction match the implementation?</p></li>
</ol>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># instantiate a model</span>
<span class="c1"># make sure to check that you understand what the single arguments mean -- ideally, add comments to each of them</span>
<span class="n">transformer_model</span> <span class="o">=</span> <span class="n">TransformerModel</span><span class="p">(</span>
    <span class="mi">128</span><span class="p">,</span>
    <span class="mi">32</span><span class="p">,</span>
    <span class="mi">1</span><span class="p">,</span>
    <span class="mi">64</span><span class="p">,</span>
    <span class="mi">3</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/anaconda3/envs/understanding_llms/lib/python3.10/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f&quot;enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}&quot;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># print the model</span>
<span class="n">transformer_model</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>TransformerModel(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-2): 3 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
        )
        (linear1): Linear(in_features=32, out_features=64, bias=True)
        (dropout): Dropout(p=0.5, inplace=False)
        (linear2): Linear(in_features=64, out_features=32, bias=True)
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.5, inplace=False)
        (dropout2): Dropout(p=0.5, inplace=False)
      )
    )
    (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0-2): 3 x TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
        )
        (linear1): Linear(in_features=32, out_features=64, bias=True)
        (dropout): Dropout(p=0.5, inplace=False)
        (linear2): Linear(in_features=64, out_features=32, bias=True)
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.5, inplace=False)
        (dropout2): Dropout(p=0.5, inplace=False)
        (dropout3): Dropout(p=0.5, inplace=False)
      )
    )
    (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.5, inplace=False)
  )
  (input_emb): Embedding(128, 32)
)
</pre></div>
</div>
</div>
</div>
<p>Now, we want to actually look at the single components of the transformer. First, we can use the following code to see what kinds of attributes the different building blocks of the transformer modules have. For this, let’s pick the first encoder transformer layer and take a closer look at the self-attention:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">transformer_model</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">self_attn</span><span class="o">.</span><span class="vm">__dict__</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;training&#39;: True,
 &#39;_parameters&#39;: OrderedDict([(&#39;in_proj_weight&#39;,
               Parameter containing:
               tensor([[ 0.1548,  0.1673,  0.2034,  ...,  0.0239,  0.0457, -0.0236],
                       [ 0.1744,  0.1674,  0.1002,  ...,  0.2152, -0.1966,  0.0204],
                       [-0.0915, -0.0941, -0.1385,  ...,  0.1005,  0.0104,  0.1407],
                       ...,
                       [-0.0569, -0.1587, -0.1596,  ..., -0.0696, -0.1157, -0.0846],
                       [ 0.1056, -0.0601, -0.0706,  ...,  0.1913,  0.1003,  0.0028],
                       [ 0.1869, -0.1173,  0.1173,  ..., -0.0612, -0.1025,  0.0919]],
                      requires_grad=True)),
              (&#39;q_proj_weight&#39;, None),
              (&#39;k_proj_weight&#39;, None),
              (&#39;v_proj_weight&#39;, None),
              (&#39;in_proj_bias&#39;,
               Parameter containing:
               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
                      requires_grad=True))]),
 &#39;_buffers&#39;: OrderedDict(),
 &#39;_non_persistent_buffers_set&#39;: set(),
 &#39;_backward_pre_hooks&#39;: OrderedDict(),
 &#39;_backward_hooks&#39;: OrderedDict(),
 &#39;_is_full_backward_hook&#39;: None,
 &#39;_forward_hooks&#39;: OrderedDict(),
 &#39;_forward_hooks_with_kwargs&#39;: OrderedDict(),
 &#39;_forward_hooks_always_called&#39;: OrderedDict(),
 &#39;_forward_pre_hooks&#39;: OrderedDict(),
 &#39;_forward_pre_hooks_with_kwargs&#39;: OrderedDict(),
 &#39;_state_dict_hooks&#39;: OrderedDict(),
 &#39;_state_dict_pre_hooks&#39;: OrderedDict(),
 &#39;_load_state_dict_pre_hooks&#39;: OrderedDict(),
 &#39;_load_state_dict_post_hooks&#39;: OrderedDict(),
 &#39;_modules&#39;: OrderedDict([(&#39;out_proj&#39;,
               NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True))]),
 &#39;embed_dim&#39;: 32,
 &#39;kdim&#39;: 32,
 &#39;vdim&#39;: 32,
 &#39;_qkv_same_embed_dim&#39;: True,
 &#39;num_heads&#39;: 1,
 &#39;dropout&#39;: 0.5,
 &#39;batch_first&#39;: False,
 &#39;head_dim&#39;: 32,
 &#39;bias_k&#39;: None,
 &#39;bias_v&#39;: None,
 &#39;add_zero_attn&#39;: False}
</pre></div>
</div>
</div>
</div>
<p>Equipped with the keys to different attributes of the self-attention block, we can closer inspect single components, e.g., the weight matrix:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># we can also directly print the shape of a given weight matrix</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shape of the encoder self attention matrix (combining, Q, K, V): &quot;</span><span class="p">,</span> <span class="n">transformer_model</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">self_attn</span><span class="o">.</span><span class="n">in_proj_weight</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shape of the encoder self attention matrix (combining, Q, K, V):  torch.Size([96, 32])
</pre></div>
</div>
</div>
</div>
<p>In particular, we see that the three matrices that are involved in computing attention scores – query, key and value matrices, are represented as one matrix. If we wanted to access them separately and, e.g., manually reimplement the attention weight computations, we can split the matrix:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">in_proj_weight</span> <span class="o">=</span> <span class="n">transformer_model</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">self_attn</span><span class="o">.</span><span class="n">in_proj_weight</span>
<span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">in_proj_weight</span><span class="p">,</span> <span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Query matrix shape &quot;</span><span class="p">,</span> <span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Key matrix shape &quot;</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Value matrix shape &quot;</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="c1"># Note that if we want to re-compute the attention scores, we need to transpose the matrices</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Query matrix shape  torch.Size([32, 32])
Key matrix shape  torch.Size([32, 32])
Value matrix shape  torch.Size([32, 32])
</pre></div>
</div>
</div>
</div>
<p>Feel free to play around with these retrievals to better understand the architecture.</p>
</section>
<section id="pretrained-transformers">
<h3>Pretrained transformers<a class="headerlink" href="#pretrained-transformers" title="Permalink to this heading">#</a></h3>
<p>Below, we consider in more detail what is under the hood when we load a pretrained transformer from HF. We will look at GPT-2. GPT-2 is a <em>decoder-only</em> model. That is, it only uses self-attention and only has a <em>decoder</em> block. Under the hood, it also uses the same building blocks as the transformer above, but provides higher-level wrappers around the core functionality and already implements the required configurations that were developed as the GPT-2 architecture.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2LMHeadModel</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># first, we load the pretrained model and inspect it</span>
<span class="n">gpt2_lm</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">gpt2_lm</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=768, out_features=50257, bias=False)
)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gpt2_lm</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">c_attn</span><span class="o">.</span><span class="vm">__dict__</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;training&#39;: False,
 &#39;_parameters&#39;: OrderedDict([(&#39;weight&#39;,
               Parameter containing:
               tensor([[-0.4738, -0.2614, -0.0978,  ...,  0.0513, -0.0584,  0.0250],
                       [ 0.0874,  0.1473,  0.2387,  ..., -0.0525, -0.0113, -0.0156],
                       [ 0.0039,  0.0695,  0.3668,  ...,  0.1143,  0.0363, -0.0318],
                       ...,
                       [-0.2592, -0.0164,  0.1991,  ...,  0.0095, -0.0516,  0.0319],
                       [ 0.1517,  0.2170,  0.1043,  ...,  0.0293, -0.0429, -0.0475],
                       [-0.4100, -0.1924, -0.2400,  ..., -0.0046,  0.0070,  0.0198]],
                      requires_grad=True)),
              (&#39;bias&#39;,
               Parameter containing:
               tensor([ 0.4803, -0.5254, -0.4293,  ...,  0.0126, -0.0499,  0.0032],
                      requires_grad=True))]),
 &#39;_buffers&#39;: OrderedDict(),
 &#39;_non_persistent_buffers_set&#39;: set(),
 &#39;_backward_pre_hooks&#39;: OrderedDict(),
 &#39;_backward_hooks&#39;: OrderedDict(),
 &#39;_is_full_backward_hook&#39;: None,
 &#39;_forward_hooks&#39;: OrderedDict(),
 &#39;_forward_hooks_with_kwargs&#39;: OrderedDict(),
 &#39;_forward_hooks_always_called&#39;: OrderedDict(),
 &#39;_forward_pre_hooks&#39;: OrderedDict(),
 &#39;_forward_pre_hooks_with_kwargs&#39;: OrderedDict(),
 &#39;_state_dict_hooks&#39;: OrderedDict(),
 &#39;_state_dict_pre_hooks&#39;: OrderedDict(),
 &#39;_load_state_dict_pre_hooks&#39;: OrderedDict(),
 &#39;_load_state_dict_post_hooks&#39;: OrderedDict(),
 &#39;_modules&#39;: OrderedDict(),
 &#39;nf&#39;: 2304,
 &#39;_is_hf_initialized&#39;: True}
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><p><strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 3.1.6: GPT-2 architecture</span></strong></p>
<ol class="arabic simple">
<li><p>What are the dimensions of the embedding layer? What does each dimension represent? How do they relate to the vocabulary size and why is that so?</p></li>
<li><p>In order to understand the architecture, for yourself, answer the following questions: (a) how many transformer blocks does GPT-2 consist of? (b) Which layer type is used to compute attention weights? (c) What is the dimension of the resulting contextualized embedding?</p></li>
<li><p>Find out the mathematical formula that is used for Layer Normalization that was discussed in the lecture. What does it do? Why do we use it?</p></li>
</ol>
</div></blockquote>
<p>We can actually access the single components of the model. Let’s inspect the components of the first attention block in more detail. Feel free to play around with the printing to get an intuition how to acccess different parts of the model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;shape of the attention weights &quot;</span><span class="p">,</span> <span class="n">gpt2_lm</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">c_attn</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Attention matrix shape&quot;</span><span class="p">)</span>
<span class="c1"># remove .shape to see the full matrix</span>
<span class="n">gpt2_lm</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">c_attn</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="outlook-optional">
<h2>Outlook (optional)<a class="headerlink" href="#outlook-optional" title="Permalink to this heading">#</a></h2>
<p>For those who want to know more about topics that go beyond the contents of the sheet and the lecture, here are some pointers:</p>
<ul class="simple">
<li><p>A different task that relies on text generation is image captioning. We have briefly seen it in the context of attention visualization. <a class="reference external" href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Vinyals_Show_and_Tell_2015_CVPR_paper.pdf">This</a> paper introduces one of the first successful architectures in this domain.</p>
<ul>
<li><p>How is this architecture related to the encoder-decoder models that we have seen in the lecture? What is similar, what differs?</p></li>
<li><p>Consider <a class="reference external" href="https://arxiv.org/pdf/1502.03044">this</a> paper which was mentioned in the lecture. What is different in terms of architecture?</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://sbert.net/index.html">Sentence Transformers</a>: this is a commonly used sentence embedding library.</p>
<ul>
<li><p>For yourself, try to understand the difference of these embeddings to contextualized embeddings that were discussed in the lecture.</p></li>
</ul>
</li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "CogSciPrag/Understanding-LLMs-course",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./tutorials"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../lectures/03-LSTMs-Transformers.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">LSTMs &amp; Transformers</p>
      </div>
    </a>
    <a class="right-next"
       href="../homework/01-language-modeling.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Homework 1: Language models (50 points)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenization">Tokenization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bpe-tokenization">BPE tokenization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#special-tokens">Special tokens</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#outlook-optional-eos-tokens">Outlook (optional): EOS tokens</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pretrained-tokenizers">Pretrained tokenizers</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-masks">Attention masks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transformers">Transformers</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch">PyTorch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pretrained-transformers">Pretrained transformers</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#outlook-optional">Outlook (optional)</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Michael Franke, Carsten Eickhoff, Polina Tsvilodub
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>