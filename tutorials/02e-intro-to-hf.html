

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Sheet 2.5: Introduction to HuggingFace &amp; LMs &#8212; Understanding LLMs</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'tutorials/02e-intro-to-hf';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="LSTMs &amp; Transformers" href="../lectures/03-LSTMs-Transformers.html" />
    <link rel="prev" title="Sheet 2.4: Character-level sequence modeling w/ RNNs" href="02d-char-level-RNN.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo-ULM-2024.png" class="logo__image only-light" alt="Understanding LLMs - Home"/>
    <script>document.write(`<img src="../_static/logo-ULM-2024.png" class="logo__image only-dark" alt="Understanding LLMs - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Course overview: Understanding LLMs
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">01 Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../lectures/01-introduction.html">Background</a></li>
<li class="toctree-l1"><a class="reference internal" href="01-introduction.html">Sheet 1.1: Practical set-up &amp; Training data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">02 ANNs &amp; RNNs</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../lectures/02-torch-ANNs-RNNs.html">PyTorch, ANNs &amp; LMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="02a-pytorch-intro.html">Sheet 2.1: PyTorch essentials</a></li>
<li class="toctree-l1"><a class="reference internal" href="02b-MLE.html">Sheet 2.2: ML-estimation</a></li>
<li class="toctree-l1"><a class="reference internal" href="02c-MLP-pytorch.html">Sheet 2.3: Non-linear regression (MLP w/ PyTorch modules)</a></li>
<li class="toctree-l1"><a class="reference internal" href="02d-char-level-RNN.html">Sheet 2.4: Character-level sequence modeling w/ RNNs</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Sheet 2.5: Introduction to HuggingFace &amp; LMs</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">03 LSTMs &amp; transformers</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../lectures/03-LSTMs-Transformers.html">LSTMs &amp; Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="03a-tokenization-transformers.html">Sheet 3.1: Tokenization &amp; Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="03b-transformers-heads-training.html">Sheet 3.2: Transformer configurations &amp; Training utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lectures/04-LLMs-Prompting.html">Prompting &amp; Current LMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="03c-decoding-prompting.html">Sheet 3.3: Prompting &amp; Decoding</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">04 Fine-tuning &amp; RLHF</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../lectures/05-finetuning-RLHF.html">Fine-tuning and RLHF</a></li>
<li class="toctree-l1"><a class="reference internal" href="04a-finetuning-RL.html">Sheet 4.1 Supervised fine-tuning and RL fine-tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lectures/06-agents.html">LLM systems &amp; agents</a></li>
<li class="toctree-l1"><a class="reference internal" href="05a-agents.html">Sheet 5.1 LLM agents</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lectures/07-attribution.html">Attribution methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="06a-attribution.html">Sheet 6.1 LLM probing &amp; attribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lectures/08-evaluation.html">Evaluation &amp; behavioral assessment</a></li>
<li class="toctree-l1"><a class="reference internal" href="07a-behavioral-assessment.html">Sheet 7.1: Behavioral assessment &amp; Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="07b-biases-assessment.html">Sheet 7.2: Advanced evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="08a-mechanistic-interpretability.html">Sheet 8.1: Mechanistic interpretability</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Homework</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../homework/01-language-modeling.html">Homework 1: Language models (50 points)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../homework/02-prompting.html">Homework 2: Prompting &amp; Generation with LMs (50 points)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../homework/03-agents-RL.html">Homework 3: LLM agents &amp; RL fine-tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../homework/04-evaluation.html">Homework 4: LLM evaluation</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/CogSciPrag/Understanding-LLMs-course/main?urlpath=tree/understanding-llms/tutorials/02e-intro-to-hf.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onBinder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li><a href="https://colab.research.google.com/github/CogSciPrag/Understanding-LLMs-course/blob/main/understanding-llms/tutorials/02e-intro-to-hf.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="initThebeSBT()"
  class="btn btn-sm btn-launch-thebe dropdown-item"
  title="Launch Thebe"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="btn__text-container">Live Code</span>
</button>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/CogSciPrag/Understanding-LLMs-course" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/CogSciPrag/Understanding-LLMs-course/issues/new?title=Issue%20on%20page%20%2Ftutorials/02e-intro-to-hf.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/tutorials/02e-intro-to-hf.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Sheet 2.5: Introduction to HuggingFace & LMs</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-ml-models">Introduction: ML models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#huggingface">HuggingFace ü§ó</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#working-with-lms-via-transformers">Working with LMs via ü§ó Transformers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#outlook">Outlook</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="sheet-2-5-introduction-to-huggingface-lms">
<h1>Sheet 2.5: Introduction to HuggingFace &amp; LMs<a class="headerlink" href="#sheet-2-5-introduction-to-huggingface-lms" title="Permalink to this heading">#</a></h1>
<p><strong>Author</strong>: Polina Tsvilodub</p>
<section id="introduction-ml-models">
<h2>Introduction: ML models<a class="headerlink" href="#introduction-ml-models" title="Permalink to this heading">#</a></h2>
<p>In the previous sheets, we have learned how to explicitly define neural networks and recurrent language models using native PyTorch. Further, we have seen the core steps of machine learning workflows with any kind of model, in general:</p>
<ol class="arabic simple">
<li><p>prepare data (sheet 1.1, respective steps in sheets 2.2-2.4)</p></li>
<li><p>define model architecture (i.e., define which layers with which parameters the network should have)</p></li>
<li><p>train model on data</p></li>
<li><p>evaluate model (sheet 2.4; more on evaluation of language models to come in future sessions)</p></li>
</ol>
<p>The contents of the single steps differ by application (e.g., we use different kinds of data and models to train an image classifier vs. to train a language model), but the general structure of the process is the same.</p>
<p>With this background, in principle, you should be able to put together any well-known neural network yourself (e.g., implement Llama-3 or GPT-3, assuming sufficient time and access to information like PyTorch docs, details of the architecture, maybe some concepts from the next lecture, of course).</p>
<p>However, putting together the network (i.e., specifying all the layers, their sizes, implementing the forward pass through the net etc.) does <em>not</em> mean that you will be able to use the model for generating predictions in practice. Generating predictions (= running inference) on new inputs (e.g., predicting continuations of new input texts or classifying new images) is the purpose of creating a machine learning model ‚Äì we build them because we want to automate these tasks. Ideally, we want the model to <em>generalize well</em> ‚Äì to be able to handle inputs for that the model was not trained on and still perform the task correctly (e.g., produce a sensible text completion for an entirely new text input).</p>
<p>For generating predictions, we need the <strong>trained</strong> model, i.e., we need not just the definition of the model, but also the <em>model weights</em> set to optimized values that result from (successful) training. That is, we need the values of all the weight matrices (we have seen those in sheet 2.3). In practice, successful training of state-of-the-art models is quite costly and data-hungry (at least for creating actually useful models).</p>
<p><strong>NB</strong>: The practical sheets will try to be consistent and use the term ‚Äúarchitecture‚Äù to refer to the more abstract definition, carcass, of a model (i.e,. the specific layer types and parameterizations underlying the model), and the term ‚Äúmodel‚Äù for the particular instatiation with particular weights of that model configuration. When referring to a specific model instance, there are different important concepts / terms:</p>
<ul class="simple">
<li><p><strong>pretrained</strong> model: this is commonly used in the domains of NLP and computer vision (CV) where models are usually first trained on a very large collection of data to learn their task (in general). In the context of NLP, this refers to a model that has been trained to predict the next word on a large collection of text (e.g., on the Pile which we have seen in sheet 1.1). On a high level, this step can be thought of as the step where the model learns to predict fluent language in general. In the context of CV, this often refers to a model that has been trained to classify images on the ImageNet dataset.</p></li>
<li><p><strong>fine-tuned</strong> model: this refers to a pretrained model that has been further trained on a <em>task-specific</em>, or, generally curated and therefore, usually smaller, dataset. This step can be thought of as optimizing the model for a particular task, like predicting texts in a particular domain (e.g., movie reviews). This is usually done based on a pretrained model because curated datasets are often too small to learn a task from scratch well.</p></li>
</ul>
<p>The term ‚Äútrained‚Äù model will be mostly used as an umbrella terms for both types of models.</p>
<blockquote>
<div><p><strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 2.5.1: ML models workflow </span></strong></p>
<ol class="arabic simple">
<li><p>Given examples from the previous sheets, in your own words, describe the difference between <em>training</em> a model and running <em>inference</em> with a model. (Understanding this is absolutely critical; please ask questions if have any doubts!)</p></li>
<li><p>Suppose you want to train the model GPT-2 to generate IMDB movie reviews. In your own words (few intuitive sentences), describe the high-level steps that you would do to accomplish this task.</p></li>
</ol>
</div></blockquote>
<p>Click below to see the solution.</p>
<div class="toggle docutils container">
<blockquote>
<div><p><strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 2.5.1: ML models workflow </span></strong></p>
<ol class="arabic simple">
<li><p>Given examples from the previous sheets, in your own words, describe the difference between <em>training</em> a model and running <em>inference</em> with a model. (Understanding this is absolutely critical; please ask questions if have any doubts!) <strong>Answer</strong> Training means that we fit the model to training data to improve the model. Inference means that we take the trained model and apply it to new cases to make predictions.</p></li>
<li><p>Suppose you want to train the model GPT-2 to generate IMDB movie reviews. In your own words (few intuitive sentences), describe the highlevel steps that you would do to accomplish this task. <strong>Answer</strong> Get the pretrained GPT-2 model and a dataset of IMDB reviews. Split the data into training and test data. Train the model on the training data, adjusting the parameters, and use the test data to evaluate it.</p></li>
</ol>
</div></blockquote>
</div>
</section>
<section id="huggingface">
<h2>HuggingFace ü§ó<a class="headerlink" href="#huggingface" title="Permalink to this heading">#</a></h2>
<p>Luckily, there are many popular <em>open-source</em> models, i.e., those for which the architecture and the weights have been made freely accessible by the developers: e.g., GPT-2, the LLama models, BERT and others. In contrast, e.g., GPT-4 is <em>closed-source</em> ‚Äì we don‚Äôt know exactly which layers with which configurations are inside the model, and we cannot access the weights ourselves; they are only hosted by OpenAI, and we can send our input data to their servers and get back predictions (by using the OpenAI API).</p>
<p>Open-source models are frequently used by the community and have been made available via platforms and packages which provide an easy interface for loading pretrained models, generating predictions from these pretrained models, or training models from scratch. The by-far most used platform for open-source resources which we will heavily use in this course is <a class="reference external" href="https://huggingface.co/"><strong>HuggingFace</strong></a>. HuggingFace provides infrastructure for working with many different types of machine learning models (for working with audio, vision, language tasks), but, as you might guess, we will be focusing on functionality relevant to language modeling. HuggingFace will often be abbreviated as HF throughout the sheets.</p>
<p>Here is a high-level overview of all the nice things that HuggingFace provides:</p>
<ul class="simple">
<li><p>(most relevant for us) the packages <a class="reference external" href="https://huggingface.co/docs/transformers/en/index"><code class="docutils literal notranslate"><span class="pre">transformers</span></code></a> and <a class="reference external" href="https://huggingface.co/docs/datasets/index"><code class="docutils literal notranslate"><span class="pre">datasets</span></code></a> which we already installed last week. These allow easy access and workflow with models and with datasets.</p>
<ul>
<li><p>Overviews, guides, tutorials for different tasks that can be done with <code class="docutils literal notranslate"><span class="pre">transformers</span></code>, docs for different models as well as the documentation of the API can be found <a class="reference external" href="https://huggingface.co/docs/transformers/index">here</a>.</p></li>
<li><p>The same resources for <code class="docutils literal notranslate"><span class="pre">datasets</span></code> can be found <a class="reference external" href="https://huggingface.co/docs/datasets/index">here</a>.</p></li>
</ul>
</li>
<li><p>The (online) HF platform called HuggingFace Hub actually stores trained model weights (also sometimes called checkpoints) and datasets. They are downloaded to the local machine when accessed through the <code class="docutils literal notranslate"><span class="pre">transformers</span></code> or <code class="docutils literal notranslate"><span class="pre">datasets</span></code> package. Available models can be browsed <a class="reference external" href="https://huggingface.co/models">here</a>; available datasets can be browsed <a class="reference external" href="https://huggingface.co/datasets">here</a>.</p>
<ul>
<li><p>In order to <em>upload</em> your own resources to HF Hub and in order to access some models (inclding LLama-2), you need a free account. It can easily be created on HF. So far, an account is not required for following the class.</p></li>
</ul>
</li>
<li><p>The platform provides much more (feel free to explore, but also try not to feel overwhelmed if you are seeing HF for the first time). Here are some highlights most relevant to the course:</p>
<ul>
<li><p><a class="reference external" href="https://huggingface.co/learn/nlp-course/chapter1/1">NLP course</a> which includes practical tutorials for common NLP tasks with transformers.</p></li>
<li><p><a class="reference external" href="https://huggingface.co/docs/evaluate/index"><code class="docutils literal notranslate"><span class="pre">evaluate</span></code></a> package for computing common evaluation metrics (more on this in the next sessions).</p></li>
<li><p>Further (more advanced) packages for <a class="reference external" href="https://huggingface.co/docs/accelerate/index">accelerated</a> and parallelized training on GPUs, via parameter-efficient fine-tuning (<a class="reference external" href="https://huggingface.co/docs/peft/index">PEFT</a>) and more.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">tokenizers</span></code> <a class="reference external" href="https://huggingface.co/docs/tokenizers/index">package</a>, which we will not make (heavy) use of, because we will only use functionality of pretrained tokenizers which are shipped together with models via transformers.</p></li>
</ul>
</li>
</ul>
<blockquote>
<div><p><strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 2.5.2: Familiarization with HuggingFace </span></strong></p>
<ol class="arabic simple">
<li><p>Just for yourself, try to find whether the model BERT is available on HF. If yes, try get a sense of the kind of information that HF provides about available models.</p></li>
</ol>
</div></blockquote>
<section id="working-with-lms-via-transformers">
<h3>Working with LMs via ü§ó Transformers<a class="headerlink" href="#working-with-lms-via-transformers" title="Permalink to this heading">#</a></h3>
<p>We will dive right into using the <code class="docutils literal notranslate"><span class="pre">transformers</span></code> package ourselves. The learning goals for the remaining part of the sheet are:</p>
<ul class="simple">
<li><p>learn how to load a pretrained language model</p></li>
<li><p>learn how to generate a prediction with the model</p></li>
<li><p>learn about the <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> class which allows to train from scratch or fine-tune the model. (The following code is inspired by <a class="reference external" href="https://huggingface.co/docs/transformers/training">this</a> tutorial.)</p></li>
</ul>
<p><strong>NB:</strong> this sheet uses the pretrained <a class="reference external" href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">GPT-2</a>, a causal (decoder-only) transformer model. We have not covered transformers in detail yet, so the setup of the model and some of the required setup for using it are treated as a blackbox in this sheet. More in-depth explanations will follow the next lecture covering transformers.</p>
<p>There are two ways of generating predictions with a trained model available on HF. First, we will do so explicitly. The steps are:</p>
<ul class="simple">
<li><p>load the pretrained model with its dedicated class</p></li>
<li><p>tokenize some input text</p></li>
<li><p>generate predictions with greedy decoding (i.e., under the hood, compute forward passes through the model and retrieve the most likely token, given the previous input)</p></li>
<li><p>convert the newly generated tokens back into human-readable strings</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># import packages</span>
<span class="c1"># note: if you are on Colab, you might need to install some requirements</span>
<span class="c1"># as we did in Sheet 1.1. Otherwise, don&#39;t forget to activate your local environment</span>

<span class="kn">import</span> <span class="nn">transformers</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2Tokenizer</span><span class="p">,</span> <span class="n">GPT2LMHeadModel</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">DataCollatorForLanguageModeling</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">TrainingArguments</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/anaconda3/envs/understanding_llms/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># additioanly, we need to install accelerate</span>
<span class="c1"># uncomment and run the following line on Colab or in your environment</span>
<span class="c1"># !pip install accelerate</span>
<span class="c1"># NOTE: in a notebook, reloading of the kernel might be required after installation if you get dependency errors with the transformers package</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># define computational device</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Device: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">elif</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;mps&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Device: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Device: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># define input text</span>
<span class="n">input_text</span> <span class="o">=</span> <span class="s2">&quot;Natural language processing is &quot;</span>

<span class="c1"># load pretrained model and respective tokenizer</span>
<span class="c1"># the class GPT2LMHeadModel initialized the &quot;container&quot; for the GPT-2</span>
<span class="c1"># architecture with an LM head - i.e., an output layer for predicting next tokens</span>
<span class="c1"># .from_pretrained loads the pretrained weights into the container</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1"># load pretrained tokenizer belonging to GPT-2 (more on tokenizers next session)</span>
<span class="c1">##### YOUR CODE HERE ##### (Hint: you have seen how to do this in sheet 1.1)</span>
<span class="n">tokenizer</span> <span class="o">=</span> 

<span class="c1"># tokenize our input text</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">input_text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Tokenized text (input IDs) &quot;</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">)</span>

<span class="c1"># generate predictions</span>
<span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
<span class="c1"># decode the prediction (i.e., convert tokens back to text)</span>
<span class="n">answer</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">prediction</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Predicted continuation: &quot;</span><span class="p">,</span> <span class="n">answer</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Click below to see the solution.</p>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># define input text</span>
<span class="n">input_text</span> <span class="o">=</span> <span class="s2">&quot;Natural language processing is &quot;</span>

<span class="c1"># load pretrained model and respective tokenizer</span>
<span class="c1"># the class GPT2LMHeadModel initialized the &quot;container&quot; for the GPT-2</span>
<span class="c1"># architecture with an LM head - i.e., an output layer for predicting next tokens</span>
<span class="c1"># .from_pretrained loads the pretrained weights into the container</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1"># load pretrained tokenizer belonging to GPT-2 (more on tokenizers next session)</span>
<span class="c1">##### YOUR CODE HERE ##### (Hint: you have seen how to do this in sheet 1.1)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>

<span class="c1"># tokenize our input text</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">input_text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Tokenized text (input IDs) &quot;</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">)</span>

<span class="c1"># generate predictions</span>
<span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
<span class="c1"># decode the prediction (i.e., convert tokens back to text)</span>
<span class="n">answer</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">prediction</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Predicted continuation: &quot;</span><span class="p">,</span> <span class="n">answer</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
</div>
<p>Alternatively, we can make use of transformer‚Äôs <code class="docutils literal notranslate"><span class="pre">pipeline</span></code> ‚Äì a utility wrapper around these steps which makes it easier for us to generate predictions, without having to manually implement them. HF provides pipelines for many common tasks.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># initialize the pipeline with the model of our choice</span>
<span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span>
    <span class="c1"># specify task</span>
    <span class="s1">&#39;text-generation&#39;</span><span class="p">,</span> 
    <span class="n">model</span><span class="o">=</span><span class="s1">&#39;gpt2&#39;</span>
<span class="p">)</span>

<span class="c1"># run the pipeline with default configs (note that the default is NOT greedy decoding)</span>
<span class="n">pipe_output</span> <span class="o">=</span> <span class="n">generator</span><span class="p">(</span><span class="n">input_text</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Pipeline output: &quot;</span><span class="p">,</span> <span class="n">pipe_output</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><p><strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 2.5.3: Generating predictions </span></strong></p>
<ol class="arabic simple">
<li><p>Come up with a different input string and generate a prediction. Would you consider the output a good prediction?</p></li>
<li><p>Change the code (explicit or implicit) by adding a few parameters so as to use <em>pure sampling</em> for predicting the output. How do the predictions change? (Hint: see slides for a reminder about decoding schemes and <a class="reference external" href="https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.TextGenerationPipeline">docs</a> for practical implementation)</p></li>
</ol>
</div></blockquote>
<p>Click below to see the solution.</p>
<div class="toggle docutils container">
<blockquote>
<div><p><strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 2.5.3: Generating predictions </span></strong></p>
<ol class="arabic simple">
<li><p>Come up with a different input string and generate a prediction. Would you consider the output a good prediction?</p></li>
<li><p>Change the code (explicit or implicit) by adding a few parameters so as to use <em>pure sampling</em> for predicting the output. How do the predictions change? (Hint: see slides for a reminder about decoding schemes and <a class="reference external" href="https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.TextGenerationPipeline">docs</a> for practical implementation) <strong>Answer</strong> model.generate(input_ids, do_sample=True, top_k=0)</p></li>
</ol>
</div></blockquote>
</div>
<p>Another common use of pretrained models is to further fine-tune them on a specific dataset, e.g., to specialize them on generating text of a particular domain, or to train them for a specific task. The latter is often with a task-specific output <em>head</em> ‚Äì i.e., a different output layer is used. For instance, for classification tasks (like sentiment classification), a classification layer could be trained on a sentiment classification dataset, while keeping the rest of the network fixed from pretraining (or, frozen).</p>
<p>The following part shows how to fine-tune GPT-2 on a specific corpus ‚Äì IMDB movie reviews. Specifically, we will make use of the transformers <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> class which provides convenient wrappers around different steps that need to be done in order to train a model. It essentially simplifies the implementation of the single substeps that are summarized under step 3 in the high-level overview of the ML workflow at the beginning of the sheet.</p>
<blockquote>
<div><p><strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 2.5.4: Training steps </span></strong></p>
<ol class="arabic simple">
<li><p>As a reminder, using pseudo-code, list of concrete steps that constitute a single training step in a training loop. Take a look at the training loops in sheets 2.2 and 2.3 to compare your results.</p></li>
<li><p>As you go through the following code, please complete it in spots which are commented with ‚Äú### YOUR CODE HERE ###‚Äù</p></li>
</ol>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># load IMDB dataset</span>
<span class="c1"># dataset = load_dataset(&quot;truthful_qa&quot;, &quot;generation&quot;)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;stanfordnlp/imdb&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># set the pad token of the tokenizer which we loaded above</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
<span class="c1"># set the side on which pad tokens should be added to the input (if required)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">padding_side</span> <span class="o">=</span> <span class="s2">&quot;left&quot;</span>
<span class="c1"># define a function to collate the data into batches and tokenize it</span>
<span class="c1"># this helper allows to massage the data such that it can be batched into the same </span>
<span class="c1"># tensor representing a batch of inputs and can be used for training.</span>
<span class="c1"># specifically it creates *labels* for the model to predict</span>
<span class="c1"># note that the labels are the same as the input, **not** shifted by one token relative to the input</span>
<span class="c1"># the shifting is done automatically by the HF model class when the automatic loss computation is used</span>
<span class="n">data_collator</span> <span class="o">=</span> <span class="n">DataCollatorForLanguageModeling</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">mlm</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span>
        <span class="n">input_text</span><span class="p">,</span> 
        <span class="n">max_length</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
        <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span>
    <span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">   YOUR TEXT HERE.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    input_text : YOUR TEXT HERE.</span>
<span class="sd">    max_length : YOUR TEXT HERE.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    YOUR TEXT HERE.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">tokenized_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
        <span class="n">input_text</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span>
        <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;max_length&quot;</span><span class="p">,</span>
        <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span>
    <span class="p">)</span>
    <span class="c1"># mask padding tokens so that the model doesn&#39;t attend to them</span>
    <span class="c1"># more on masking next week</span>
    <span class="n">tokenized_input</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">tokenized_input</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="o">!=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="n">tokenized_input</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">],</span>
            <span class="s2">&quot;attention_mask&quot;</span><span class="p">:</span> <span class="n">tokenized_input</span><span class="p">[</span><span class="s1">&#39;attention_mask&#39;</span><span class="p">]}</span>

<span class="n">tokenized_datasets</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
    <span class="n">tokenize</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">column_names</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Click below to see the solution.</p>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># set the pad token of the tokenizer which we loaded above</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
<span class="c1"># set the side on which pad tokens should be added to the input (if required)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">padding_side</span> <span class="o">=</span> <span class="s2">&quot;left&quot;</span>
<span class="c1"># define a function to collate the data into batches and tokenize it</span>
<span class="c1"># this helper allows to massage the data such that it can be batched into the same</span>
<span class="c1"># tensor representing a batch of inputs and can be used for training.</span>
<span class="c1"># specifically it creates *labels* for the model to predict</span>
<span class="c1"># note that the labels are the same as the input, **not** shifted by one token relative to the input</span>
<span class="c1"># the shifting is done automatically by the HF model class when the automatic loss computation is used</span>
<span class="n">data_collator</span> <span class="o">=</span> <span class="n">DataCollatorForLanguageModeling</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">mlm</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span>
        <span class="n">input_text</span><span class="p">,</span>
        <span class="n">max_length</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
        <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span>
    <span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function to tokenize text into a series of input ids.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    input_text : the text to tokenize.</span>
<span class="sd">    max_length : the maximal number of resulting tokens.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    input_ids corresponding to the tokenized text and an attention_mask,</span>
<span class="sd">    where padding tokens are masked.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">tokenized_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
        <span class="n">input_text</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span>
        <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;max_length&quot;</span><span class="p">,</span>
        <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span>
    <span class="p">)</span>
    <span class="c1"># mask padding tokens so that the model doesn&#39;t attend to them</span>
    <span class="c1"># more on masking next week</span>
    <span class="n">tokenized_input</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">tokenized_input</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="o">!=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="n">tokenized_input</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">],</span>
            <span class="s2">&quot;attention_mask&quot;</span><span class="p">:</span> <span class="n">tokenized_input</span><span class="p">[</span><span class="s1">&#39;attention_mask&#39;</span><span class="p">]}</span>

<span class="n">tokenized_datasets</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
    <span class="n">tokenize</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">column_names</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># print a sample of the tokenized dataset</span>
<span class="n">tokenized_datasets</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># since we don&#39;t want to fully fine-tune the model but rather learn </span>
<span class="c1"># the basics of the set up, we will only use a small subset of the data</span>
<span class="n">subsampled_dataset</span> <span class="o">=</span> <span class="n">tokenized_datasets</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># we inspect what the data_collator does</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">data_collator</span><span class="p">([</span><span class="n">subsampled_dataset</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">)])</span>
<span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">out</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Tensor </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">out</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>IMPORTANT:</strong> it is key to understand the language modeling objective (i.e., mechanism used to train a model to predict the next token) and how it is implemented in practice. Conceptually, the LM objective is discussed at the beginning of the tutorial. In terms of implementation, the addition of special tokens and shifting happens within each transformers model, it is taken care of by the package. You can find and example for GPT-2 <a class="reference external" href="https://github.com/huggingface/transformers/blob/571dd693b5d20754ecc472030903a94f92cfa9f8/src/transformers/models/gpt2/modeling_gpt2.py#L1100">here</a>. As you can see, the collator function above creates the labels that are used as target for prediction for us. Please make sure that you understand this step and that you would be able to create labels, given an input, yourself. Note that if you manually implement the language modeling loss function (e.g., using the CrossEntropyLoss), you will have to do the processing of the inputs and labels yourself.</p>
<blockquote>
<div><p><strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 2.5.5: Understanding Trainer </span></strong></p>
<p>The next cell defines the key configurations of the training in <code class="docutils literal notranslate"><span class="pre">TrainingArguments</span></code>. These are passed to the <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> in order to instantiate a training loop.</p>
<ol class="arabic simple">
<li><p>Please use the documentation <a class="reference external" href="https://huggingface.co/docs/transformers/v4.40.1/en/main_classes/trainer#transformers.Trainer">here</a> to write short comments for each relevant line of the following code cell, so that you know what different training parameters there are.</p></li>
<li><p>What is the size of the training data? What is the size of the test data?</p></li>
</ol>
</div></blockquote>
<p>Click below to see the solution.</p>
<div class="toggle docutils container">
<blockquote>
<div><p><strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 2.5.5: Understanding Trainer </span></strong></p>
<p>The next cell defines the key configurations of the training in <code class="docutils literal notranslate"><span class="pre">TrainingArguments</span></code>. These are passed to the <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> in order to instantiate a training loop.</p>
<ol class="arabic simple">
<li><p>Please use the documentation <a class="reference external" href="https://huggingface.co/docs/transformers/v4.40.1/en/main_classes/trainer#transformers.Trainer">here</a> to write short comments for each relevant line of the following code cell, so that you know what different training parameters there are.</p></li>
<li><p>What is the size of the training data? What is the size of the test data? <strong>Answer:</strong> both have the size of subsampled_dataset, which is 500</p></li>
</ol>
</div></blockquote>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># define training arguments</span>
<span class="n">args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="s2">&quot;imdb_gpt2&quot;</span><span class="p">,</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">evaluation_strategy</span><span class="o">=</span><span class="s2">&quot;steps&quot;</span><span class="p">,</span>
    <span class="n">eval_steps</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">logging_steps</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">gradient_accumulation_steps</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">lr_scheduler_type</span><span class="o">=</span><span class="s2">&quot;cosine&quot;</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">5e-4</span><span class="p">,</span>
    <span class="n">save_steps</span><span class="o">=</span><span class="mi">5_000</span><span class="p">,</span>
    <span class="n">fp16</span><span class="o">=</span><span class="kc">True</span> <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span> <span class="k">else</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">push_to_hub</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">use_mps_device</span><span class="o">=</span><span class="kc">True</span> <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="s2">&quot;mps&quot;</span> <span class="k">else</span> <span class="kc">False</span><span class="p">,</span>
    
<span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span>
    <span class="n">data_collator</span><span class="o">=</span><span class="n">data_collator</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">subsampled_dataset</span><span class="p">,</span>
    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">subsampled_dataset</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># define training arguments</span>
<span class="n">args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="s2">&quot;imdb_gpt2&quot;</span><span class="p">,</span> <span class="c1"># output directory for predictions and checkpoints</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="c1"># batch size per GPU/CPU in training</span>
    <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="c1"># batch size per GPU/CPU in evaluation</span>
    <span class="n">evaluation_strategy</span><span class="o">=</span><span class="s2">&quot;steps&quot;</span><span class="p">,</span> <span class="c1"># evaluation at every 5 steps</span>
    <span class="n">eval_steps</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">logging_steps</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">gradient_accumulation_steps</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="c1"># number of steps before performing backward pass</span>
    <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="c1"># number of training epochs (runs through training set)</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="c1"># weight decay is used to prevent overfitting</span>
    <span class="n">lr_scheduler_type</span><span class="o">=</span><span class="s2">&quot;cosine&quot;</span><span class="p">,</span> <span class="c1"># learning rate scheduler</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">5e-4</span><span class="p">,</span> <span class="c1"># learning rate</span>
    <span class="n">save_steps</span><span class="o">=</span><span class="mi">5_000</span><span class="p">,</span> <span class="c1"># number of steps before checkpoint</span>
    <span class="n">fp16</span><span class="o">=</span><span class="kc">True</span> <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span> <span class="k">else</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">push_to_hub</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">use_mps_device</span><span class="o">=</span><span class="kc">True</span> <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="s2">&quot;mps&quot;</span> <span class="k">else</span> <span class="kc">False</span><span class="p">,</span>

<span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span>
    <span class="n">data_collator</span><span class="o">=</span><span class="n">data_collator</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">subsampled_dataset</span><span class="p">,</span>
    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">subsampled_dataset</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># run the training of the model</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Note that we did not explicitly pass a loss function to the training. The Trainer constructs it automatically. However, we could also explicitly define the language modeling loss. For this, we need to overrise the <code class="docutils literal notranslate"><span class="pre">compute_loss()</span></code> method of the Trainer and build a custom Trainer with it.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">IMDBTrainer</span><span class="p">(</span><span class="n">Trainer</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">compute_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">return_outputs</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">loss</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span> <span class="k">if</span> <span class="n">return_outputs</span> <span class="k">else</span> <span class="n">loss</span>
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><p><strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 2.5.6: CrossEntropyLoss [more advanced]</span></strong></p>
<ol class="arabic simple">
<li><p>Implement an even more explicit definition of the loss function by using the <code class="docutils literal notranslate"><span class="pre">torch.nn.CrossEntropyLoss()</span></code> and the labels which are passed as part of the inputs instead of using outputs.loss. The respective docs can be found on the PyTorch website. NB: Make sure to correctly shift your labels.</p></li>
</ol>
</div></blockquote>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">IMDBTrainer</span><span class="p">(</span><span class="n">Trainer</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">compute_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">return_outputs</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">celoss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
        <span class="n">modelout</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">]</span>
        <span class="c1"># CELoss expects the input to be of shape (batch_size, num_classes, additional_dims)</span>
        <span class="c1"># So we want to swap the last two dimensions</span>
        <span class="n">modelout</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="n">modelout</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="c1"># create tensor of EOS ids with shape (batch_size*1)</span>
        <span class="n">eos_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">]</span><span class="o">*</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">eos_tensor</span> <span class="o">=</span> <span class="n">eos_tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="c1"># exclude first input from target labels</span>
        <span class="n">target_out</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">][:,</span><span class="mi">1</span><span class="p">:]</span>
        <span class="n">target_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">target_out</span><span class="p">,</span><span class="n">eos_tensor</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">celoss</span><span class="p">(</span><span class="n">modelout</span><span class="p">,</span> <span class="n">target_out</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span> <span class="k">if</span> <span class="n">return_outputs</span> <span class="k">else</span> <span class="n">loss</span>
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># use explicit trainer</span>
<span class="n">explicit_trainer</span> <span class="o">=</span> <span class="n">IMDBTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span>
    <span class="n">data_collator</span><span class="o">=</span><span class="n">data_collator</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">subsampled_dataset</span><span class="p">,</span>
    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">subsampled_dataset</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">explicit_trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Finally, we want to inspect the training dynamics by plotting the training and evaluation losses. These can be accessed in the Trainer history. Ideally, both of them should decrease ‚Äì this would indicate successful optimization. Feel free to play around with training parameters and plot the losses to see what effects different values have.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># plot the train and evaluation losses</span>
<span class="n">log_history</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">log_history</span>

<span class="c1"># wrangle the trainer logs into a dataframe for easier plotting</span>
<span class="n">train_losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">test_losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">log_history</span><span class="p">:</span>
    <span class="k">if</span> <span class="s2">&quot;loss&quot;</span> <span class="ow">in</span> <span class="n">d</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="n">train_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">])</span>
    <span class="k">elif</span> <span class="s2">&quot;eval_loss&quot;</span> <span class="ow">in</span> <span class="n">d</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="n">test_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="s2">&quot;eval_loss&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;train loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test_losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;test loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Logged steps&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="outlook">
<h3>Outlook<a class="headerlink" href="#outlook" title="Permalink to this heading">#</a></h3>
<p>The code above used a GPT-2 specific class, <code class="docutils literal notranslate"><span class="pre">GPT2LMHeadModel</span></code>, to load the model. However, instead of the architecture-specific classes the so called AutoClass can often be used instead. It essentially ‚Äúinfers‚Äù the architecture from a configuration file that is retrieved when a model is downloaded from HF. In the next tutorials, you will often see subclasses of this being used. You can find more (optional) information <a class="reference external" href="https://huggingface.co/docs/transformers/model_doc/auto">here</a>.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "CogSciPrag/Understanding-LLMs-course",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./tutorials"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="02d-char-level-RNN.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Sheet 2.4: Character-level sequence modeling w/ RNNs</p>
      </div>
    </a>
    <a class="right-next"
       href="../lectures/03-LSTMs-Transformers.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">LSTMs &amp; Transformers</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-ml-models">Introduction: ML models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#huggingface">HuggingFace ü§ó</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#working-with-lms-via-transformers">Working with LMs via ü§ó Transformers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#outlook">Outlook</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Michael Franke, Carsten Eickhoff, Polina Tsvilodub
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>